{"title":"Integrated excercise","markdown":{"yaml":{"title":"Integrated excercise","format":{"html":{"self-contained":true,"code-fold":true}},"bibliography":"references.bib"},"headingText":"Background behind the dataset","containsRefs":false,"markdown":"\n\n```{r}\n#| message: false\n#| warning: false\n#| error: false\n#| echo: false\nlibrary(here)\n\n```\n\n```{r setup, include=FALSE}\n.libPaths(\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library\")\nlibrary(rstan)\n```\n\n![](work_in_progress.png){fig-align=\"center\" width=\"25%\"}\n\n\nThis dataset is a simulated dataset that is based on an existing study of @frumuselu2015. In this study, the key question was whether subtitles help in foreign language acquisition. Spanish students (n = 36) watched episodes of the popular tv-show \"Friends\" for half an hour each week, during 26 weeks. The students were assigned to 3 conditions:\n\n-   English subtitled (condition \"FL\")\n\n-   Spanish subtitled (condition \"MT\")\n\n-   No subtitles (condition \"NoSub\")\n\nAt 3 occasions students got a Fluency test:\n\n-   Before the 26 weeks started\n\n-   After 12 weeks\n\n-   After the experiment\n\nThe dependent variable is a measure based on the number of words used in a scripted spontaneous interview with a test taker. The data is structured as follows:\n\n```{r}\nload(file = here(\"Integrated_exercise\",\"Subtitles.RData\"))\nhead(Subtitles, 9)\n```\n\nIf we visualize the dataset we get a first impression of the effect of the condition. In this exercise it is your task to do the proper Bayesian modelling and interpretation.\n\n```{r}\n#| message: false\n#| warning: false\n#| error: false\n\nlibrary(tidyverse)\n\ntheme_set(theme_linedraw() +\n            theme(text = element_text(family = \"Times\", size = 12),\n                  panel.grid = element_blank())\n)\n\nSubtitles %>%\n  ggplot(\n    aes(\n      x = occasion,\n      y = fluency,\n      group = student\n      )\n  ) +\n  geom_path(\n    aes(\n      color = condition\n    )\n  ) \n```\n\n------------------------------------------------------------------------\n\n```{r, out.height = \"40%\", out.width=\"40%\", echo = FALSE}\nknitr::include_graphics(here(\"Integrated_exercise\",\"friends3.jpeg\"))\n```\n\n------------------------------------------------------------------------\n\n# 1. Task 1\n\nFirst we will start by building 4 alternative mixed effects models. In each of the models we have to take into account the fact that we have multiple observations per student. So we need a random effect for students in the model.\n\n-   M0: only an intercept and a random effect for `student`\n\n-   M1: M0 + fixed effect of `occasion`\n\n-   M2: M1 + fixed effect of `condition`\n\n-   M3: M2 + interaction effect between `occasion` and `condition`\n\nMake use of the default priors of `brms`.\n\nOnce the models are estimated, compare the models on their fit making use of the leave-one-out cross-validation. Determine which model fits best and will be used to build our inferences.\n\n------------------------------------------------------------------------\n\n<i>Solution</i>\n\nThe following code-block shows how the models can be estimated and compared on their fit. Notice how I first create a set of dummy-variables for the categories of the `occasion` and the `condition` variables. This makes it a bit harder (more code writing) to define my model but it generates some flexibility later on. For instance, when thinking about setting priors, I can set priors for each of the effects of these dummy variables. Also, it will result in shorter names of parameters in my model in the resulting objects for the model.\n\n```{r}\n#| echo: true\n#| eval: false\n#| message: false\n#| error: false\n#| cache: false\n\nlibrary(brms)\nlibrary(tidyverse)\n\nSubtitles <- Subtitles %>%\n  mutate(\n\n    # dummy for Occ2\n    Occ2 = case_when(\n      occasion == \"Occ2\" ~ 1,\n      occasion == \"Occ1\" ~ 0,\n      occasion == \"Occ3\" ~ 0,\n    ),\n    \n    # dummy for Occ3\n    Occ3 = case_when(\n      occasion == \"Occ3\" ~ 1,\n      occasion == \"Occ1\" ~ 0,\n      occasion == \"Occ2\" ~ 0,\n    ),\n    \n    # dummy for FL condition\n    FL = case_when(\n      condition == \"FL\" ~ 1,\n      condition == \"MT\" ~ 0,\n      condition == \"NoSub\" ~ 0,\n    ),\n    \n    # dummy for MT\n    MT = case_when(\n      condition == \"MT\" ~ 1,\n      condition == \"FL\" ~ 0,\n      condition == \"NoSub\" ~ 0,\n    )\n  )\n\n# Estimate the models\n\nM0 <- brm(\n  fluency ~ 1 + (1|student),\n  data = Subtitles,\n  cores = 4,\n  backend = \"cmdstanr\",\n  seed = 1975\n)\n\nM1 <- brm(\n  fluency ~ 1 + Occ2 + Occ3 + (1|student),\n  data = Subtitles,\n  cores = 4,\n  backend = \"cmdstanr\",\n  seed = 1975\n)\n\nM2 <- brm(\n  fluency ~ 1 + Occ2 + Occ3 + FL + MT + (1|student),\n  data = Subtitles,\n  cores = 4,\n  backend = \"cmdstanr\",\n  seed = 1975\n)\n\nM3 <- brm(\n  fluency ~ 1 + Occ2*FL + Occ2*MT + Occ3*FL + Occ3*MT + (1|student),\n  data = Subtitles,\n  cores = 4,\n  backend = \"cmdstanr\",\n  seed = 1975\n)\n\n# loo cross-validation of the models\n\nloo_M0 <- loo(M0)\nloo_M1 <- loo(M1)\nloo_M2 <- loo(M2)\nloo_M3 <- loo(M3)\n\nloo_models <- loo_compare(\n  loo_M0,\n  loo_M1,\n  loo_M2,\n  loo_M3\n)\n\nprint(loo_models, simplify = F)\n\n```\n\n```{r}\n#| echo: false\n#| eval: true\nlibrary(brms)\nlibrary(tidyverse)\n\nSubtitles <- Subtitles %>%\n  mutate(\n\n    # dummy for Occ2\n    Occ2 = case_when(\n      occasion == \"Occ2\" ~ 1,\n      occasion == \"Occ1\" ~ 0,\n      occasion == \"Occ3\" ~ 0,\n    ),\n    \n    # dummy for Occ3\n    Occ3 = case_when(\n      occasion == \"Occ3\" ~ 1,\n      occasion == \"Occ1\" ~ 0,\n      occasion == \"Occ2\" ~ 0,\n    ),\n    \n    # dummy for FL condition\n    FL = case_when(\n      condition == \"FL\" ~ 1,\n      condition == \"MT\" ~ 0,\n      condition == \"NoSub\" ~ 0,\n    ),\n    \n    # dummy for MT\n    MT = case_when(\n      condition == \"MT\" ~ 1,\n      condition == \"FL\" ~ 0,\n      condition == \"NoSub\" ~ 0,\n    )\n  )\n\nloo_models <- readRDS(file = here(\"Integrated_exercise\",\"loo_models.RDS\"))\n\nprint(loo_models, simplify = F)\n```\n\nBased on the model comparison we can conclude that the final model (M3) fits the data best. We will use this model in the next sections to build our inferences.\n\n::: callout-note\n## Note\n\nWhen doing the loo comparison you might encounter a warning message saying that there is one or more observations showing a Pareto K higher than .7. What this means is that the leave-one-out cross-validation might be biased. The warning message suggest 'moment matching' as potential solution. You could try this. In my experience, the impact of one or two observations suffering a Pareto K value that is too high is rather small or even negligible. But it is always better to double check. Also, be aware that the moment matching solution creates a very slow estimation of the loo!\n:::\n\n------------------------------------------------------------------------\n\n# 2. Task 2\n\nNow that we have established the best fitting model, it is our task to approach the model critically before delving into the interpretation of the results.\n\nApply the different steps of the WAMBS check-list template for the final model.\n\n## Subtask 2.1: what about the priors?\n\nWhat are the default `brms` priors? Do they make sense? Do they generate impossible datasets? If necessary, specify your own (weakly informative) priors and approach them critically as well.\n\n------------------------------------------------------------------------\n\n<i> Potential Solution </i>\n\nLet's start with a prior predictive check. \n\n```{r}\n#| echo: true\n#| eval: false\n\nM3_priors <- brm(\n  fluency ~ 1 + Occ2*FL + Occ2*MT + Occ3*FL + Occ3*MT + (1|student),\n  data = Subtitles,\n  cores = 4,\n  backend = \"cmdstanr\",\n  seed = 1975,\n  sample_prior = \"only\"\n)\n\npp_check(M3_priors)\n```\n\nAs you might notice, you will get an error message saying that sampling from the priors is not possible. This is due to the fact that `brms` by default uses flat priors which generates this error message.\n\nTo get the priors used by `brms` we use the `get_prior()` command.\n\n```{r}\nget_prior(\n  fluency ~ 1 + Occ2*FL + Occ2*MT + Occ3*FL + Occ3*MT + (1|student),\n  data = Subtitles\n)\n```\n\nAs can be seen above, for all the beta's (fixed effects) `brms` uses a flat prior. Actually, that is something that is better avoided. More appropriate would be to come up with our own priors. Let's think about this. All the explanatory variables are dummy variables. So, they quantify differences between groups of observations (based on time or condition). \n\nAs we have no prior idea about the directions of the effects of condition nor of the effect of time, we could use a prior distribution centred around 0 (most probability assigned to no effect). \n\nNext, we have to think about setting the width of the prior. For instance, if we use a normal distribution to express our prior belief, we have to think about the sd for the normal distribution that captures our prior belief. In our case, the sd has to be high enough to assign some probability to very strong positive effects as well as to very strong negative effects. Here, it is important that we know our data well. I mean, we need to know the scale of our dependent variable (`fluency`). This variable has a standard deviation of `r round(sd(Subtitles$fluency, na.rm=T), 1)`. Now we can use *Effect Sizes* as a frame of reference to determine what a large positive and negative effect implies on the scale of the `fluency` variable. Remember, an effect size of 0.8 (or higher) indicates a strong effect (this is based on the *Cohen's d* rules of thumb). So, on our scale of the `fluency` variable an effect of 5.7 (= 7.1 * 0.8) indicates a strong effect. If we use the value 5.7 as our sd for priors for the effects of our dummy variables, this would imply that we think that the 95% most probable parameter values for the effect of the dummy variables would be situated between -11.4 (= -2 * 5.7) and 11.4 (= 2 * 5.7). Visually the prior would look like this:\n\n```{r}\n# Setting a plotting theme\n\nlibrary(ggplot2)\nlibrary(ggtext) # to be able to change the fonts etc in graphs\n\ntheme_set(theme_linedraw() +\n            theme(text = element_text(family = \"Times\", size = 8),\n                  panel.grid = element_blank(),\n                  plot.title = element_markdown())\n)\n\nPrior_betas <- ggplot( ) +\n  stat_function(\n    fun = dnorm,    # We use the normal distribution\n    args = list(mean = 0, sd = 5.7), # \n    xlim = c(-15,15)\n  ) +\n  scale_y_continuous(name = \"density\") +\n  labs(title = \"Prior for the effects of independent variables\",\n       subtitle = \"N(0,5.7)\")\n\nPrior_betas\n```\n\nNotice that even effects of -11 and 11 (almost effect sizes of -2 and 2) still get a decent amount of probability in our prior density function. \n\nLet's set these priors and try to apply a `pp_check()`. Notice that I set the priors for all slopes (`class = \"b\"`) at once.\n\n```{r}\n#| message: false\n#| error: false\n#| eval: false\n\n\nCustom_prior <- c(\n  set_prior(\n    \"normal(0,5.7)\",\n    class = \"b\"\n  )\n)\n\nM3_priors <- brm(\n  fluency ~ 1 + Occ2*FL + Occ2*MT + Occ3*FL + Occ3*MT + (1|student),\n  data = Subtitles,\n  cores = 4,\n  backend = \"cmdstanr\",\n  seed = 1975,\n  prior = Custom_prior,\n  sample_prior = \"only\"\n)\n\npp_check(M3_priors)\n```\n\n```{r}\n#| echo: false\n#| eval: true\n#| message: false\n\nlibrary(here)\n\nM3_priors <- readRDS(\n  here(\"Integrated_exercise\", \"M3_priors.RDS\")\n)\n\npp_check(M3_priors)\n```\n\n\nThe simulated data goes all the way (light blue lines)! But it doesn't generate extremely high or low observations and from this check we also learn that we have set quiet broad priors as they result in big differences between distributions of `fluency` based on the simulated datasets coming from our model with these priors.\n\nTime to apply these priors (that we somehow understand now) to estimate the real model.\n\n```{r}\n#| message: false\n#| error: false\n#| eval: false\n#| echo: true\n\nCustom_prior <- c(\n  set_prior(\n    \"normal(0,5.7)\",\n    class = \"b\"\n  )\n)\n\nM3 <- brm(\n  fluency ~ 1 + Occ2*FL + Occ2*MT + Occ3*FL + Occ3*MT + (1|student),\n  data = Subtitles,\n  cores = 4,\n  backend = \"cmdstanr\",\n  seed = 1975,\n  prior = Custom_prior\n  )\n```\n\n```{r}\n#| echo: false\n#| eval: true\n#| message: false\n\nlibrary(here)\n\nM3b <- readRDS(\n  here(\"Integrated_exercise\", \"M3b.RDS\")\n)\n```\n\n---\n\n## Subtask 2.2: did the model converge properly?\n\nPerform different checks on the convergence of the model.\n\n---\n\n<i> Possible solution </i>\n\nLet's start by checking the trace-plots.\n\n```{r}\n#| echo: true\n#| message: false\n#| error: false\n#| cache: false\n#| warning: false\n\nlibrary(ggmcmc)\n\nModel_chains <- ggs(M3b)\n\nModel_chains %>%\n  filter(Parameter %in% c(\n    \"b_Intercept\",\n    \"b_FL\", \n    \"b_MT\", \n    \"b_Occ2\", \n    \"b_Occ3\",\n    \"b_FL:Occ2\",\n    \"b_FL:Occ3\",\n    \"b_MT:Occ2\",\n    \"b_MT:Occ3\"\n    )\n  ) %>%\n  ggplot(aes(\n    x   = Iteration,\n    y   = value, \n    col = as.factor(Chain)))+\n  geom_line() +\n  facet_grid(Parameter ~ .,\n             scale  = 'free_y',\n             switch = 'y') +\n  labs(title = \"Caterpillar Plots for the parameters\",\n       col   = \"Chains\")\n```\n\nLooking at the trace-plots, we can conclude that all chains mixed very well. This is already a first indication of successful convergence.\n\nNext, we can check the R-hat statistic for each of the parameters. With the following plot we get a visual overview of all the R-hat statistics (notice the large number of parameters, because we also have random effects in our model):\n\n::: {.callout-note}\nIn the code below you can see that I first create a vector called `Rhats` to use in the `mcmc_rhat()` function. To create the vector I call the `brms::rhat()` function. By writing explicitly `brms::` before the `rhat()` I make sure that the `rhat()` function from the package `brms` is used. I do this to avoid the use of another `rhat()` function that might be loaded by activating other packages and that results in incompatible results with the `mcmc_rhat()` function.\n:::\n\n```{r}\n#| echo: true\n#| message: false\n#| error: false\n#| cache: false\n\nlibrary(brms)\nlibrary(bayesplot)\n\n# Extract posterior draws as array (robust method)\nRhats <- brms::rhat(M3b)\n\n# Plot with bayesplot\nmcmc_rhat(Rhats, size = 2) +\n  yaxis_text(hjust = 1)\n\n\n```\n\nNone of the parameters shows a high R-hat statistic. They all are below the threshold of 1.05, indicating that all parameters converged well.\n\nTime to get insight in the amount of autocorrelation. A first check is plotting the ratio of the number of *Effective Sample Sizes* to the *Total Sampel Sizes* for all the parameters. Remember that this ratio should be above 0.1 to be sure that the amount of autocorrelation is acceptable. Following code gives a visual overview of these ratios.\n\n```{r}\n#| echo: true\n#| message: false\n#| error: false\n#| cache: false\n\nmcmc_neff(\n  neff_ratio(M3b)\n  ) + \n  yaxis_text(hjust = 1)\n```\n\nFrom the plot we learn that for all the parameters the ratio is above 0.1. So, we can conclude that the amount of autocorrelation is not problematic for the estimation of any of the parameters.\n\n---\n\n## Subtask 2.3: does the posterior distribution histogram have enough information?\n\nCheck if the posterior distribution histograms of the different parameters are informative enough to substantiate our inferences.\n\n---\n\n<i> Possible solution </i>\n\nTo evaluate this, we create histograms based on the draws for our parameter values based on our model. We will apply this first for all main fixed effects.\n\n```{r}\n#| echo: true\n#| message: false\n#| error: false\n#| cache: false\n#| warning: false\n\nlibrary(patchwork)\n\nposterior_PD <- as_draws_df(M3b)\n\npost_intercept <- \n  posterior_PD %>%\n  select(b_Intercept) %>%\n  ggplot(aes(x = b_Intercept)) +\n  geom_histogram() +\n  ggtitle(\"Intercept\") \n\npost_Occ2 <- \n  posterior_PD %>%\n  select(b_Occ2) %>%\n  ggplot(aes(x = b_Occ2)) +\n  geom_histogram() +\n  ggtitle(\"Beta Occ2\") \n\npost_Occ3 <- \n  posterior_PD %>%\n  select(b_Occ3) %>%\n  ggplot(aes(x = b_Occ3)) +\n  geom_histogram() +\n  ggtitle(\"Beta Occ3\") \n\npost_FL <- \n  posterior_PD %>%\n  select(b_FL) %>%\n  ggplot(aes(x = b_FL)) +\n  geom_histogram() +\n  ggtitle(\"Beta FL\") \n\npost_MT <- \n  posterior_PD %>%\n  select(b_MT) %>%\n  ggplot(aes(x = b_MT)) +\n  geom_histogram() +\n  ggtitle(\"Beta MT\") \n\n\npost_intercept + post_Occ2 + post_Occ3 + post_FL + post_MT +\n  plot_layout(ncol = 3)\n```\n\nThese plots show clear slopes and a peak, indicating that the posterior is informative enough for each of these parameters.\n\nNow, let's do the same for the interaction effects.\n\n```{r}\n#| echo: true\n#| message: false\n#| error: false\n#| cache: false\n#| warning: false\n\npost_Occ2_FL <- \n  posterior_PD %>%\n  select(`b_Occ2:FL`) %>%\n  ggplot(aes(x = `b_Occ2:FL`)) +\n  geom_histogram() +\n  ggtitle(\"Beta Occ2:FL\") \n\npost_Occ2_MT <- \n  posterior_PD %>%\n  select(`b_Occ2:MT`) %>%\n  ggplot(aes(x = `b_Occ2:MT`)) +\n  geom_histogram() +\n  ggtitle(\"Beta Occ2:MT\") \n\npost_Occ3_FL <- \n  posterior_PD %>%\n  select(`b_FL:Occ3`) %>%\n  ggplot(aes(x = `b_FL:Occ3`)) +\n  geom_histogram() +\n  ggtitle(\"Beta Occ3:FL\") \n\npost_Occ3_MT <- \n  posterior_PD %>%\n  select(`b_MT:Occ3`) %>%\n  ggplot(aes(x = `b_MT:Occ3`)) +\n  geom_histogram() +\n  ggtitle(\"Beta Occ3:MT\") \n\npost_Occ2_FL + post_Occ2_MT + post_Occ3_FL + post_Occ3_MT +\n  plot_layout(ncol = 3)\n\n```\n\nHere the conclusion is the same. These histograms show no problematic cases.\n\n\n## Subtask 2.4: how well does the model predict the observed data?\n\nPerform posterior predictive checks based on the model.\n\n---\n\n<i> Possible solutions </i>\n\n```{r}\n#| echo: true\n#| message: false\n#| error: false\n#| cache: false\n\npp_check(M3b)\n```\n\nLooking at the posterior probability checks, we can see that the distributions of simulated data show a strong resemblance to the distribution of the observed data. So, the conclusion could be that our model is performing quiet well.\n\n## Subtask 2.5: what about prior sensitivity of the results?\n\nFinally, we have to check if the results of our model are not too dependent on the priors we specified in the model.\n\n```{r}\n#| echo: true\n#| message: false\n#| error: false\n#| cache: false\n\nlibrary(priorsense)\n\npowerscale_sensitivity(M3b)\n```\n\nWe learn from this prior sensitivity analyses that there is a *prior-data conflict*. \n\n::: callout-note\n## Note\n\nIf we delve into the paper on the `priorsense` package we can read this paragraph:\n\n\n>Prior-data conflict (Evans & Moshonov, 2006; Nott, Wang, et al., 2020; Walter & Augustin, 2009) can arise due to intentionally or unintentionally informative priors disagreeing with, but not being dominated by, the likelihood. When this is the case, the posterior will be sensitive to power-scaling both the prior and the likelihood, as illustrated in Figure 5. When prior-data conflict has been detected, the modeller may wish to modify the model by using a less informative prior (e.g., Evans & Jang, 2011; Nott, Seah, et al., 2020) or using heavy-tailed distributions (e.g., Gagnon, 2022; O’Hagan & Pericchi, 2012).\n\nReference:\nKallioinen, N., Paananen, T., Bürkner, P.-C., & Vehtari, A. (2023). Detecting and diagnosing prior and likelihood sensitivity with power-scaling. Statistics and Computing, 34(1), 57. https://doi.org/10.1007/s11222-023-10366-5\n\n:::\n\nSo, it might imply that our priors are \"unintentionally informative\". Let's see what happens if we would stick to the default priors of `brms`. Given that the object called `M3` contains the estimation of the model using the default priors we can quickly learn about the prior sensitivity of this model.\n\n```{r}\n#| echo: false\n#| eval: true\n#| message: false\n\n\nM3 <- readRDS(\n  here(\"Integrated_exercise\", \"M3.RDS\")\n)\n```\n\n```{r}\npowerscale_sensitivity(M3)\n```\n\nLooking at this prior sensitivity check we learn that using the default priors makes the model less sensitive to the priors used! \n\n\"What to do now?\"\n\nWell, in my opinion there is not a single best way to deal with this situation. What I would do first is compare the results based on both models (`M3` and `M3b`). Let's print the parameter estimates of both models. To do this I rely on the `tab_model()` function from the very helpful package `sjPlot` (so, do not forget to install that one if you want to use it!), as it allows me to show the results of both models side-to-side.\n\n```{r}\n#| echo: false\n#| eval: true\n#| message: false\nlibrary(sjPlot)\n\ntab_model(\n  M3,\n  M3b,\n  dv.labels= c(\"M3 (brms default priors)\", \"M3b (custom priors)\")\n  )\n\n```\n\nThis table clearly demonstrates how the model making use of the default priors has different results than the model with the priors we had set ourselves!\n\nIf we think about this more profoundly this doesn't have to come as a surprise! The priors used for the effect of Occasion was maybe nonsense. Why not assuming that students got more fluent over the time course of 26 weeks? To be fairly honest, I deliberately used these priors that seemed uninformative so that we would - for didactical reasons - run into this issue! \n\nFor the rest of the exercise we will stick to the results of the model with default priors. But, another option could be to change our custom priors, mimicking the idea that we expect students to become more fluent on the course of 26 weeks. That's what I actually did in a blogpost that I wrote about using the `priorsense` package that can be found here: [https://sdemaeyer.quarto.pub/posts/2024-02-PriorSense/PriorSensePost.html](https://sdemaeyer.quarto.pub/posts/2024-02-PriorSense/PriorSensePost.html)\n\n# 3. Task 3\n\nNow a more general task. Make different visualizations of the model results.\n\nOne of the possible visualizations could be a rather complex one. Remember, there are 3 conditions and 3 occasions. What I like to see is a plot showing the expected means for each of the conditions on each of the 3 occasions.\n\nAnd what do we learn about the progress between Occ1 and Occ2 in each of the groups? And what about the progress between Occ2 and Occ3?\n\n---\n\n\n\n<i> Possible solution </i>\n\nLet's start tackling this challenge: \n\n|\"One of the possible visualizations could be a rather complex one. Remember, there are 3 conditions and 3 occasions. What I like to see is a plot showing the expected means for each of the conditions on each of the 3 occasions.\"\n\nStarting point: create an object containing the draws from the posterior based on model M3.\n\n```{r}\n#| echo: true\n#| message: false\n#| error: false\n#| cache: false\n#| warning: false\n\nposterior_M3 <- as_draws_df(M3)\n\n```\n\nThe next step is calculating some predicted means, based on combinations of our dummy variables in the model. This is a 'tricky' step because we have to keep in mind that the model also contains interaction effects.\n\n```{r}\n#| echo: true\n#| message: false\n#| error: false\n#| cache: false\n#| warning: false\n\nposterior_M3 <- posterior_M3 %>%\n  mutate(\n    \n    # calculate expected mean for Occ1 condition NoSub (that's just our intercept)\n    Occ1_NoSub = b_Intercept,\n    # calculate expected mean for Occ1 condition FL (add main effect of FL)\n    Occ1_FL = b_Intercept + b_FL,\n    # calculate expected mean for Occ1 condition MT (add main effect of MT)\n    Occ1_MT = b_Intercept + b_MT,\n\n    # calculate expected mean for Occ2 condition NoSub (add main effect of Occ2)\n    Occ2_NoSub = b_Intercept + b_Occ2,\n    # calculate expected mean for Occ2 condition FL (add main effects and interaction term)\n    Occ2_FL = b_Intercept + b_Occ2 + b_FL + `b_Occ2:FL`,\n    # calculate expected mean for Occ2 condition MT (add main effects and interaction term)\n    Occ2_MT = b_Intercept + b_Occ2 + b_MT + `b_Occ2:MT`,\n\n    # calculate expected mean for Occ3 condition NoSub (add main effect of Occ2)\n    Occ3_NoSub = b_Intercept + b_Occ3,\n    # calculate expected mean for Occ3 condition FL (add main effects and interaction term)\n    Occ3_FL = b_Intercept + b_Occ3 + b_FL + `b_FL:Occ3`,\n    # calculate expected mean for Occ3 condition MT (add main effects and interaction term)\n    Occ3_MT = b_Intercept + b_Occ3 + b_MT + `b_MT:Occ3`\n\n  )\n```\n\nNow that we have calculated these estimated means, we can use these columns to create plots. What \n\n```{r}\n#| echo: true\n#| message: false\n#| error: false\n#| cache: false\n#| warning: false\n\nlibrary(ggplot2)\nlibrary(ggdist)\n\nposterior_M3 %>%\n\n  # First: select only the relevant columns of our posterior draws object\n  select(\n    Occ1_NoSub, Occ1_FL, Occ1_MT,\n    Occ2_NoSub, Occ2_FL, Occ2_MT,\n    Occ3_NoSub, Occ3_FL, Occ3_MT,\n  ) %>%\n  \n  # Second: pivot everything longer to get two columns\n  # column called `name` containing the name of the parameter\n  # calumn called `value` containing the value for that parameter in that draw\n  pivot_longer(everything()) %>%\n  \n  # Now create new variable that indicates the Occasion because I want to use that as X-axis\n  # And create new variable that indicates the group (NoSub, MT, or FL) to make groups in the plot\n  mutate(\n    Occasion = case_when(\n      name == \"Occ1_NoSub\" | name == \"Occ1_FL\" | name == \"Occ1_MT\" ~ \"Occ1\",\n      name == \"Occ2_NoSub\" | name == \"Occ2_FL\" | name == \"Occ2_MT\" ~ \"Occ2\",\n      name == \"Occ3_NoSub\" | name == \"Occ3_FL\" | name == \"Occ3_MT\" ~ \"Occ3\",\n    ),\n    Group = case_when(\n      name == \"Occ1_NoSub\" | name == \"Occ2_NoSub\" | name == \"Occ3_NoSub\" ~ \"No subtitles\",\n      name == \"Occ1_FL\"    | name == \"Occ2_FL\"    | name == \"Occ3_FL\"    ~ \"Foreign language\",\n      name == \"Occ1_MT\"    | name == \"Occ2_MT\"    | name == \"Occ3_MT\"    ~ \"Mother tongue\",\n    )\n  ) %>%\n  \n  # Time to create the plot\n  ggplot(\n    aes(x = Occasion, \n        y = value, \n        group = Group, \n        fill = Group)  # identify the experimental groups by the fill color\n  ) +\n  stat_halfeye(\n    .width = c(.5, .89),\n    alpha = .4 # make the fill color of the density plots more transparent\n  )\n\n```\n\nThe next challenge: \n\n|\"And what do we learn about the progress between Occ1 and Occ2 in each of the groups? And what about the progress between Occ2 and Occ3? Is progress between two occasions different in each of the conditions? \"\n\nThese questions can be compared with using tests on <i>estimated marginal means</i> and post-hoc testing by making use of contrasts in the frequentist realm. \n\nWe can first answer these questions by making visualisations of the posterior probability distributions for the estimated differences between occasions in each of the conditions. To make plots we rely again on our draws drawn from our posterior probability distributions. As a starting point we use the `posterior_M3b` object created above.\n\n```{r}\n#| echo: true\n#| message: false\n#| error: false\n#| cache: false\n#| warning: false\n\n# Start by creating difference scores between occasions based on draws\n\nposterior_M3 <- posterior_M3 %>%\n  mutate(\n\n    # Difference between occasions for condition NoSub\n    Occ2_Occ1_NoSub = Occ2_NoSub - Occ1_NoSub,\n    Occ3_Occ2_NoSub = Occ3_NoSub - Occ2_NoSub,\n\n    # Difference between occasions for condition FL\n    Occ2_Occ1_FL = Occ2_FL - Occ1_FL,\n    Occ3_Occ2_FL = Occ3_FL - Occ2_FL,\n\n    # Difference between occasions for condition MT\n    Occ2_Occ1_MT = Occ2_MT - Occ1_MT,\n    Occ3_Occ2_MT = Occ3_MT - Occ2_MT\n    \n  )\n\n# Create the visualisation\n\nposterior_M3 %>%\n\n  # select only the relevant columns for creating the plot\n  select(\n    Occ2_Occ1_NoSub, Occ3_Occ2_NoSub,\n    Occ2_Occ1_FL, Occ3_Occ2_FL,\n    Occ2_Occ1_MT, Occ3_Occ2_MT,\n  ) %>%\n  \n  # do the pivot_longer to create one vector of values and a vector of parameter names\n  pivot_longer(\n    everything( )\n    ) %>%\n\n # Create variables that help identify groups of parameters to color the distributions\n  mutate(\n    Occasion = case_when(\n      name == \"Occ2_Occ1_NoSub\" | name == \"Occ2_Occ1_FL\" | name == \"Occ2_Occ1_MT\" ~ \"Progress Occ1 Occ2\",\n      name == \"Occ3_Occ2_NoSub\" | name == \"Occ3_Occ2_FL\" | name == \"Occ3_Occ2_MT\" ~ \"Progress Occ2 Occ3\"),\n    Group = case_when(\n      name == \"Occ2_Occ1_NoSub\" | name == \"Occ3_Occ2_NoSub\"  ~ \"No subtitles\",\n      name == \"Occ2_Occ1_FL\" | name == \"Occ3_Occ2_FL\"  ~ \"Foreign language\",\n      name == \"Occ2_Occ1_MT\" | name == \"Occ3_Occ2_MT\"  ~ \"Mother tongue\",\n    )\n  ) %>%\n\n  ggplot(\n    aes(\n      x = value,\n      y = Group\n    )\n  ) + \n\n  # Let's plot interval visualisations\n  \n  stat_pointinterval(\n    .width = c(.5, .89)  \n    ) +\n  scale_y_discrete(name = \"\") +           # Drop the label of the y-scale\n  scale_x_continuous(name = \"Fluency\") +  # Name the x-scale\n  facet_wrap(.~Occasion)\n  \n```\n\nFrom this visualisation we learn that the progress between occasion 1 and 2 is rather similar in both subtitle groups (intervals strongly overlap). The students in the condition without subtitles make a smaller progress between occasion 1 and occasion 2 than the students in both subtitling conditions.\n\nFocussing on the progress between occasion 2 and 3, we see that the progress of the students in the foreign language subtitling condition stands out from the progress made in the two other conditions.\n\n```{r}\n#| echo: true\n#| message: false\n#| error: false\n#| cache: false\n#| warning: false\nhypothesis(\n  posterior_M3,\n  hypothesis = \"Occ2_Occ1_MT > Occ2_Occ1_FL\"\n)\n```\n\nIf we wrap this `hypothesis()` part within a `plot()` function, we get a visualization of the posterior probability distribution of the difference in progress for the two subtitling conditions.\n\n```{r}\n#| echo: true\n#| message: false\n#| error: false\n#| cache: false\n#| warning: false\n\nplot(\n  hypothesis(\n    posterior_M3,\n    hypothesis = \"Occ2_Occ1_MT > Occ2_Occ1_FL\"\n    )\n)\n```\n\nThis visualization shows that a great amount of credible estimates of this difference in progress between both subtitling conditions is situated around zero, with high probabilities for both a negative and positive difference. In other words, most of the evidence shows that we are not sure to state whether both subtitling conditions differ in the progress in fluency they induced between occasion 1 and occasion 2.\n\nNow, let's focus on the difference between occasion 2 and occasion 3: how much evidence is there that this progress is stronger in the condition of foreign language subtitling compared to the condition of mother tongue subtitling?\n\n```{r}\n#| echo: true\n#| message: false\n#| error: false\n#| cache: false\n#| warning: false\n\nhypothesis(\n  posterior_M3,\n  hypothesis = \"Occ3_Occ2_FL > Occ3_Occ2_MT\"\n)\n\nplot(\n hypothesis(\n  posterior_M3,\n  hypothesis = \"Occ3_Occ2_FL > Occ3_Occ2_MT\"\n ) \n)\n```\n\nHere we can see that we have a lot of supporting evidence to conclude that the improvement between occasion 2 and 3 in fluency is stronger for the students in the foreign language condition than for the students in the mother tongue condition.\n\n# References\n","srcMarkdownNoYaml":"\n\n```{r}\n#| message: false\n#| warning: false\n#| error: false\n#| echo: false\nlibrary(here)\n\n```\n\n```{r setup, include=FALSE}\n.libPaths(\"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library\")\nlibrary(rstan)\n```\n\n![](work_in_progress.png){fig-align=\"center\" width=\"25%\"}\n\n# Background behind the dataset\n\nThis dataset is a simulated dataset that is based on an existing study of @frumuselu2015. In this study, the key question was whether subtitles help in foreign language acquisition. Spanish students (n = 36) watched episodes of the popular tv-show \"Friends\" for half an hour each week, during 26 weeks. The students were assigned to 3 conditions:\n\n-   English subtitled (condition \"FL\")\n\n-   Spanish subtitled (condition \"MT\")\n\n-   No subtitles (condition \"NoSub\")\n\nAt 3 occasions students got a Fluency test:\n\n-   Before the 26 weeks started\n\n-   After 12 weeks\n\n-   After the experiment\n\nThe dependent variable is a measure based on the number of words used in a scripted spontaneous interview with a test taker. The data is structured as follows:\n\n```{r}\nload(file = here(\"Integrated_exercise\",\"Subtitles.RData\"))\nhead(Subtitles, 9)\n```\n\nIf we visualize the dataset we get a first impression of the effect of the condition. In this exercise it is your task to do the proper Bayesian modelling and interpretation.\n\n```{r}\n#| message: false\n#| warning: false\n#| error: false\n\nlibrary(tidyverse)\n\ntheme_set(theme_linedraw() +\n            theme(text = element_text(family = \"Times\", size = 12),\n                  panel.grid = element_blank())\n)\n\nSubtitles %>%\n  ggplot(\n    aes(\n      x = occasion,\n      y = fluency,\n      group = student\n      )\n  ) +\n  geom_path(\n    aes(\n      color = condition\n    )\n  ) \n```\n\n------------------------------------------------------------------------\n\n```{r, out.height = \"40%\", out.width=\"40%\", echo = FALSE}\nknitr::include_graphics(here(\"Integrated_exercise\",\"friends3.jpeg\"))\n```\n\n------------------------------------------------------------------------\n\n# 1. Task 1\n\nFirst we will start by building 4 alternative mixed effects models. In each of the models we have to take into account the fact that we have multiple observations per student. So we need a random effect for students in the model.\n\n-   M0: only an intercept and a random effect for `student`\n\n-   M1: M0 + fixed effect of `occasion`\n\n-   M2: M1 + fixed effect of `condition`\n\n-   M3: M2 + interaction effect between `occasion` and `condition`\n\nMake use of the default priors of `brms`.\n\nOnce the models are estimated, compare the models on their fit making use of the leave-one-out cross-validation. Determine which model fits best and will be used to build our inferences.\n\n------------------------------------------------------------------------\n\n<i>Solution</i>\n\nThe following code-block shows how the models can be estimated and compared on their fit. Notice how I first create a set of dummy-variables for the categories of the `occasion` and the `condition` variables. This makes it a bit harder (more code writing) to define my model but it generates some flexibility later on. For instance, when thinking about setting priors, I can set priors for each of the effects of these dummy variables. Also, it will result in shorter names of parameters in my model in the resulting objects for the model.\n\n```{r}\n#| echo: true\n#| eval: false\n#| message: false\n#| error: false\n#| cache: false\n\nlibrary(brms)\nlibrary(tidyverse)\n\nSubtitles <- Subtitles %>%\n  mutate(\n\n    # dummy for Occ2\n    Occ2 = case_when(\n      occasion == \"Occ2\" ~ 1,\n      occasion == \"Occ1\" ~ 0,\n      occasion == \"Occ3\" ~ 0,\n    ),\n    \n    # dummy for Occ3\n    Occ3 = case_when(\n      occasion == \"Occ3\" ~ 1,\n      occasion == \"Occ1\" ~ 0,\n      occasion == \"Occ2\" ~ 0,\n    ),\n    \n    # dummy for FL condition\n    FL = case_when(\n      condition == \"FL\" ~ 1,\n      condition == \"MT\" ~ 0,\n      condition == \"NoSub\" ~ 0,\n    ),\n    \n    # dummy for MT\n    MT = case_when(\n      condition == \"MT\" ~ 1,\n      condition == \"FL\" ~ 0,\n      condition == \"NoSub\" ~ 0,\n    )\n  )\n\n# Estimate the models\n\nM0 <- brm(\n  fluency ~ 1 + (1|student),\n  data = Subtitles,\n  cores = 4,\n  backend = \"cmdstanr\",\n  seed = 1975\n)\n\nM1 <- brm(\n  fluency ~ 1 + Occ2 + Occ3 + (1|student),\n  data = Subtitles,\n  cores = 4,\n  backend = \"cmdstanr\",\n  seed = 1975\n)\n\nM2 <- brm(\n  fluency ~ 1 + Occ2 + Occ3 + FL + MT + (1|student),\n  data = Subtitles,\n  cores = 4,\n  backend = \"cmdstanr\",\n  seed = 1975\n)\n\nM3 <- brm(\n  fluency ~ 1 + Occ2*FL + Occ2*MT + Occ3*FL + Occ3*MT + (1|student),\n  data = Subtitles,\n  cores = 4,\n  backend = \"cmdstanr\",\n  seed = 1975\n)\n\n# loo cross-validation of the models\n\nloo_M0 <- loo(M0)\nloo_M1 <- loo(M1)\nloo_M2 <- loo(M2)\nloo_M3 <- loo(M3)\n\nloo_models <- loo_compare(\n  loo_M0,\n  loo_M1,\n  loo_M2,\n  loo_M3\n)\n\nprint(loo_models, simplify = F)\n\n```\n\n```{r}\n#| echo: false\n#| eval: true\nlibrary(brms)\nlibrary(tidyverse)\n\nSubtitles <- Subtitles %>%\n  mutate(\n\n    # dummy for Occ2\n    Occ2 = case_when(\n      occasion == \"Occ2\" ~ 1,\n      occasion == \"Occ1\" ~ 0,\n      occasion == \"Occ3\" ~ 0,\n    ),\n    \n    # dummy for Occ3\n    Occ3 = case_when(\n      occasion == \"Occ3\" ~ 1,\n      occasion == \"Occ1\" ~ 0,\n      occasion == \"Occ2\" ~ 0,\n    ),\n    \n    # dummy for FL condition\n    FL = case_when(\n      condition == \"FL\" ~ 1,\n      condition == \"MT\" ~ 0,\n      condition == \"NoSub\" ~ 0,\n    ),\n    \n    # dummy for MT\n    MT = case_when(\n      condition == \"MT\" ~ 1,\n      condition == \"FL\" ~ 0,\n      condition == \"NoSub\" ~ 0,\n    )\n  )\n\nloo_models <- readRDS(file = here(\"Integrated_exercise\",\"loo_models.RDS\"))\n\nprint(loo_models, simplify = F)\n```\n\nBased on the model comparison we can conclude that the final model (M3) fits the data best. We will use this model in the next sections to build our inferences.\n\n::: callout-note\n## Note\n\nWhen doing the loo comparison you might encounter a warning message saying that there is one or more observations showing a Pareto K higher than .7. What this means is that the leave-one-out cross-validation might be biased. The warning message suggest 'moment matching' as potential solution. You could try this. In my experience, the impact of one or two observations suffering a Pareto K value that is too high is rather small or even negligible. But it is always better to double check. Also, be aware that the moment matching solution creates a very slow estimation of the loo!\n:::\n\n------------------------------------------------------------------------\n\n# 2. Task 2\n\nNow that we have established the best fitting model, it is our task to approach the model critically before delving into the interpretation of the results.\n\nApply the different steps of the WAMBS check-list template for the final model.\n\n## Subtask 2.1: what about the priors?\n\nWhat are the default `brms` priors? Do they make sense? Do they generate impossible datasets? If necessary, specify your own (weakly informative) priors and approach them critically as well.\n\n------------------------------------------------------------------------\n\n<i> Potential Solution </i>\n\nLet's start with a prior predictive check. \n\n```{r}\n#| echo: true\n#| eval: false\n\nM3_priors <- brm(\n  fluency ~ 1 + Occ2*FL + Occ2*MT + Occ3*FL + Occ3*MT + (1|student),\n  data = Subtitles,\n  cores = 4,\n  backend = \"cmdstanr\",\n  seed = 1975,\n  sample_prior = \"only\"\n)\n\npp_check(M3_priors)\n```\n\nAs you might notice, you will get an error message saying that sampling from the priors is not possible. This is due to the fact that `brms` by default uses flat priors which generates this error message.\n\nTo get the priors used by `brms` we use the `get_prior()` command.\n\n```{r}\nget_prior(\n  fluency ~ 1 + Occ2*FL + Occ2*MT + Occ3*FL + Occ3*MT + (1|student),\n  data = Subtitles\n)\n```\n\nAs can be seen above, for all the beta's (fixed effects) `brms` uses a flat prior. Actually, that is something that is better avoided. More appropriate would be to come up with our own priors. Let's think about this. All the explanatory variables are dummy variables. So, they quantify differences between groups of observations (based on time or condition). \n\nAs we have no prior idea about the directions of the effects of condition nor of the effect of time, we could use a prior distribution centred around 0 (most probability assigned to no effect). \n\nNext, we have to think about setting the width of the prior. For instance, if we use a normal distribution to express our prior belief, we have to think about the sd for the normal distribution that captures our prior belief. In our case, the sd has to be high enough to assign some probability to very strong positive effects as well as to very strong negative effects. Here, it is important that we know our data well. I mean, we need to know the scale of our dependent variable (`fluency`). This variable has a standard deviation of `r round(sd(Subtitles$fluency, na.rm=T), 1)`. Now we can use *Effect Sizes* as a frame of reference to determine what a large positive and negative effect implies on the scale of the `fluency` variable. Remember, an effect size of 0.8 (or higher) indicates a strong effect (this is based on the *Cohen's d* rules of thumb). So, on our scale of the `fluency` variable an effect of 5.7 (= 7.1 * 0.8) indicates a strong effect. If we use the value 5.7 as our sd for priors for the effects of our dummy variables, this would imply that we think that the 95% most probable parameter values for the effect of the dummy variables would be situated between -11.4 (= -2 * 5.7) and 11.4 (= 2 * 5.7). Visually the prior would look like this:\n\n```{r}\n# Setting a plotting theme\n\nlibrary(ggplot2)\nlibrary(ggtext) # to be able to change the fonts etc in graphs\n\ntheme_set(theme_linedraw() +\n            theme(text = element_text(family = \"Times\", size = 8),\n                  panel.grid = element_blank(),\n                  plot.title = element_markdown())\n)\n\nPrior_betas <- ggplot( ) +\n  stat_function(\n    fun = dnorm,    # We use the normal distribution\n    args = list(mean = 0, sd = 5.7), # \n    xlim = c(-15,15)\n  ) +\n  scale_y_continuous(name = \"density\") +\n  labs(title = \"Prior for the effects of independent variables\",\n       subtitle = \"N(0,5.7)\")\n\nPrior_betas\n```\n\nNotice that even effects of -11 and 11 (almost effect sizes of -2 and 2) still get a decent amount of probability in our prior density function. \n\nLet's set these priors and try to apply a `pp_check()`. Notice that I set the priors for all slopes (`class = \"b\"`) at once.\n\n```{r}\n#| message: false\n#| error: false\n#| eval: false\n\n\nCustom_prior <- c(\n  set_prior(\n    \"normal(0,5.7)\",\n    class = \"b\"\n  )\n)\n\nM3_priors <- brm(\n  fluency ~ 1 + Occ2*FL + Occ2*MT + Occ3*FL + Occ3*MT + (1|student),\n  data = Subtitles,\n  cores = 4,\n  backend = \"cmdstanr\",\n  seed = 1975,\n  prior = Custom_prior,\n  sample_prior = \"only\"\n)\n\npp_check(M3_priors)\n```\n\n```{r}\n#| echo: false\n#| eval: true\n#| message: false\n\nlibrary(here)\n\nM3_priors <- readRDS(\n  here(\"Integrated_exercise\", \"M3_priors.RDS\")\n)\n\npp_check(M3_priors)\n```\n\n\nThe simulated data goes all the way (light blue lines)! But it doesn't generate extremely high or low observations and from this check we also learn that we have set quiet broad priors as they result in big differences between distributions of `fluency` based on the simulated datasets coming from our model with these priors.\n\nTime to apply these priors (that we somehow understand now) to estimate the real model.\n\n```{r}\n#| message: false\n#| error: false\n#| eval: false\n#| echo: true\n\nCustom_prior <- c(\n  set_prior(\n    \"normal(0,5.7)\",\n    class = \"b\"\n  )\n)\n\nM3 <- brm(\n  fluency ~ 1 + Occ2*FL + Occ2*MT + Occ3*FL + Occ3*MT + (1|student),\n  data = Subtitles,\n  cores = 4,\n  backend = \"cmdstanr\",\n  seed = 1975,\n  prior = Custom_prior\n  )\n```\n\n```{r}\n#| echo: false\n#| eval: true\n#| message: false\n\nlibrary(here)\n\nM3b <- readRDS(\n  here(\"Integrated_exercise\", \"M3b.RDS\")\n)\n```\n\n---\n\n## Subtask 2.2: did the model converge properly?\n\nPerform different checks on the convergence of the model.\n\n---\n\n<i> Possible solution </i>\n\nLet's start by checking the trace-plots.\n\n```{r}\n#| echo: true\n#| message: false\n#| error: false\n#| cache: false\n#| warning: false\n\nlibrary(ggmcmc)\n\nModel_chains <- ggs(M3b)\n\nModel_chains %>%\n  filter(Parameter %in% c(\n    \"b_Intercept\",\n    \"b_FL\", \n    \"b_MT\", \n    \"b_Occ2\", \n    \"b_Occ3\",\n    \"b_FL:Occ2\",\n    \"b_FL:Occ3\",\n    \"b_MT:Occ2\",\n    \"b_MT:Occ3\"\n    )\n  ) %>%\n  ggplot(aes(\n    x   = Iteration,\n    y   = value, \n    col = as.factor(Chain)))+\n  geom_line() +\n  facet_grid(Parameter ~ .,\n             scale  = 'free_y',\n             switch = 'y') +\n  labs(title = \"Caterpillar Plots for the parameters\",\n       col   = \"Chains\")\n```\n\nLooking at the trace-plots, we can conclude that all chains mixed very well. This is already a first indication of successful convergence.\n\nNext, we can check the R-hat statistic for each of the parameters. With the following plot we get a visual overview of all the R-hat statistics (notice the large number of parameters, because we also have random effects in our model):\n\n::: {.callout-note}\nIn the code below you can see that I first create a vector called `Rhats` to use in the `mcmc_rhat()` function. To create the vector I call the `brms::rhat()` function. By writing explicitly `brms::` before the `rhat()` I make sure that the `rhat()` function from the package `brms` is used. I do this to avoid the use of another `rhat()` function that might be loaded by activating other packages and that results in incompatible results with the `mcmc_rhat()` function.\n:::\n\n```{r}\n#| echo: true\n#| message: false\n#| error: false\n#| cache: false\n\nlibrary(brms)\nlibrary(bayesplot)\n\n# Extract posterior draws as array (robust method)\nRhats <- brms::rhat(M3b)\n\n# Plot with bayesplot\nmcmc_rhat(Rhats, size = 2) +\n  yaxis_text(hjust = 1)\n\n\n```\n\nNone of the parameters shows a high R-hat statistic. They all are below the threshold of 1.05, indicating that all parameters converged well.\n\nTime to get insight in the amount of autocorrelation. A first check is plotting the ratio of the number of *Effective Sample Sizes* to the *Total Sampel Sizes* for all the parameters. Remember that this ratio should be above 0.1 to be sure that the amount of autocorrelation is acceptable. Following code gives a visual overview of these ratios.\n\n```{r}\n#| echo: true\n#| message: false\n#| error: false\n#| cache: false\n\nmcmc_neff(\n  neff_ratio(M3b)\n  ) + \n  yaxis_text(hjust = 1)\n```\n\nFrom the plot we learn that for all the parameters the ratio is above 0.1. So, we can conclude that the amount of autocorrelation is not problematic for the estimation of any of the parameters.\n\n---\n\n## Subtask 2.3: does the posterior distribution histogram have enough information?\n\nCheck if the posterior distribution histograms of the different parameters are informative enough to substantiate our inferences.\n\n---\n\n<i> Possible solution </i>\n\nTo evaluate this, we create histograms based on the draws for our parameter values based on our model. We will apply this first for all main fixed effects.\n\n```{r}\n#| echo: true\n#| message: false\n#| error: false\n#| cache: false\n#| warning: false\n\nlibrary(patchwork)\n\nposterior_PD <- as_draws_df(M3b)\n\npost_intercept <- \n  posterior_PD %>%\n  select(b_Intercept) %>%\n  ggplot(aes(x = b_Intercept)) +\n  geom_histogram() +\n  ggtitle(\"Intercept\") \n\npost_Occ2 <- \n  posterior_PD %>%\n  select(b_Occ2) %>%\n  ggplot(aes(x = b_Occ2)) +\n  geom_histogram() +\n  ggtitle(\"Beta Occ2\") \n\npost_Occ3 <- \n  posterior_PD %>%\n  select(b_Occ3) %>%\n  ggplot(aes(x = b_Occ3)) +\n  geom_histogram() +\n  ggtitle(\"Beta Occ3\") \n\npost_FL <- \n  posterior_PD %>%\n  select(b_FL) %>%\n  ggplot(aes(x = b_FL)) +\n  geom_histogram() +\n  ggtitle(\"Beta FL\") \n\npost_MT <- \n  posterior_PD %>%\n  select(b_MT) %>%\n  ggplot(aes(x = b_MT)) +\n  geom_histogram() +\n  ggtitle(\"Beta MT\") \n\n\npost_intercept + post_Occ2 + post_Occ3 + post_FL + post_MT +\n  plot_layout(ncol = 3)\n```\n\nThese plots show clear slopes and a peak, indicating that the posterior is informative enough for each of these parameters.\n\nNow, let's do the same for the interaction effects.\n\n```{r}\n#| echo: true\n#| message: false\n#| error: false\n#| cache: false\n#| warning: false\n\npost_Occ2_FL <- \n  posterior_PD %>%\n  select(`b_Occ2:FL`) %>%\n  ggplot(aes(x = `b_Occ2:FL`)) +\n  geom_histogram() +\n  ggtitle(\"Beta Occ2:FL\") \n\npost_Occ2_MT <- \n  posterior_PD %>%\n  select(`b_Occ2:MT`) %>%\n  ggplot(aes(x = `b_Occ2:MT`)) +\n  geom_histogram() +\n  ggtitle(\"Beta Occ2:MT\") \n\npost_Occ3_FL <- \n  posterior_PD %>%\n  select(`b_FL:Occ3`) %>%\n  ggplot(aes(x = `b_FL:Occ3`)) +\n  geom_histogram() +\n  ggtitle(\"Beta Occ3:FL\") \n\npost_Occ3_MT <- \n  posterior_PD %>%\n  select(`b_MT:Occ3`) %>%\n  ggplot(aes(x = `b_MT:Occ3`)) +\n  geom_histogram() +\n  ggtitle(\"Beta Occ3:MT\") \n\npost_Occ2_FL + post_Occ2_MT + post_Occ3_FL + post_Occ3_MT +\n  plot_layout(ncol = 3)\n\n```\n\nHere the conclusion is the same. These histograms show no problematic cases.\n\n\n## Subtask 2.4: how well does the model predict the observed data?\n\nPerform posterior predictive checks based on the model.\n\n---\n\n<i> Possible solutions </i>\n\n```{r}\n#| echo: true\n#| message: false\n#| error: false\n#| cache: false\n\npp_check(M3b)\n```\n\nLooking at the posterior probability checks, we can see that the distributions of simulated data show a strong resemblance to the distribution of the observed data. So, the conclusion could be that our model is performing quiet well.\n\n## Subtask 2.5: what about prior sensitivity of the results?\n\nFinally, we have to check if the results of our model are not too dependent on the priors we specified in the model.\n\n```{r}\n#| echo: true\n#| message: false\n#| error: false\n#| cache: false\n\nlibrary(priorsense)\n\npowerscale_sensitivity(M3b)\n```\n\nWe learn from this prior sensitivity analyses that there is a *prior-data conflict*. \n\n::: callout-note\n## Note\n\nIf we delve into the paper on the `priorsense` package we can read this paragraph:\n\n\n>Prior-data conflict (Evans & Moshonov, 2006; Nott, Wang, et al., 2020; Walter & Augustin, 2009) can arise due to intentionally or unintentionally informative priors disagreeing with, but not being dominated by, the likelihood. When this is the case, the posterior will be sensitive to power-scaling both the prior and the likelihood, as illustrated in Figure 5. When prior-data conflict has been detected, the modeller may wish to modify the model by using a less informative prior (e.g., Evans & Jang, 2011; Nott, Seah, et al., 2020) or using heavy-tailed distributions (e.g., Gagnon, 2022; O’Hagan & Pericchi, 2012).\n\nReference:\nKallioinen, N., Paananen, T., Bürkner, P.-C., & Vehtari, A. (2023). Detecting and diagnosing prior and likelihood sensitivity with power-scaling. Statistics and Computing, 34(1), 57. https://doi.org/10.1007/s11222-023-10366-5\n\n:::\n\nSo, it might imply that our priors are \"unintentionally informative\". Let's see what happens if we would stick to the default priors of `brms`. Given that the object called `M3` contains the estimation of the model using the default priors we can quickly learn about the prior sensitivity of this model.\n\n```{r}\n#| echo: false\n#| eval: true\n#| message: false\n\n\nM3 <- readRDS(\n  here(\"Integrated_exercise\", \"M3.RDS\")\n)\n```\n\n```{r}\npowerscale_sensitivity(M3)\n```\n\nLooking at this prior sensitivity check we learn that using the default priors makes the model less sensitive to the priors used! \n\n\"What to do now?\"\n\nWell, in my opinion there is not a single best way to deal with this situation. What I would do first is compare the results based on both models (`M3` and `M3b`). Let's print the parameter estimates of both models. To do this I rely on the `tab_model()` function from the very helpful package `sjPlot` (so, do not forget to install that one if you want to use it!), as it allows me to show the results of both models side-to-side.\n\n```{r}\n#| echo: false\n#| eval: true\n#| message: false\nlibrary(sjPlot)\n\ntab_model(\n  M3,\n  M3b,\n  dv.labels= c(\"M3 (brms default priors)\", \"M3b (custom priors)\")\n  )\n\n```\n\nThis table clearly demonstrates how the model making use of the default priors has different results than the model with the priors we had set ourselves!\n\nIf we think about this more profoundly this doesn't have to come as a surprise! The priors used for the effect of Occasion was maybe nonsense. Why not assuming that students got more fluent over the time course of 26 weeks? To be fairly honest, I deliberately used these priors that seemed uninformative so that we would - for didactical reasons - run into this issue! \n\nFor the rest of the exercise we will stick to the results of the model with default priors. But, another option could be to change our custom priors, mimicking the idea that we expect students to become more fluent on the course of 26 weeks. That's what I actually did in a blogpost that I wrote about using the `priorsense` package that can be found here: [https://sdemaeyer.quarto.pub/posts/2024-02-PriorSense/PriorSensePost.html](https://sdemaeyer.quarto.pub/posts/2024-02-PriorSense/PriorSensePost.html)\n\n# 3. Task 3\n\nNow a more general task. Make different visualizations of the model results.\n\nOne of the possible visualizations could be a rather complex one. Remember, there are 3 conditions and 3 occasions. What I like to see is a plot showing the expected means for each of the conditions on each of the 3 occasions.\n\nAnd what do we learn about the progress between Occ1 and Occ2 in each of the groups? And what about the progress between Occ2 and Occ3?\n\n---\n\n\n\n<i> Possible solution </i>\n\nLet's start tackling this challenge: \n\n|\"One of the possible visualizations could be a rather complex one. Remember, there are 3 conditions and 3 occasions. What I like to see is a plot showing the expected means for each of the conditions on each of the 3 occasions.\"\n\nStarting point: create an object containing the draws from the posterior based on model M3.\n\n```{r}\n#| echo: true\n#| message: false\n#| error: false\n#| cache: false\n#| warning: false\n\nposterior_M3 <- as_draws_df(M3)\n\n```\n\nThe next step is calculating some predicted means, based on combinations of our dummy variables in the model. This is a 'tricky' step because we have to keep in mind that the model also contains interaction effects.\n\n```{r}\n#| echo: true\n#| message: false\n#| error: false\n#| cache: false\n#| warning: false\n\nposterior_M3 <- posterior_M3 %>%\n  mutate(\n    \n    # calculate expected mean for Occ1 condition NoSub (that's just our intercept)\n    Occ1_NoSub = b_Intercept,\n    # calculate expected mean for Occ1 condition FL (add main effect of FL)\n    Occ1_FL = b_Intercept + b_FL,\n    # calculate expected mean for Occ1 condition MT (add main effect of MT)\n    Occ1_MT = b_Intercept + b_MT,\n\n    # calculate expected mean for Occ2 condition NoSub (add main effect of Occ2)\n    Occ2_NoSub = b_Intercept + b_Occ2,\n    # calculate expected mean for Occ2 condition FL (add main effects and interaction term)\n    Occ2_FL = b_Intercept + b_Occ2 + b_FL + `b_Occ2:FL`,\n    # calculate expected mean for Occ2 condition MT (add main effects and interaction term)\n    Occ2_MT = b_Intercept + b_Occ2 + b_MT + `b_Occ2:MT`,\n\n    # calculate expected mean for Occ3 condition NoSub (add main effect of Occ2)\n    Occ3_NoSub = b_Intercept + b_Occ3,\n    # calculate expected mean for Occ3 condition FL (add main effects and interaction term)\n    Occ3_FL = b_Intercept + b_Occ3 + b_FL + `b_FL:Occ3`,\n    # calculate expected mean for Occ3 condition MT (add main effects and interaction term)\n    Occ3_MT = b_Intercept + b_Occ3 + b_MT + `b_MT:Occ3`\n\n  )\n```\n\nNow that we have calculated these estimated means, we can use these columns to create plots. What \n\n```{r}\n#| echo: true\n#| message: false\n#| error: false\n#| cache: false\n#| warning: false\n\nlibrary(ggplot2)\nlibrary(ggdist)\n\nposterior_M3 %>%\n\n  # First: select only the relevant columns of our posterior draws object\n  select(\n    Occ1_NoSub, Occ1_FL, Occ1_MT,\n    Occ2_NoSub, Occ2_FL, Occ2_MT,\n    Occ3_NoSub, Occ3_FL, Occ3_MT,\n  ) %>%\n  \n  # Second: pivot everything longer to get two columns\n  # column called `name` containing the name of the parameter\n  # calumn called `value` containing the value for that parameter in that draw\n  pivot_longer(everything()) %>%\n  \n  # Now create new variable that indicates the Occasion because I want to use that as X-axis\n  # And create new variable that indicates the group (NoSub, MT, or FL) to make groups in the plot\n  mutate(\n    Occasion = case_when(\n      name == \"Occ1_NoSub\" | name == \"Occ1_FL\" | name == \"Occ1_MT\" ~ \"Occ1\",\n      name == \"Occ2_NoSub\" | name == \"Occ2_FL\" | name == \"Occ2_MT\" ~ \"Occ2\",\n      name == \"Occ3_NoSub\" | name == \"Occ3_FL\" | name == \"Occ3_MT\" ~ \"Occ3\",\n    ),\n    Group = case_when(\n      name == \"Occ1_NoSub\" | name == \"Occ2_NoSub\" | name == \"Occ3_NoSub\" ~ \"No subtitles\",\n      name == \"Occ1_FL\"    | name == \"Occ2_FL\"    | name == \"Occ3_FL\"    ~ \"Foreign language\",\n      name == \"Occ1_MT\"    | name == \"Occ2_MT\"    | name == \"Occ3_MT\"    ~ \"Mother tongue\",\n    )\n  ) %>%\n  \n  # Time to create the plot\n  ggplot(\n    aes(x = Occasion, \n        y = value, \n        group = Group, \n        fill = Group)  # identify the experimental groups by the fill color\n  ) +\n  stat_halfeye(\n    .width = c(.5, .89),\n    alpha = .4 # make the fill color of the density plots more transparent\n  )\n\n```\n\nThe next challenge: \n\n|\"And what do we learn about the progress between Occ1 and Occ2 in each of the groups? And what about the progress between Occ2 and Occ3? Is progress between two occasions different in each of the conditions? \"\n\nThese questions can be compared with using tests on <i>estimated marginal means</i> and post-hoc testing by making use of contrasts in the frequentist realm. \n\nWe can first answer these questions by making visualisations of the posterior probability distributions for the estimated differences between occasions in each of the conditions. To make plots we rely again on our draws drawn from our posterior probability distributions. As a starting point we use the `posterior_M3b` object created above.\n\n```{r}\n#| echo: true\n#| message: false\n#| error: false\n#| cache: false\n#| warning: false\n\n# Start by creating difference scores between occasions based on draws\n\nposterior_M3 <- posterior_M3 %>%\n  mutate(\n\n    # Difference between occasions for condition NoSub\n    Occ2_Occ1_NoSub = Occ2_NoSub - Occ1_NoSub,\n    Occ3_Occ2_NoSub = Occ3_NoSub - Occ2_NoSub,\n\n    # Difference between occasions for condition FL\n    Occ2_Occ1_FL = Occ2_FL - Occ1_FL,\n    Occ3_Occ2_FL = Occ3_FL - Occ2_FL,\n\n    # Difference between occasions for condition MT\n    Occ2_Occ1_MT = Occ2_MT - Occ1_MT,\n    Occ3_Occ2_MT = Occ3_MT - Occ2_MT\n    \n  )\n\n# Create the visualisation\n\nposterior_M3 %>%\n\n  # select only the relevant columns for creating the plot\n  select(\n    Occ2_Occ1_NoSub, Occ3_Occ2_NoSub,\n    Occ2_Occ1_FL, Occ3_Occ2_FL,\n    Occ2_Occ1_MT, Occ3_Occ2_MT,\n  ) %>%\n  \n  # do the pivot_longer to create one vector of values and a vector of parameter names\n  pivot_longer(\n    everything( )\n    ) %>%\n\n # Create variables that help identify groups of parameters to color the distributions\n  mutate(\n    Occasion = case_when(\n      name == \"Occ2_Occ1_NoSub\" | name == \"Occ2_Occ1_FL\" | name == \"Occ2_Occ1_MT\" ~ \"Progress Occ1 Occ2\",\n      name == \"Occ3_Occ2_NoSub\" | name == \"Occ3_Occ2_FL\" | name == \"Occ3_Occ2_MT\" ~ \"Progress Occ2 Occ3\"),\n    Group = case_when(\n      name == \"Occ2_Occ1_NoSub\" | name == \"Occ3_Occ2_NoSub\"  ~ \"No subtitles\",\n      name == \"Occ2_Occ1_FL\" | name == \"Occ3_Occ2_FL\"  ~ \"Foreign language\",\n      name == \"Occ2_Occ1_MT\" | name == \"Occ3_Occ2_MT\"  ~ \"Mother tongue\",\n    )\n  ) %>%\n\n  ggplot(\n    aes(\n      x = value,\n      y = Group\n    )\n  ) + \n\n  # Let's plot interval visualisations\n  \n  stat_pointinterval(\n    .width = c(.5, .89)  \n    ) +\n  scale_y_discrete(name = \"\") +           # Drop the label of the y-scale\n  scale_x_continuous(name = \"Fluency\") +  # Name the x-scale\n  facet_wrap(.~Occasion)\n  \n```\n\nFrom this visualisation we learn that the progress between occasion 1 and 2 is rather similar in both subtitle groups (intervals strongly overlap). The students in the condition without subtitles make a smaller progress between occasion 1 and occasion 2 than the students in both subtitling conditions.\n\nFocussing on the progress between occasion 2 and 3, we see that the progress of the students in the foreign language subtitling condition stands out from the progress made in the two other conditions.\n\n```{r}\n#| echo: true\n#| message: false\n#| error: false\n#| cache: false\n#| warning: false\nhypothesis(\n  posterior_M3,\n  hypothesis = \"Occ2_Occ1_MT > Occ2_Occ1_FL\"\n)\n```\n\nIf we wrap this `hypothesis()` part within a `plot()` function, we get a visualization of the posterior probability distribution of the difference in progress for the two subtitling conditions.\n\n```{r}\n#| echo: true\n#| message: false\n#| error: false\n#| cache: false\n#| warning: false\n\nplot(\n  hypothesis(\n    posterior_M3,\n    hypothesis = \"Occ2_Occ1_MT > Occ2_Occ1_FL\"\n    )\n)\n```\n\nThis visualization shows that a great amount of credible estimates of this difference in progress between both subtitling conditions is situated around zero, with high probabilities for both a negative and positive difference. In other words, most of the evidence shows that we are not sure to state whether both subtitling conditions differ in the progress in fluency they induced between occasion 1 and occasion 2.\n\nNow, let's focus on the difference between occasion 2 and occasion 3: how much evidence is there that this progress is stronger in the condition of foreign language subtitling compared to the condition of mother tongue subtitling?\n\n```{r}\n#| echo: true\n#| message: false\n#| error: false\n#| cache: false\n#| warning: false\n\nhypothesis(\n  posterior_M3,\n  hypothesis = \"Occ3_Occ2_FL > Occ3_Occ2_MT\"\n)\n\nplot(\n hypothesis(\n  posterior_M3,\n  hypothesis = \"Occ3_Occ2_FL > Occ3_Occ2_MT\"\n ) \n)\n```\n\nHere we can see that we have a lot of supporting evidence to conclude that the improvement between occasion 2 and 3 in fluency is stronger for the students in the foreign language condition than for the students in the mother tongue condition.\n\n# References\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"self-contained":true,"output-file":"Exercise_response.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.39","extensions":["r-wasm/quarto-live"],"theme":["cosmo","../theme.scss"],"mainfont":"Atkinson Hyperlegible","code-copy":true,"title":"Integrated excercise","bibliography":["references.bib"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}