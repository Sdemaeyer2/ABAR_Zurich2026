{"title":"Applied Bayesian Analyses in R","markdown":{"yaml":{"title":"Applied Bayesian Analyses in R","subtitle":"Part2","author":"Sven De Maeyer","format":{"revealjs":{"theme":["simple","My_theme.scss"],"width":1422,"height":805,"slide-number":true}},"self-contained":true,"execute":{"echo":false,"include":true,"output":true},"code":{"code-copy":true,"code-line-numbers":true},"code-annotations":"hover"},"headingText":"Setting a plotting theme","containsRefs":false,"markdown":"\n\n```{r}\nlibrary(here)\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(bayesplot)\nlibrary(ggmcmc)\nlibrary(patchwork)\nlibrary(priorsense)\n\nload(\n  file = here(\"Presentations\", \"MarathonData.RData\")\n)\n\nMarathonTimes_Mod2 <-\n  readRDS(file = \n            here(\"Presentations\",\n              \"Output\",\n              \"MarathonTimes_Mod2.RDS\")\n          )\n\ntheme_set(theme_linedraw() +\n            theme(text = element_text(family = \"Times\", size = 10),\n                  panel.grid = element_blank())\n)\n```\n\n## `r fontawesome::fa(\"laptop-code\", \"white\")` Your Turn {background-color=\"#447099\" transition=\"slide-in\"}\n\n-   Open `MarathonData.RData`\n-   Estimate your first Bayesian Models\n-   Dependent variable: `MarathonTimeM`\n-   Model1: only an intercept\n-   Model2: introduce the effect of `km4week` and `sp4week` on `MarathonTimeM`\n-   Change the `brms` defaults (4 chains of 6000 iterations)\n-   Make plots with the `plot()` function\n-   What do we learn? (rough interpretation)\n\n::: aside\n[Note: I centred both]{style=\"color: white\"} `km4week`[and]{style=\"color: white\"} `sp4week` [around their mean!]{style=\"color: white\"}\n:::\n\n## Results Exercise\n\n*Model1: only an intercept (brms defaults)*\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\n\nload(file = here(\n    \"Presentations\",\n    \"MarathonData.RData\")\n    )\n\nMod_MT1 <- brm(                        \n  MarathonTimeM ~ 1, \n  data = MarathonData,   \n  backend = \"cmdstanr\",\n  seed = 1975           \n)\n```\n\n## Results Exercise\n\n*Model1: only an intercept (brms defaults)*\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\nsummary(Mod_MT1)\n```\n\n## Results Exercise\n\n*Model1: only an intercept (brms defaults)*\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\nplot(Mod_MT1)\n```\n\n## Results Exercise\n\n*Model2: add the effect of* `km4week` *and* `sp4week`\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\n\nMod_MT2 <- brm(                        \n  MarathonTimeM ~ 1 + km4week + sp4week, \n  data = MarathonData,   \n  backend = \"cmdstanr\",\n  seed = 1975           \n)\n```\n\n## Results Exercise\n\n*Model2: add the effect of* `km4week` *and* `sp4week`\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\nsummary(Mod_MT2)\n```\n\n## Results Exercise\n\n*Model2: add the effect of* `km4week` *and* `sp4week`\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\nplot(Mod_MT2)\n```\n\n## Results Exercise\n\n*Model3: change the brms defaults*\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\n\nMod_MT3 <- brm(                        \n  MarathonTimeM ~ 1 + km4week + sp4week, \n  data = MarathonData,\n  chains = 4,\n  iter = 4000,\n  cores = 4,\n  backend = \"cmdstanr\",\n  seed = 1975           \n)\n```\n\n## Results Exercise\n\n*Model3: change the brms defaults*\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\nsummary(Mod_MT3)\n```\n\n## Results Exercise\n\n*Model3: change the brms defaults*\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\nplot(Mod_MT3)\n```\n\n## About convergence\n\n::::: columns\n::: {.column width=\"50%\"}\n```{r, out.height = \"99%\", out.width=\"99%\", echo = FALSE}\nknitr::include_graphics(\"Vethari_paper.jpg\")\n```\n:::\n\n::: {.column width=\"50%\"}\n-   $\\widehat R$ \\< 1.015 for each parameter estimate\n\n-   at least 4 chains are recommended\n\n-   Effective Sample Size (ESS) \\> 400 to rely on $\\widehat R$\n:::\n:::::\n\n## But is it a good model?\n\n<br> <br> Two complementary procedures: <br> <br>\n\n-   posterior-predictive check\n\n-   compare models with <i>[leave one out cross-validation]{style=\"color:#447099\"}</i>\n\n## Posterior-predictive check\n\nA visual check that can be performed with `pp_check()` from `brms`\n\n```{r, fig.width = 5, fig.height = 5}\n#| echo: true\n#| output-location: slide\n\npp_check(Mod_MT2) + theme_minimal()\n```\n\n## Model comparison with loo cross-validation\n\n$\\sim$ AIC or BIC in Frequentist statistics\n\n$\\widehat{elpd}$: \"expected log predictive density\" (higher $\\widehat{elpd}$ implies better model fit without being sensitive for overfitting!)\n\n```{r}\nMarathonTimes_Mod1 <-\n  readRDS(file = \n            here(\"Presentations\",\n              \"Output\",\n              \"MarathonTimes_Mod1.RDS\")\n          )\n```\n\n```{r}\n#| eval: true\n#| echo: true\n#| output-location: slide\n\nloo_Mod1 <- loo(MarathonTimes_Mod1)\nloo_Mod2 <- loo(MarathonTimes_Mod2)\n\nComparison<- \n  loo_compare(\n    loo_Mod1, \n    loo_Mod2\n    )\n\nprint(Comparison, simplify = F)\n```\n\n# WAMBS checklist\n\n```{css echo=FALSE}\n.small-code{\n  font-size: 75%  \n}\n```\n\n## When to Worry and How to Avoid Misuse of Bayesian Statistics {.smaller}\n\nby Laurent Smeets and Rens van der Schoot\n\n:::::: columns\n::: {.column width=\"33%\"}\nBefore estimating the model:\n\n1.  Do you understand the priors?\n:::\n\n::: {.column width=\"33%\"}\nAfter estimation before inspecting results:\n\n2.  Does the trace-plot exhibit convergence?\n3.  Does convergence remain after doubling the number of iterations?\n4.  Does the posterior distribution histogram have enough information?\n5.  Do the chains exhibit a strong degree of autocorrelation?\n6.  Do the posterior distributions make substantive sense?\n:::\n\n::: {.column width=\"33%\"}\nUnderstanding the exact influence of the priors\n\n7.  Do different specification of the multivariate variance priors influence the results?\n8.  Is there a notable effect of the prior when compared with non-informative priors?\n9.  Are the results stable from a sensitivity analysis?\n10. Is the Bayesian way of interpreting and reporting model results used?\n:::\n::::::\n\n::: aside\nTutorial source: <https://www.rensvandeschoot.com/brms-wambs/>\nAlternatives exist as well like the BARG framework (Kruschke, J.K. Bayesian Analysis Reporting Guidelines. Nat Hum Behav 5, 1282–1291 (2021). https://doi.org/10.1038/s41562-021-01177-7)\n:::\n\n## WAMBS Template to use\n\n-   File called [WAMBS_workflow_MarathonData.qmd]{style=\"color: #447099\"} (quarto document)\n\n-   <a href=\"https://abar-geneva-2024.netlify.app/WAMBS_template/WAMBS_workflow_MarathonData.qmd\" target=\"blank\">Click here </a> for the Quarto version\n\n-   Create your own project and project folder\n\n-   Copy the template and rename it\n\n-   We will go through the different parts in the slide show\n\n-   You can apply/adapt the code in the template\n\n-   To render the document properly with references, you also need the <a href=\"https://abar-geneva-2024.netlify.app/WAMBS_template/references.bib\" target=\"blank\">references.bib file </a>\n\n## Side-path: projects in RStudio and the `here` package\n\nIf you do not know how to use Projects in RStudio or the `here` package, these two sources might be helpfull:\n\nProjects: <https://youtu.be/MdTtTN8PUqU?si=mmQGlU063EMt86B2>\n\n`here` package: <https://youtu.be/oh3b3k5uM7E?si=0-heLJXfFVLtTohh>\n\n## Preparations for applying it to Marathon model\n\nPackages needed:\n\n```{r}\n#| echo: true\n#| eval: false\n\nlibrary(here)\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(bayesplot)\nlibrary(ggmcmc)\nlibrary(patchwork)\nlibrary(priorsense)\n```\n\n## Preparations for applying it to Marathon model\n\nLoad the dataset and the model:\n\n```{r}\n#| echo: true\n#| eval: false\nload(\n  file = here(\"Presentations\", \"MarathonData.RData\")\n)\n\nMarathonTimes_Mod2 <-\n  readRDS(file = \n            here(\"Presentations\",\n              \"Output\",\n              \"MarathonTimes_Mod2.RDS\")\n          )\n```\n\n# Focus on the [priors]{style=\"color: #447099\"} before estimation\n\n## Remember: priors come in many disguises\n\n::::: columns\n::: {.column width=\"50%\"}\n[Uninformative/Weakly informative]{style=\"color: #447099\"}\n\nWhen objectivity is crucial and you want *let the data speak for itself...*\n:::\n\n::: {.column width=\"50%\"}\n[Informative]{style=\"color: #447099\"}\n\nWhen including significant information is crucial\n\n-   previously collected data\n-   results from former research/analyses\n-   data of another source\n-   theoretical considerations\n-   elicitation\n:::\n:::::\n\n## `brms` defaults\n\n-   Weakly informative priors\n\n-   If dataset is big, impact of priors is minimal\n\n-   But, always better to know what you are doing!\n\n-   Complex models might run into convergence issues $\\rightarrow$ specifying more informative priors might help!\n\nSo, how to deviate from the defaults?\n\n## Check priors used by `brms`\n\nFunction: `get_prior( )`\n\nRemember our model 2 for Marathon Times:\n\n$$\\begin{aligned}\n& \\text{MarathonTimeM}_i \\sim N(\\mu,\\sigma_e)\\\\\n& \\mu = \\beta_0 + \\beta_1*\\text{km4week}_i + \\beta_2*\\text{sp4week}_i \n\\end{aligned}$$\n\n```{r}\n#| echo: true\n#| eval: false\n\nget_prior(\n  MarathonTimeM ~ 1 + km4week + sp4week, \n  data = MarathonData\n)\n```\n\n## Check priors used by `brms`\n\n```{r, out.height = \"70%\", out.width=\"70%\", echo = FALSE}\nknitr::include_graphics(\"Priors_Mod2.jpg\")\n```\n\n-   `prior`: type of prior distribution\n-   `class`: parameter class (with `b` being population-effects)\n-   `coef`: name of the coefficient within parameter class\n-   `group`: grouping factor for group-level parameters (when using mixed effects models)\n-   `resp` : name of the response variable when using multivariate models\n-   `lb` & `ub`: lower and upper bound for parameter restriction\n\n## Visualizing priors\n\nThe best way to make sense of the priors used is visualizing them!\n\nMany options:\n\n-   The Zoo of Distributions <https://ben18785.shinyapps.io/distribution-zoo/>\n-   making your own visualizations\n\nSee WAMBS template!\n\nThere we demonstrate the use of `ggplot2`, `metRology`, `ggtext` and `patchwork` to visualize the priors.\n\n## Visualizing priors\n\n```{r}\n#| message: false\n#| warning: false\n#| error: false\n#| echo: true\n#| output-location: slide\n#| fig-cap: \"Probability density plots for the different priors used in the example model\"\n#| fig-cap-location: margin\n#| cache: true\n\nlibrary(metRology)\nlibrary(ggplot2)\nlibrary(ggtext)\nlibrary(patchwork)\n\n# Setting a plotting theme\ntheme_set(theme_linedraw() +\n            theme(text = element_text(family = \"Times\", size = 8),\n                  panel.grid = element_blank(),\n                  plot.title = element_markdown())\n)\n\n# Generate the plot for the prior of the Intercept (mu)\nPrior_mu <- ggplot( ) +\n  stat_function(\n    fun = dt.scaled,    # We use the dt.scaled function of metRology\n    args = list(df = 3, mean = 199.2, sd = 24.9), # \n    xlim = c(120,300)\n  ) +\n  scale_y_continuous(name = \"density\") +\n  labs(title = \"Prior for the intercept\",\n       subtitle = \"student_t(3,199.2,24.9)\")\n\n# Generate the plot for the prior of the error variance (sigma)\nPrior_sigma <- ggplot( ) +\n  stat_function(\n    fun = dt.scaled,    # We use the dt.scaled function of metRology\n    args = list(df = 3, mean = 0, sd = 24.9), # \n    xlim = c(0,6)\n  ) +\n  scale_y_continuous(name = \"density\") +\n  labs(title = \"Prior for the residual variance\",\n       subtitle = \"student_t(3,0,24.9)\")\n\n# Generate the plot for the prior of the effects of independent variables\nPrior_betas <- ggplot( ) +\n  stat_function(\n    fun = dnorm,    # We use the normal distribution\n    args = list(mean = 0, sd = 10), # \n    xlim = c(-20,20)\n  ) +\n  scale_y_continuous(name = \"density\") +\n  labs(title = \"Prior for the effects of independent variables\",\n       subtitle = \"N(0,10)\")\n\nPrior_mu + Prior_sigma + Prior_betas +\n  plot_layout(ncol = 3)\n```\n\n## Understanding priors... another example\n\nExperimental study (pretest - posttest design) with 3 conditions:\n\n-   control group;\n-   experimental group 1;\n-   experimental group 2.\n\nModel:\n\n$$\\begin{aligned}\n  & Posttest_{i}  \\sim N(\\mu,\\sigma_{e_{i}})\\\\\n  & \\mu = \\beta_0 + \\beta_1*\\text{Pretest}_{i} + \\beta_2*\\text{Exp_cond1}_{i} + \\beta_3*\\text{Exp_cond2}_{i}\n\\end{aligned}$$\n\nOur job: coming up with priors that reflect that we expect both conditions to have a positive effect (hypothesis based on literature) and no indications that one experimental condition will outperform the other.\n\n## Understanding priors... another example\n\n-   Assuming pre- and posttest scores are standardized\n-   Assuming no increase between pre- and posttest in control condition\n\n```{r}\n# Generate the plot for the prior of the Intercept (mu)\nPrior_mu <- ggplot( ) +\n  stat_function(\n    fun = dnorm,    # We use a normal distribution\n    args = list(mean = 0, sd = .5), # \n    xlim = c(-1,1)\n  ) +\n  scale_y_continuous(name = \"density\") +\n  labs(title = \"Prior for the intercept\",\n       subtitle = \"N(0,0.5)\")\n\nPrior_mu\n```\n\n## Understanding priors... another example\n\n-   Assuming a strong correlation between pre- and posttest\n\n```{r}\n# Generate the plot for the prior of beta 1\nPrior_beta1 <- ggplot( ) +\n  stat_function(\n    fun = dnorm,    # We use the dt.scaled function of metRology\n    args = list(mean = 1, sd = 0.5), # \n    xlim = c(-1,3)\n  ) +\n  scale_y_continuous(name = \"density\") +\n  labs(title = \"Prior for the effect of pretest score\",\n       subtitle = \"N(1,0.5)\")\n\nPrior_beta1\n```\n\n## Understanding priors... another example\n\n-   Assuming a small effect of experimental conditions\n-   No difference between both experimental conditions\n\n```{r}\n# Generate the plot for the prior of the effects experimental conditions\nPrior_betas <- ggplot( ) +\n  stat_function(\n    fun = dnorm,    # We use the normal distribution\n    args = list(mean = 0.2, sd = .6), # \n    xlim = c(-1,2)\n  ) +\n  scale_y_continuous(name = \"density\") +\n  labs(title = \"Prior for the effects of experimental conditions\",\n       subtitle = \"N(0.2,0.6)\")\n\nPrior_betas\n```\n\nRemember Cohen's d: 0.2 = small effect size; 0.5 = medium effect size; 0.8 or higher = large effect size\n\n## Setting custom priors in `brms`\n\n<br>\n\nSetting our custom priors can be done with `set_prior( )` command\n\n<br>\n\nE.g., change the priors for the beta's (effects of `km4week` and `sp4week`):\n\n<br>\n\n```{r}\n#| message: false\n#| warning: false\n#| error: false\n#| echo: true\n#| cache: true\n\n\nCustom_priors <- \n  c(\n    set_prior(\n      \"normal(0,10)\", \n      class = \"b\", \n      coef = \"km4week\"),\n    set_prior(\n      \"normal(0,10)\", \n      class = \"b\", \n      coef = \"sp4week\")\n    )\n\n```\n\n## Prior Predictive Check\n\n<br>\n\nDid you set sensible priors?\n\n<br>\n\n-   Simulate data based on the model and the priors\n\n<br>\n\n-   Visualize the simulated data and compare with real data\n\n<br>\n\n-   Check if the plot shows impossible simulated datasets\n\n## Prior Predictive Check in `brms`\n\n<br>\n\nStep 1: Fit the model with custom priors with option `sample_prior=\"only\"`\n\n<br>\n\n```{r}\n#| message: false\n#| warning: false\n#| error: false\n#| echo: true\n#| eval: false\n#| cache: true\n#| code-line-numbers: \"5|8\"\nFit_Model_priors <- \n  brm(\n    MarathonTimeM ~ 1 + km4week + sp4week, \n    data = MarathonData,\n    prior = Custom_priors,\n    backend = \"cmdstanr\",\n    cores = 4,\n    sample_prior = \"only\"\n    )\n```\n\n```{r}\nFit_Model_priors <- readRDS(\n  here(\n    \"Presentations\",\n    \"Output\",\n    \"Fit_Model_priors.RDS\"\n  )\n)\n```\n\n## Prior Predictive Check in `brms`\n\n<br>\n\nStep 2: visualize the data with the `pp_check( )` function\n\n<br>\n\n```{r}\n#| echo: true\n#| output-location: slide\n\nset.seed(1975)\n\npp_check(\n  Fit_Model_priors, \n  ndraws = 300) # number of simulated datasets you wish for\n\n```\n\n## Check some summary statistics\n\n-   How are summary statistics of simulated datasets (e.g., median, min, max, ...) distributed over the datasets?\n\n-   How does that compare to our real data?\n\n-   Use `type = \"stat\"` argument within `pp_check()`\n\n```{r}\n#| echo: true\n#| output-location: slide\npp_check(Fit_Model_priors, \n         type = \"stat\", \n         stat = \"median\")\n```\n\n## `r fontawesome::fa(\"laptop-code\", \"white\")` Your Turn {background-color=\"#447099\" transition=\"slide-in\"}\n\n-   Your data and model\n\n-   Perform a prior predictive check\n\n-   If necessary re-think your priors and check again\n\n# Focus on convergence of the model (before interpreting the model!)\n\n## Does the trace-plot exhibits convergence?\n\n<br>\n\nCreate custom trace-plots (aka caterpillar plots) with `ggs( )` function from `ggmcmc` package\n\n```{r}\n#| fig-cap: \"Caterpillar plots for the parameters in the model\"\n#| fig-cap-location: margin\n#| output-location: slide\n#| cache: true\n#| echo: true\n#| warning: false\nModel_chains <- ggs(MarathonTimes_Mod2)\n\nModel_chains %>%\n  filter(Parameter %in% c(\n          \"b_Intercept\", \n          \"b_km4week\", \n          \"b_sp4week\", \n          \"sigma\"\n          )\n  ) %>%\n  ggplot(aes(\n    x   = Iteration,\n    y   = value, \n    col = as.factor(Chain)))+\n  geom_line() +\n  facet_grid(Parameter ~ .,\n             scale  = 'free_y',\n             switch = 'y') +\n  labs(title = \"Caterpillar Plots for the parameters\",\n       col   = \"Chains\")\n```\n\n## Does convergence remain after doubling the number of iterations?\n\n<br>\n\nRe-fit the model with more iterations\n\n<br>\n\nCheck trace-plots again\n\n<br>\n\n::: callout-warning\nFirst consider the need to do this! If you have a complex model that already took a long time to run, this check will take at least twice as much time...\n:::\n\n## `r fontawesome::fa(\"laptop-code\", \"white\")` Your Turn {background-color=\"#447099\" transition=\"slide-in\"}\n\n-   Your data and model\n-   Do the first checks on the model convergence\n\n## R-hat statistics\n\nSampling of parameters done by:\n\n-   multiple chains\n-   multiple iterations within chains\n\nIf variance between chains is big $\\rightarrow$ NO CONVERGENCE\n\nR-hat ($\\widehat{R}$) : compares the between- and within-chain estimates for model parameters\n\n## R-hat statistics\n\n::::: columns\n::: {.column width=\"50%\"}\n```{r, out.height = \"95%\", out.width=\"95%\", echo = FALSE}\nknitr::include_graphics(\"Vethari_paper.jpg\")\n```\n:::\n\n::: {.column width=\"50%\"}\n-   $\\widehat{R}$ \\< 1.015 for each parameter estimate\n\n-   at least 4 chains are recommended\n\n-   Effective Sample Size (ESS) \\> 400 to rely on $\\widehat{R}$\n:::\n:::::\n\n## R-hat in `brms`\n\n`mcmc_rhat()` function from the `bayesplot` package\n\n```{r}\n#| echo: true\n#| output-location: slide\n\nmcmc_rhat(\n  brms::rhat(MarathonTimes_Mod2), \n  size = 3\n  )+ \n  yaxis_text(hjust = 1)  # to print parameter names\n```\n\n## `r fontawesome::fa(\"laptop-code\", \"white\")` Your Turn {background-color=\"#447099\" transition=\"slide-in\"}\n\n-   Your data and model\n\n-   Check the R-hat statistics\n\n## Autocorrelation\n\n-   Sampling of parameter values are not independent!\n\n-   So there is autocorrelation\n\n-   But you don't want too much impact of autocorrelation\n\n-   2 approaches to check this:\n\n    -   ratio of the effective sample size to the total sample size\n    -   plot degree of autocorrelation\n\n## Ratio effective sample size / total sample size\n\n-   Should be higher than 0.1 (Gelman et al., 2013)\n\n-   Visualize making use of the `mcmc_neff( )` function from `bayesplot`\n\n```{r}\n#| echo: true\n#| output-location: slide\nmcmc_neff(\n  neff_ratio(MarathonTimes_Mod2)\n  ) + \n  yaxis_text(hjust = 1)  # to print parameter names\n```\n\n## Plot degree of autocorrelation\n\n-   Visualize making use of the `mcmc_acf( )` function\n\n```{r}\n#| echo: true\n#| output-location: slide\nmcmc_acf(\n  as.array(MarathonTimes_Mod2), \n  regex = \"b\") # to plot only the parameters starting with b (our beta's)\n```\n\n## `r fontawesome::fa(\"laptop-code\", \"white\")` Your Turn {background-color=\"#447099\" transition=\"slide-in\"}\n\n-   Your data and model\n\n-   Check the autocorrelation\n\n## Rank order plots\n\n-   additional way to assess the convergence of MCMC\n\n-   if the algorithm converged, plots of all chains look similar\n\n```{r}\n#| echo: true\n#| output-location: slide\n\nmcmc_rank_hist(\n  MarathonTimes_Mod2, \n  regex = \"b\" # only intercept and beta's\n  ) \n```\n\n## `r fontawesome::fa(\"laptop-code\", \"white\")` Your Turn {background-color=\"#447099\" transition=\"slide-in\"}\n\n-   Your data and model\n\n-   Check the rank order plots\n\n# Focus on the Posterior\n\n## Does the posterior distribution histogram have enough information?\n\n-   Histogram of posterior for each parameter\n\n-   Have clear peak and sliding slopes\n\n## Plotting the posterior distribution histogram\n\n<br>\n\nStep 1: create a new object with 'draws' based on the final model\n\n<br>\n\n```{r}\n#| echo: true\nposterior_PD <- as_draws_df(MarathonTimes_Mod2)\n```\n\n## Plotting the posterior distribution histogram\n\n<br>\n\nStep 2: create histogram making use of that object\n\n<br>\n\n```{r}\n#| echo: true\n\npost_intercept <- \n  posterior_PD %>%\n  select(b_Intercept) %>%\n  ggplot(aes(x = b_Intercept)) +\n  geom_histogram() +\n  ggtitle(\"Intercept\") \n\npost_km4week <- \n  posterior_PD %>%\n  select(b_km4week) %>%\n  ggplot(aes(x = b_km4week)) +\n  geom_histogram() +\n  ggtitle(\"Beta km4week\") \n\npost_sp4week <- \n  posterior_PD %>%\n  select(b_sp4week) %>%\n  ggplot(aes(x = b_sp4week)) +\n  geom_histogram() +\n  ggtitle(\"Beta sp4week\") \n```\n\n## Plotting the posterior distribution histogram\n\n<br>\n\nStep 3: print the plot making use of `patchwork` 's workflow to combine plots <br>\n\n```{r}\n#| echo: true\n#| output-location: slide\npost_intercept + post_km4week + post_sp4week +\n  plot_layout(ncol = 3)\n```\n\n## Posterior Predictive Check\n\n-   Generate data based on the posterior probability distribution\n\n-   Create plot of distribution of y-values in these simulated datasets\n\n-   Overlay with distribution of observed data\n\nusing `pp_check()` again, now with our model\n\n```{r}\n#| echo: true\n#| output-location: slide\npp_check(MarathonTimes_Mod2, \n         ndraws = 100)\n```\n\n## Posterior Predictive Check\n\n-   We can also focus on some summary statistics (like we did with prior predictive checks as well)\n\n```{r}\n#| echo: true\n#| message: false\n#| warning: false\n#| output-location: slide\npp_check(MarathonTimes_Mod2, \n         ndraws = 300,\n         type = \"stat\",\n         stat = \"median\")\n```\n\n## `r fontawesome::fa(\"laptop-code\", \"white\")` Your Turn {background-color=\"#447099\" transition=\"slide-in\"}\n\n-   Your data and model\n\n-   Focus on the posterior and do some checks!\n\n# Prior sensibility analyses\n\n## Why prior sensibility analyses?\n\n-   Often we rely on 'arbitrary' chosen (default) weakly informative priors\n\n-   What is the influence of the prior (and the likelihood) on our results?\n\n-   You could ad hoc set new priors and re-run the analyses and compare (a lot of work, without strict sytematical guidelines)\n\n-   Semi-automated checks can be done with `priorsense` package\n\n## Using the `priorsense` package\n\nRecently a package dedicated to prior sensibility analyses is launched\n\n```{r}\n#| eval: false\n#| echo: true\n# install.packages(\"remotes\")\nremotes::install_github(\"n-kall/priorsense\")\n```\n\nKey-idea: power-scaling (both prior and likelihood)\n\nbackground reading:\n\n-   <https://arxiv.org/pdf/2107.14054.pdf>\n\nYouTube talk:\n\n-   <https://www.youtube.com/watch?v=TBXD3HjcIps&t=920s>\n\n## Basic table with indices\n\nFirst check is done by using the `powerscale_sensitivity( )` function\n\n-   column prior contains info on sensibility for prior (should be lower than 0.05)\n\n-   column likelihood contains info on sensibility for likelihood (that we want to be high, 'let our data speak')\n\n-   column diagnosis is a verbalization of potential problem (- if none)\n\n```{r}\n#| echo: true\n#| output-location: slide\npowerscale_sensitivity(MarathonTimes_Mod2)\n```\n\n## Visualization of prior sensibility\n\n```{r}\n#| echo: true\n#| warning: false\n#| message: false\n#| cache: true\n#| output-location: slide\n\npowerscale_plot_dens(\n  powerscale_sequence(\n    MarathonTimes_Mod2\n    ),\n  variable = c(\n      \"b_Intercept\",\n      \"b_km4week\",\n      \"b_sp4week\"\n    )\n  )\n```\n\n## Visualization of prior sensibility\n\n```{r}\n#| echo: true\n#| warning: false\n#| message: false\n#| cache: true\n#| output-location: slide\n\npowerscale_plot_quantities(\n  powerscale_sequence(\n    MarathonTimes_Mod2\n    ),\n  variable = c(\n      \"b_km4week\"\n      )\n  )\n```\n\n## `r fontawesome::fa(\"laptop-code\", \"white\")` Your Turn {background-color=\"#447099\" transition=\"slide-in\"}\n\n-   Your data and model\n\n-   Check the prior sensibility of your results","srcMarkdownNoYaml":"\n\n```{r}\nlibrary(here)\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(bayesplot)\nlibrary(ggmcmc)\nlibrary(patchwork)\nlibrary(priorsense)\n\nload(\n  file = here(\"Presentations\", \"MarathonData.RData\")\n)\n\nMarathonTimes_Mod2 <-\n  readRDS(file = \n            here(\"Presentations\",\n              \"Output\",\n              \"MarathonTimes_Mod2.RDS\")\n          )\n\n# Setting a plotting theme\ntheme_set(theme_linedraw() +\n            theme(text = element_text(family = \"Times\", size = 10),\n                  panel.grid = element_blank())\n)\n```\n\n## `r fontawesome::fa(\"laptop-code\", \"white\")` Your Turn {background-color=\"#447099\" transition=\"slide-in\"}\n\n-   Open `MarathonData.RData`\n-   Estimate your first Bayesian Models\n-   Dependent variable: `MarathonTimeM`\n-   Model1: only an intercept\n-   Model2: introduce the effect of `km4week` and `sp4week` on `MarathonTimeM`\n-   Change the `brms` defaults (4 chains of 6000 iterations)\n-   Make plots with the `plot()` function\n-   What do we learn? (rough interpretation)\n\n::: aside\n[Note: I centred both]{style=\"color: white\"} `km4week`[and]{style=\"color: white\"} `sp4week` [around their mean!]{style=\"color: white\"}\n:::\n\n## Results Exercise\n\n*Model1: only an intercept (brms defaults)*\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\n\nload(file = here(\n    \"Presentations\",\n    \"MarathonData.RData\")\n    )\n\nMod_MT1 <- brm(                        \n  MarathonTimeM ~ 1, \n  data = MarathonData,   \n  backend = \"cmdstanr\",\n  seed = 1975           \n)\n```\n\n## Results Exercise\n\n*Model1: only an intercept (brms defaults)*\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\nsummary(Mod_MT1)\n```\n\n## Results Exercise\n\n*Model1: only an intercept (brms defaults)*\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\nplot(Mod_MT1)\n```\n\n## Results Exercise\n\n*Model2: add the effect of* `km4week` *and* `sp4week`\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\n\nMod_MT2 <- brm(                        \n  MarathonTimeM ~ 1 + km4week + sp4week, \n  data = MarathonData,   \n  backend = \"cmdstanr\",\n  seed = 1975           \n)\n```\n\n## Results Exercise\n\n*Model2: add the effect of* `km4week` *and* `sp4week`\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\nsummary(Mod_MT2)\n```\n\n## Results Exercise\n\n*Model2: add the effect of* `km4week` *and* `sp4week`\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\nplot(Mod_MT2)\n```\n\n## Results Exercise\n\n*Model3: change the brms defaults*\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\n\nMod_MT3 <- brm(                        \n  MarathonTimeM ~ 1 + km4week + sp4week, \n  data = MarathonData,\n  chains = 4,\n  iter = 4000,\n  cores = 4,\n  backend = \"cmdstanr\",\n  seed = 1975           \n)\n```\n\n## Results Exercise\n\n*Model3: change the brms defaults*\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\nsummary(Mod_MT3)\n```\n\n## Results Exercise\n\n*Model3: change the brms defaults*\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\nplot(Mod_MT3)\n```\n\n## About convergence\n\n::::: columns\n::: {.column width=\"50%\"}\n```{r, out.height = \"99%\", out.width=\"99%\", echo = FALSE}\nknitr::include_graphics(\"Vethari_paper.jpg\")\n```\n:::\n\n::: {.column width=\"50%\"}\n-   $\\widehat R$ \\< 1.015 for each parameter estimate\n\n-   at least 4 chains are recommended\n\n-   Effective Sample Size (ESS) \\> 400 to rely on $\\widehat R$\n:::\n:::::\n\n## But is it a good model?\n\n<br> <br> Two complementary procedures: <br> <br>\n\n-   posterior-predictive check\n\n-   compare models with <i>[leave one out cross-validation]{style=\"color:#447099\"}</i>\n\n## Posterior-predictive check\n\nA visual check that can be performed with `pp_check()` from `brms`\n\n```{r, fig.width = 5, fig.height = 5}\n#| echo: true\n#| output-location: slide\n\npp_check(Mod_MT2) + theme_minimal()\n```\n\n## Model comparison with loo cross-validation\n\n$\\sim$ AIC or BIC in Frequentist statistics\n\n$\\widehat{elpd}$: \"expected log predictive density\" (higher $\\widehat{elpd}$ implies better model fit without being sensitive for overfitting!)\n\n```{r}\nMarathonTimes_Mod1 <-\n  readRDS(file = \n            here(\"Presentations\",\n              \"Output\",\n              \"MarathonTimes_Mod1.RDS\")\n          )\n```\n\n```{r}\n#| eval: true\n#| echo: true\n#| output-location: slide\n\nloo_Mod1 <- loo(MarathonTimes_Mod1)\nloo_Mod2 <- loo(MarathonTimes_Mod2)\n\nComparison<- \n  loo_compare(\n    loo_Mod1, \n    loo_Mod2\n    )\n\nprint(Comparison, simplify = F)\n```\n\n# WAMBS checklist\n\n```{css echo=FALSE}\n.small-code{\n  font-size: 75%  \n}\n```\n\n## When to Worry and How to Avoid Misuse of Bayesian Statistics {.smaller}\n\nby Laurent Smeets and Rens van der Schoot\n\n:::::: columns\n::: {.column width=\"33%\"}\nBefore estimating the model:\n\n1.  Do you understand the priors?\n:::\n\n::: {.column width=\"33%\"}\nAfter estimation before inspecting results:\n\n2.  Does the trace-plot exhibit convergence?\n3.  Does convergence remain after doubling the number of iterations?\n4.  Does the posterior distribution histogram have enough information?\n5.  Do the chains exhibit a strong degree of autocorrelation?\n6.  Do the posterior distributions make substantive sense?\n:::\n\n::: {.column width=\"33%\"}\nUnderstanding the exact influence of the priors\n\n7.  Do different specification of the multivariate variance priors influence the results?\n8.  Is there a notable effect of the prior when compared with non-informative priors?\n9.  Are the results stable from a sensitivity analysis?\n10. Is the Bayesian way of interpreting and reporting model results used?\n:::\n::::::\n\n::: aside\nTutorial source: <https://www.rensvandeschoot.com/brms-wambs/>\nAlternatives exist as well like the BARG framework (Kruschke, J.K. Bayesian Analysis Reporting Guidelines. Nat Hum Behav 5, 1282–1291 (2021). https://doi.org/10.1038/s41562-021-01177-7)\n:::\n\n## WAMBS Template to use\n\n-   File called [WAMBS_workflow_MarathonData.qmd]{style=\"color: #447099\"} (quarto document)\n\n-   <a href=\"https://abar-geneva-2024.netlify.app/WAMBS_template/WAMBS_workflow_MarathonData.qmd\" target=\"blank\">Click here </a> for the Quarto version\n\n-   Create your own project and project folder\n\n-   Copy the template and rename it\n\n-   We will go through the different parts in the slide show\n\n-   You can apply/adapt the code in the template\n\n-   To render the document properly with references, you also need the <a href=\"https://abar-geneva-2024.netlify.app/WAMBS_template/references.bib\" target=\"blank\">references.bib file </a>\n\n## Side-path: projects in RStudio and the `here` package\n\nIf you do not know how to use Projects in RStudio or the `here` package, these two sources might be helpfull:\n\nProjects: <https://youtu.be/MdTtTN8PUqU?si=mmQGlU063EMt86B2>\n\n`here` package: <https://youtu.be/oh3b3k5uM7E?si=0-heLJXfFVLtTohh>\n\n## Preparations for applying it to Marathon model\n\nPackages needed:\n\n```{r}\n#| echo: true\n#| eval: false\n\nlibrary(here)\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(bayesplot)\nlibrary(ggmcmc)\nlibrary(patchwork)\nlibrary(priorsense)\n```\n\n## Preparations for applying it to Marathon model\n\nLoad the dataset and the model:\n\n```{r}\n#| echo: true\n#| eval: false\nload(\n  file = here(\"Presentations\", \"MarathonData.RData\")\n)\n\nMarathonTimes_Mod2 <-\n  readRDS(file = \n            here(\"Presentations\",\n              \"Output\",\n              \"MarathonTimes_Mod2.RDS\")\n          )\n```\n\n# Focus on the [priors]{style=\"color: #447099\"} before estimation\n\n## Remember: priors come in many disguises\n\n::::: columns\n::: {.column width=\"50%\"}\n[Uninformative/Weakly informative]{style=\"color: #447099\"}\n\nWhen objectivity is crucial and you want *let the data speak for itself...*\n:::\n\n::: {.column width=\"50%\"}\n[Informative]{style=\"color: #447099\"}\n\nWhen including significant information is crucial\n\n-   previously collected data\n-   results from former research/analyses\n-   data of another source\n-   theoretical considerations\n-   elicitation\n:::\n:::::\n\n## `brms` defaults\n\n-   Weakly informative priors\n\n-   If dataset is big, impact of priors is minimal\n\n-   But, always better to know what you are doing!\n\n-   Complex models might run into convergence issues $\\rightarrow$ specifying more informative priors might help!\n\nSo, how to deviate from the defaults?\n\n## Check priors used by `brms`\n\nFunction: `get_prior( )`\n\nRemember our model 2 for Marathon Times:\n\n$$\\begin{aligned}\n& \\text{MarathonTimeM}_i \\sim N(\\mu,\\sigma_e)\\\\\n& \\mu = \\beta_0 + \\beta_1*\\text{km4week}_i + \\beta_2*\\text{sp4week}_i \n\\end{aligned}$$\n\n```{r}\n#| echo: true\n#| eval: false\n\nget_prior(\n  MarathonTimeM ~ 1 + km4week + sp4week, \n  data = MarathonData\n)\n```\n\n## Check priors used by `brms`\n\n```{r, out.height = \"70%\", out.width=\"70%\", echo = FALSE}\nknitr::include_graphics(\"Priors_Mod2.jpg\")\n```\n\n-   `prior`: type of prior distribution\n-   `class`: parameter class (with `b` being population-effects)\n-   `coef`: name of the coefficient within parameter class\n-   `group`: grouping factor for group-level parameters (when using mixed effects models)\n-   `resp` : name of the response variable when using multivariate models\n-   `lb` & `ub`: lower and upper bound for parameter restriction\n\n## Visualizing priors\n\nThe best way to make sense of the priors used is visualizing them!\n\nMany options:\n\n-   The Zoo of Distributions <https://ben18785.shinyapps.io/distribution-zoo/>\n-   making your own visualizations\n\nSee WAMBS template!\n\nThere we demonstrate the use of `ggplot2`, `metRology`, `ggtext` and `patchwork` to visualize the priors.\n\n## Visualizing priors\n\n```{r}\n#| message: false\n#| warning: false\n#| error: false\n#| echo: true\n#| output-location: slide\n#| fig-cap: \"Probability density plots for the different priors used in the example model\"\n#| fig-cap-location: margin\n#| cache: true\n\nlibrary(metRology)\nlibrary(ggplot2)\nlibrary(ggtext)\nlibrary(patchwork)\n\n# Setting a plotting theme\ntheme_set(theme_linedraw() +\n            theme(text = element_text(family = \"Times\", size = 8),\n                  panel.grid = element_blank(),\n                  plot.title = element_markdown())\n)\n\n# Generate the plot for the prior of the Intercept (mu)\nPrior_mu <- ggplot( ) +\n  stat_function(\n    fun = dt.scaled,    # We use the dt.scaled function of metRology\n    args = list(df = 3, mean = 199.2, sd = 24.9), # \n    xlim = c(120,300)\n  ) +\n  scale_y_continuous(name = \"density\") +\n  labs(title = \"Prior for the intercept\",\n       subtitle = \"student_t(3,199.2,24.9)\")\n\n# Generate the plot for the prior of the error variance (sigma)\nPrior_sigma <- ggplot( ) +\n  stat_function(\n    fun = dt.scaled,    # We use the dt.scaled function of metRology\n    args = list(df = 3, mean = 0, sd = 24.9), # \n    xlim = c(0,6)\n  ) +\n  scale_y_continuous(name = \"density\") +\n  labs(title = \"Prior for the residual variance\",\n       subtitle = \"student_t(3,0,24.9)\")\n\n# Generate the plot for the prior of the effects of independent variables\nPrior_betas <- ggplot( ) +\n  stat_function(\n    fun = dnorm,    # We use the normal distribution\n    args = list(mean = 0, sd = 10), # \n    xlim = c(-20,20)\n  ) +\n  scale_y_continuous(name = \"density\") +\n  labs(title = \"Prior for the effects of independent variables\",\n       subtitle = \"N(0,10)\")\n\nPrior_mu + Prior_sigma + Prior_betas +\n  plot_layout(ncol = 3)\n```\n\n## Understanding priors... another example\n\nExperimental study (pretest - posttest design) with 3 conditions:\n\n-   control group;\n-   experimental group 1;\n-   experimental group 2.\n\nModel:\n\n$$\\begin{aligned}\n  & Posttest_{i}  \\sim N(\\mu,\\sigma_{e_{i}})\\\\\n  & \\mu = \\beta_0 + \\beta_1*\\text{Pretest}_{i} + \\beta_2*\\text{Exp_cond1}_{i} + \\beta_3*\\text{Exp_cond2}_{i}\n\\end{aligned}$$\n\nOur job: coming up with priors that reflect that we expect both conditions to have a positive effect (hypothesis based on literature) and no indications that one experimental condition will outperform the other.\n\n## Understanding priors... another example\n\n-   Assuming pre- and posttest scores are standardized\n-   Assuming no increase between pre- and posttest in control condition\n\n```{r}\n# Generate the plot for the prior of the Intercept (mu)\nPrior_mu <- ggplot( ) +\n  stat_function(\n    fun = dnorm,    # We use a normal distribution\n    args = list(mean = 0, sd = .5), # \n    xlim = c(-1,1)\n  ) +\n  scale_y_continuous(name = \"density\") +\n  labs(title = \"Prior for the intercept\",\n       subtitle = \"N(0,0.5)\")\n\nPrior_mu\n```\n\n## Understanding priors... another example\n\n-   Assuming a strong correlation between pre- and posttest\n\n```{r}\n# Generate the plot for the prior of beta 1\nPrior_beta1 <- ggplot( ) +\n  stat_function(\n    fun = dnorm,    # We use the dt.scaled function of metRology\n    args = list(mean = 1, sd = 0.5), # \n    xlim = c(-1,3)\n  ) +\n  scale_y_continuous(name = \"density\") +\n  labs(title = \"Prior for the effect of pretest score\",\n       subtitle = \"N(1,0.5)\")\n\nPrior_beta1\n```\n\n## Understanding priors... another example\n\n-   Assuming a small effect of experimental conditions\n-   No difference between both experimental conditions\n\n```{r}\n# Generate the plot for the prior of the effects experimental conditions\nPrior_betas <- ggplot( ) +\n  stat_function(\n    fun = dnorm,    # We use the normal distribution\n    args = list(mean = 0.2, sd = .6), # \n    xlim = c(-1,2)\n  ) +\n  scale_y_continuous(name = \"density\") +\n  labs(title = \"Prior for the effects of experimental conditions\",\n       subtitle = \"N(0.2,0.6)\")\n\nPrior_betas\n```\n\nRemember Cohen's d: 0.2 = small effect size; 0.5 = medium effect size; 0.8 or higher = large effect size\n\n## Setting custom priors in `brms`\n\n<br>\n\nSetting our custom priors can be done with `set_prior( )` command\n\n<br>\n\nE.g., change the priors for the beta's (effects of `km4week` and `sp4week`):\n\n<br>\n\n```{r}\n#| message: false\n#| warning: false\n#| error: false\n#| echo: true\n#| cache: true\n\n\nCustom_priors <- \n  c(\n    set_prior(\n      \"normal(0,10)\", \n      class = \"b\", \n      coef = \"km4week\"),\n    set_prior(\n      \"normal(0,10)\", \n      class = \"b\", \n      coef = \"sp4week\")\n    )\n\n```\n\n## Prior Predictive Check\n\n<br>\n\nDid you set sensible priors?\n\n<br>\n\n-   Simulate data based on the model and the priors\n\n<br>\n\n-   Visualize the simulated data and compare with real data\n\n<br>\n\n-   Check if the plot shows impossible simulated datasets\n\n## Prior Predictive Check in `brms`\n\n<br>\n\nStep 1: Fit the model with custom priors with option `sample_prior=\"only\"`\n\n<br>\n\n```{r}\n#| message: false\n#| warning: false\n#| error: false\n#| echo: true\n#| eval: false\n#| cache: true\n#| code-line-numbers: \"5|8\"\nFit_Model_priors <- \n  brm(\n    MarathonTimeM ~ 1 + km4week + sp4week, \n    data = MarathonData,\n    prior = Custom_priors,\n    backend = \"cmdstanr\",\n    cores = 4,\n    sample_prior = \"only\"\n    )\n```\n\n```{r}\nFit_Model_priors <- readRDS(\n  here(\n    \"Presentations\",\n    \"Output\",\n    \"Fit_Model_priors.RDS\"\n  )\n)\n```\n\n## Prior Predictive Check in `brms`\n\n<br>\n\nStep 2: visualize the data with the `pp_check( )` function\n\n<br>\n\n```{r}\n#| echo: true\n#| output-location: slide\n\nset.seed(1975)\n\npp_check(\n  Fit_Model_priors, \n  ndraws = 300) # number of simulated datasets you wish for\n\n```\n\n## Check some summary statistics\n\n-   How are summary statistics of simulated datasets (e.g., median, min, max, ...) distributed over the datasets?\n\n-   How does that compare to our real data?\n\n-   Use `type = \"stat\"` argument within `pp_check()`\n\n```{r}\n#| echo: true\n#| output-location: slide\npp_check(Fit_Model_priors, \n         type = \"stat\", \n         stat = \"median\")\n```\n\n## `r fontawesome::fa(\"laptop-code\", \"white\")` Your Turn {background-color=\"#447099\" transition=\"slide-in\"}\n\n-   Your data and model\n\n-   Perform a prior predictive check\n\n-   If necessary re-think your priors and check again\n\n# Focus on convergence of the model (before interpreting the model!)\n\n## Does the trace-plot exhibits convergence?\n\n<br>\n\nCreate custom trace-plots (aka caterpillar plots) with `ggs( )` function from `ggmcmc` package\n\n```{r}\n#| fig-cap: \"Caterpillar plots for the parameters in the model\"\n#| fig-cap-location: margin\n#| output-location: slide\n#| cache: true\n#| echo: true\n#| warning: false\nModel_chains <- ggs(MarathonTimes_Mod2)\n\nModel_chains %>%\n  filter(Parameter %in% c(\n          \"b_Intercept\", \n          \"b_km4week\", \n          \"b_sp4week\", \n          \"sigma\"\n          )\n  ) %>%\n  ggplot(aes(\n    x   = Iteration,\n    y   = value, \n    col = as.factor(Chain)))+\n  geom_line() +\n  facet_grid(Parameter ~ .,\n             scale  = 'free_y',\n             switch = 'y') +\n  labs(title = \"Caterpillar Plots for the parameters\",\n       col   = \"Chains\")\n```\n\n## Does convergence remain after doubling the number of iterations?\n\n<br>\n\nRe-fit the model with more iterations\n\n<br>\n\nCheck trace-plots again\n\n<br>\n\n::: callout-warning\nFirst consider the need to do this! If you have a complex model that already took a long time to run, this check will take at least twice as much time...\n:::\n\n## `r fontawesome::fa(\"laptop-code\", \"white\")` Your Turn {background-color=\"#447099\" transition=\"slide-in\"}\n\n-   Your data and model\n-   Do the first checks on the model convergence\n\n## R-hat statistics\n\nSampling of parameters done by:\n\n-   multiple chains\n-   multiple iterations within chains\n\nIf variance between chains is big $\\rightarrow$ NO CONVERGENCE\n\nR-hat ($\\widehat{R}$) : compares the between- and within-chain estimates for model parameters\n\n## R-hat statistics\n\n::::: columns\n::: {.column width=\"50%\"}\n```{r, out.height = \"95%\", out.width=\"95%\", echo = FALSE}\nknitr::include_graphics(\"Vethari_paper.jpg\")\n```\n:::\n\n::: {.column width=\"50%\"}\n-   $\\widehat{R}$ \\< 1.015 for each parameter estimate\n\n-   at least 4 chains are recommended\n\n-   Effective Sample Size (ESS) \\> 400 to rely on $\\widehat{R}$\n:::\n:::::\n\n## R-hat in `brms`\n\n`mcmc_rhat()` function from the `bayesplot` package\n\n```{r}\n#| echo: true\n#| output-location: slide\n\nmcmc_rhat(\n  brms::rhat(MarathonTimes_Mod2), \n  size = 3\n  )+ \n  yaxis_text(hjust = 1)  # to print parameter names\n```\n\n## `r fontawesome::fa(\"laptop-code\", \"white\")` Your Turn {background-color=\"#447099\" transition=\"slide-in\"}\n\n-   Your data and model\n\n-   Check the R-hat statistics\n\n## Autocorrelation\n\n-   Sampling of parameter values are not independent!\n\n-   So there is autocorrelation\n\n-   But you don't want too much impact of autocorrelation\n\n-   2 approaches to check this:\n\n    -   ratio of the effective sample size to the total sample size\n    -   plot degree of autocorrelation\n\n## Ratio effective sample size / total sample size\n\n-   Should be higher than 0.1 (Gelman et al., 2013)\n\n-   Visualize making use of the `mcmc_neff( )` function from `bayesplot`\n\n```{r}\n#| echo: true\n#| output-location: slide\nmcmc_neff(\n  neff_ratio(MarathonTimes_Mod2)\n  ) + \n  yaxis_text(hjust = 1)  # to print parameter names\n```\n\n## Plot degree of autocorrelation\n\n-   Visualize making use of the `mcmc_acf( )` function\n\n```{r}\n#| echo: true\n#| output-location: slide\nmcmc_acf(\n  as.array(MarathonTimes_Mod2), \n  regex = \"b\") # to plot only the parameters starting with b (our beta's)\n```\n\n## `r fontawesome::fa(\"laptop-code\", \"white\")` Your Turn {background-color=\"#447099\" transition=\"slide-in\"}\n\n-   Your data and model\n\n-   Check the autocorrelation\n\n## Rank order plots\n\n-   additional way to assess the convergence of MCMC\n\n-   if the algorithm converged, plots of all chains look similar\n\n```{r}\n#| echo: true\n#| output-location: slide\n\nmcmc_rank_hist(\n  MarathonTimes_Mod2, \n  regex = \"b\" # only intercept and beta's\n  ) \n```\n\n## `r fontawesome::fa(\"laptop-code\", \"white\")` Your Turn {background-color=\"#447099\" transition=\"slide-in\"}\n\n-   Your data and model\n\n-   Check the rank order plots\n\n# Focus on the Posterior\n\n## Does the posterior distribution histogram have enough information?\n\n-   Histogram of posterior for each parameter\n\n-   Have clear peak and sliding slopes\n\n## Plotting the posterior distribution histogram\n\n<br>\n\nStep 1: create a new object with 'draws' based on the final model\n\n<br>\n\n```{r}\n#| echo: true\nposterior_PD <- as_draws_df(MarathonTimes_Mod2)\n```\n\n## Plotting the posterior distribution histogram\n\n<br>\n\nStep 2: create histogram making use of that object\n\n<br>\n\n```{r}\n#| echo: true\n\npost_intercept <- \n  posterior_PD %>%\n  select(b_Intercept) %>%\n  ggplot(aes(x = b_Intercept)) +\n  geom_histogram() +\n  ggtitle(\"Intercept\") \n\npost_km4week <- \n  posterior_PD %>%\n  select(b_km4week) %>%\n  ggplot(aes(x = b_km4week)) +\n  geom_histogram() +\n  ggtitle(\"Beta km4week\") \n\npost_sp4week <- \n  posterior_PD %>%\n  select(b_sp4week) %>%\n  ggplot(aes(x = b_sp4week)) +\n  geom_histogram() +\n  ggtitle(\"Beta sp4week\") \n```\n\n## Plotting the posterior distribution histogram\n\n<br>\n\nStep 3: print the plot making use of `patchwork` 's workflow to combine plots <br>\n\n```{r}\n#| echo: true\n#| output-location: slide\npost_intercept + post_km4week + post_sp4week +\n  plot_layout(ncol = 3)\n```\n\n## Posterior Predictive Check\n\n-   Generate data based on the posterior probability distribution\n\n-   Create plot of distribution of y-values in these simulated datasets\n\n-   Overlay with distribution of observed data\n\nusing `pp_check()` again, now with our model\n\n```{r}\n#| echo: true\n#| output-location: slide\npp_check(MarathonTimes_Mod2, \n         ndraws = 100)\n```\n\n## Posterior Predictive Check\n\n-   We can also focus on some summary statistics (like we did with prior predictive checks as well)\n\n```{r}\n#| echo: true\n#| message: false\n#| warning: false\n#| output-location: slide\npp_check(MarathonTimes_Mod2, \n         ndraws = 300,\n         type = \"stat\",\n         stat = \"median\")\n```\n\n## `r fontawesome::fa(\"laptop-code\", \"white\")` Your Turn {background-color=\"#447099\" transition=\"slide-in\"}\n\n-   Your data and model\n\n-   Focus on the posterior and do some checks!\n\n# Prior sensibility analyses\n\n## Why prior sensibility analyses?\n\n-   Often we rely on 'arbitrary' chosen (default) weakly informative priors\n\n-   What is the influence of the prior (and the likelihood) on our results?\n\n-   You could ad hoc set new priors and re-run the analyses and compare (a lot of work, without strict sytematical guidelines)\n\n-   Semi-automated checks can be done with `priorsense` package\n\n## Using the `priorsense` package\n\nRecently a package dedicated to prior sensibility analyses is launched\n\n```{r}\n#| eval: false\n#| echo: true\n# install.packages(\"remotes\")\nremotes::install_github(\"n-kall/priorsense\")\n```\n\nKey-idea: power-scaling (both prior and likelihood)\n\nbackground reading:\n\n-   <https://arxiv.org/pdf/2107.14054.pdf>\n\nYouTube talk:\n\n-   <https://www.youtube.com/watch?v=TBXD3HjcIps&t=920s>\n\n## Basic table with indices\n\nFirst check is done by using the `powerscale_sensitivity( )` function\n\n-   column prior contains info on sensibility for prior (should be lower than 0.05)\n\n-   column likelihood contains info on sensibility for likelihood (that we want to be high, 'let our data speak')\n\n-   column diagnosis is a verbalization of potential problem (- if none)\n\n```{r}\n#| echo: true\n#| output-location: slide\npowerscale_sensitivity(MarathonTimes_Mod2)\n```\n\n## Visualization of prior sensibility\n\n```{r}\n#| echo: true\n#| warning: false\n#| message: false\n#| cache: true\n#| output-location: slide\n\npowerscale_plot_dens(\n  powerscale_sequence(\n    MarathonTimes_Mod2\n    ),\n  variable = c(\n      \"b_Intercept\",\n      \"b_km4week\",\n      \"b_sp4week\"\n    )\n  )\n```\n\n## Visualization of prior sensibility\n\n```{r}\n#| echo: true\n#| warning: false\n#| message: false\n#| cache: true\n#| output-location: slide\n\npowerscale_plot_quantities(\n  powerscale_sequence(\n    MarathonTimes_Mod2\n    ),\n  variable = c(\n      \"b_km4week\"\n      )\n  )\n```\n\n## `r fontawesome::fa(\"laptop-code\", \"white\")` Your Turn {background-color=\"#447099\" transition=\"slide-in\"}\n\n-   Your data and model\n\n-   Check the prior sensibility of your results"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","self-contained":true,"output-file":"Slides_Part2.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.6.39","auto-stretch":true,"extensions":["r-wasm/quarto-live"],"title":"Applied Bayesian Analyses in R","subtitle":"Part2","author":"Sven De Maeyer","code":{"code-copy":true,"code-line-numbers":true},"code-annotations":"hover","theme":["simple","My_theme.scss"],"width":1422,"height":805,"slideNumber":true}}},"projectFormats":["html"]}