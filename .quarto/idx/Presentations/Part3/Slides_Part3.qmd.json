{"title":"Applied Bayesian Analyses in R","markdown":{"yaml":{"title":"Applied Bayesian Analyses in R","subtitle":"Part 3","author":"Sven De Maeyer","format":{"revealjs":{"theme":["default","zurich2026-slides-clean.scss"],"width":1422,"height":805,"slide-number":true}},"editor":"visual","self-contained":true,"execute":{"echo":false,"include":true,"output":true},"code":{"code-copy":true,"code-line-numbers":true},"code-annotations":"hover"},"headingText":"Setting a plotting theme","containsRefs":false,"markdown":"\n\n```{r}\nlibrary(here)\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(bayesplot)\nlibrary(ggmcmc)\nlibrary(patchwork)\nlibrary(priorsense)\n\nload(\n  file = here(\n    \"Presentations\", \n    \"WritingData.RData\")\n)\n\nM3 <-readRDS(file = \n  here(\"Presentations\",\n        \"Part3\",\n        \"M3.RDS\"\n       )\n  )\n\ntheme_set(theme_linedraw() +\n            theme(\n              text = element_text(family = \"Times\", size = 14),\n              panel.grid = element_blank()\n              )\n)\n\n```\n\n## New example data `WritingData.RData`\n\n-   Experimental study on Writing instructions\n\n-   2 conditions:\n\n    -   Control condition (Business as usual)\n    -   Experimental condition (Observational learning)\n\n```{r, out.height = \"50%\", out.width=\"50%\", echo = FALSE}\nknitr::include_graphics(here(\"Presentations\", \"Part3\",\"WritingDataDesc.jpg\"))\n```\n\n## `r fontawesome::fa(\"laptop-code\", \"white\")` Your Turn {background-color=\"#447099\" transition=\"slide-in\"}\n\n-   Open `WritingData.RData`\n\n-   Estimate 3 models with `SecondVersion` as dependent variable\n\n    -   M1: fixed effect of `FirstVersion_GM` + random effect of `Class` (`(1|Class)`)\n    -   M2: M1 + random effect of `FirstVersion_GM` (`(1 + FirstVersion_GM |Class)`)\n    -   M3: M2 + fixed effect of `Experimental_condition`\n\n-   Compare the models on their fit\n\n-   What do we learn?\n\n-   Make a summary of the best fitting model\n\n::: aside\n[Note:]{style=\"color: white\"} `FirstVersion_GM`[is the score of the pretest centred around the mean, so a score 0 for this variable implies scoring on average for the pretest]{style=\"color: white\"}\n:::\n\n## Divergent transitions...\n\n![Exploring the parameter space...](ChatGPT_explorer.png){fig-align=\"center\"}\n\n## \n\n::: center\n```{r}\n#| eval: true\n#| warning: false\n#| message: false\n#| echo: false\n\nlibrary(plotly)\nlibrary(dplyr)\n\n# Create data for a 3D valley\nset.seed(123)\nx <- seq(-5, 5, length.out = 50)\ny <- seq(-5, 5, length.out = 50)\ngrid <- expand.grid(x = x, y = y)\ngrid$height <- with(grid, 10 - sqrt(x^2 + y^2) + rnorm(length(x), 0, 0.5))\n\n# Meetpunten van drie verschillende ontdekkingsreizigers in 3D\nExplorers <- data.frame(\n  Explorer = rep(c(\"A\", \"B\", \"C\"), each = 6),\n  x = c(-4, -3, -2, -1, 0, 1,\n        4, 3, 2, 1, 0, -1,\n        0, -1, -2, -3, -4, -5),\n  y = c(-4, -3, -1.5, 0, 1, 2,\n        4, 3.5, 2.5, 1.5, 0.5, -0.5,\n        -4, -3, -2, -1, 0, 1)\n)\nExplorers$height <- 10 - sqrt(Explorers$x^2 + Explorers$y^2)\n\nColors <- c(\"A\" = \"red\", \"B\" = \"blue\", \"C\" = \"black\")\n\n# Maak 3D-plot\nfig <- plot_ly(x = ~grid$x, y = ~grid$y, z = ~grid$height,\n               type = 'mesh3d', opacity = 0.4, colorscale = 'Viridis')\n\nfor(o in unique(Explorers$Explorer)) {\n  subset <- Explorers %>% filter(Explorer == o)\n  fig <- fig %>%\n    add_trace(data = subset, x = ~x, y = ~y, z = ~height,\n              type = 'scatter3d', mode = 'lines+markers',\n              line = list(width = 4, color = Colors[o]),\n              marker = list(size = 4, color = Colors[o]),\n              name = paste(\"Explorer\", o))\n}\n\nfig <- fig %>%\n  layout(scene = list(\n    xaxis = list(title = \"Parameter X\"),\n    yaxis = list(title = \"Parameter Y\"),\n    zaxis = list(title = \"Height (posterior density)\")\n  ),\n  title = \"3D-visualisation of MCMC-sampling by multiple Explorers\")\n\nfig <- fig %>% layout(width = 900, height = 600)\n\nfig\n\n```\n:::\n\n## Divergent transitions...\n\n-   Something to worry about!\n\n-   Essentially: sampling of parameter estimate values went wrong\n\n-   Fixes:\n\n    -   sometimes fine-tuning the sampling algorithm (e.g., `control = list(adapt_delta = 0.9)`) works\n    -   sometimes you need more informative priors\n    -   sometimes the model is just not a good model\n\n## Our fix here for model 3\n\n```{r}\n#| eval: false\n#| echo: true\n\nM3 <- brm(\n  SecondVersion ~ FirstVersion_GM + Experimental_condition + (1 + FirstVersion_GM |Class),\n  data = WritingData,\n  backend = \"cmdstanr\",\n  cores = 4,\n  control = list(adapt_delta = 0.9),\n  seed = 1975 \n)\n```\n\n## Our fix here for model 3\n\n```{r, out.height = \"50%\", out.width=\"50%\", echo = FALSE}\nknitr::include_graphics(\"M3_summary.jpg\")\n```\n\n## Interpretation of results...\n\nDifferent ways to summarize our results:\n\n-   visually\n\n-   credible intervals (eti & hdi)\n\n-   rope + hdi rule\n\n-   hypothesis tests\n\n# Visually summarizing the posterior distribution\n\n## Functions in `bayesplot` package\n\n-   `mcmc_areas()` function\n\n-   `mcmc_areas_ridges()` function\n\n-   `mcmc_intervals()` function\n\n## The `mcmc_areas()` function\n\nGives a posterior distribution including a certain credible interval that you can set manually with the `prob` argument:\n\n```{r}\n#| eval: true\n#| echo: true\n#| output-location: slide\nmcmc_areas(\n  M3,\n  pars = c(\n    \"b_FirstVersion_GM\",\n    \"b_Experimental_condition\"\n  ),\n  prob = .89\n)\n```\n\n## The `mcmc_areas_ridges()` function\n\n<br>\n\nAlmost similar to the previous, only the horizontal spacing changes a bit...\n\n<br>\n\nMeanwhile, see how you can easily change the color scheme for `bayesplot` graphs\n\n<br>\n\n```{r}\n#| eval: true\n#| echo: true\n#| output-location: slide\n\ncolor_scheme_set(scheme = \"red\")\n\nmcmc_areas_ridges(\n  M3,\n  pars = c(\n    \"b_FirstVersion_GM\",\n    \"b_Experimental_condition\"\n  ),\n  prob = .89\n)\n```\n\n## The `mcmc_intervals()` function\n\nSummarizes the posterior as a horizontal bar with identifiers for two CI.\n\nHere we set one for a 50% and one for a 89% CI\n\n```{r}\n#| eval: true\n#| echo: true\n#| output-location: slide\ncolor_scheme_set(scheme = \"gray\")\n\nmcmc_intervals(\n  M3,\n  pars = c(\n    \"b_FirstVersion_GM\",\n    \"b_Experimental_condition\"\n  ),\n  prob = .5,\n  prob_outer = .89\n)\n```\n\n## Manually create visualizations\n\n<br>\n\nPowercombo: `as_draws_df()` + `ggplot2` + `ggdist`\n\n<br>\n\nWhat does `as_draws_df()` do?\n\n<br>\n\n```{r}\n#| eval: true\n#| echo: true\n#| output-location: slide\nposterior_PD <- as_draws_df(M3)\nhead(posterior_PD)\n```\n\n## Use draws to create a plot using `ggdist` geoms\n\n::::: columns\n::: {.column width=\"60%\"}\n```{r, out.height = \"98%\", out.width=\"98%\", echo = FALSE}\nknitr::include_graphics(\"ggdist_geoms.jpg\")\n```\n:::\n\n::: {.column width=\"40%\"}\n`ggdist` package has a set of functions to visualize a distribution\n:::\n:::::\n\n## An example\n\n<br>\n\nBefore we start, set our own plot theme (not so necessary)\n\n<br>\n\n```{r}\n#| echo: true\n\n# Setting a plotting theme\ntheme_set(theme_linedraw() +\n            theme(\n              text = element_text(family = \"Times\", size = 14),\n              panel.grid = element_blank()\n              )\n)\n\n```\n\n## An example\n\n<br>\n\nWe use `posterior_PD` as a starting point (our draws)\n\n```{r}\n#| message: false\n#| warning: false\n#| error: false\n#| echo: true\n#| output-location: slide\n\nlibrary(ggdist)\n\nPlot <- ggplot(\n  posterior_PD,\n  aes(x = b_Experimental_condition)\n  ) +\n  stat_halfeye()\n\nPlot + scale_y_continuous(name = \"\", breaks = NULL)\n```\n\n## Change the CI's\n\n<br>\n\nChange the CI's to 50% and 89%\n\n<br>\n\n```{r}\n#| message: false\n#| warning: false\n#| error: false\n#| echo: true\n#| code-line-numbers: \"6\"\n#| output-location: slide\nPlot <- ggplot(\n  posterior_PD,\n  aes(x = b_Experimental_condition)\n  ) +\n  stat_halfeye(\n    .width = c(.50,.89)\n  )\n\nPlot + scale_y_continuous(name = \"\", breaks = NULL)\n```\n\n## Use another visualization\n\n<br>\n\nLet's make a dotplot... (research shows this is best interpreted) with 100 dots\n\n<br>\n\n```{r}\n#| message: false\n#| warning: false\n#| error: false\n#| echo: true\n#| code-line-numbers: \"5|6\"\n#| output-location: slide\nPlot <- ggplot(\n  posterior_PD,\n  aes(x = b_Experimental_condition)\n  ) +\n  stat_dotsinterval(\n    quantiles = 100,\n    .width = c(.50,.89)\n  )\n\nPlot + scale_y_continuous(name = \"\", breaks = NULL)\n```\n\n## Plot two parameters each in a facet\n\n<br>\n\nWe use `pivot_longer(everything())` to stack information on multiple parameters\n\n<br>\n\n```{r}\n#| message: false\n#| warning: false\n#| error: false\n#| echo: true\n#| output-location: slide\nposterior_PD %>% \n  select(\n    b_Experimental_condition, b_FirstVersion_GM\n  ) %>% \n  pivot_longer(everything()) %>%\n  ggplot(\n    aes(x = value)\n  ) +\n  stat_halfeye(\n    .width = c(.50,.89)\n  ) +\nfacet_wrap(name ~ .) +\nscale_y_continuous(name = \"\", breaks = NULL)\n\n```\n\n## Visualize calculated predictions based on posterior\n\nOur example: 2 groups according to `Experimental_condition`\n\nHow to visualize the posterior probability of averages for both groups?\n\n```{r}\n#| message: false\n#| warning: false\n#| error: false\n#| echo: true\n#| code-line-numbers: \"6|7|10\"\n#| output-location: slide\nposterior_PD %>% \n  select(\n    b_Intercept, b_Experimental_condition\n  ) %>% \n  mutate(\n    Mean_Control_condition = b_Intercept,\n    Mean_Experimental_condition = b_Intercept + b_Experimental_condition\n  ) %>% \n  select(\n    Mean_Control_condition, Mean_Experimental_condition\n  ) %>% \n  pivot_longer(everything()) %>%\n  ggplot(\n    aes(x = value, color = name, fill = name)\n  ) +\n  stat_halfeye(\n    .width = c(.50,.89),\n    alpha = .40\n  ) + \n  scale_y_continuous(name = \"\", breaks = NULL)\n```\n\n## Hypothetical Outcome Plots (HOPs)\n\nCode: see separate script called `HOP_script.R`\n\n```{r}\n### CREATING HYPOTHETICAL OUTCOME PLOTS FOR BRMS MODEL ###\n\n# make sure your model is loaded in your R session\n# here we will apply it to the model M3\n\n## Step 1: sample n number of parameter values from the posterior\n\n# here we sample n = 20\n\nS <- as_draws_df(M3) %>% \n  select(\n    # select the necessary parameters to calculate the predicted scores\n    b_Intercept, \n    b_FirstVersion_GM,\n  ) %>%\n  slice_sample(\n    n = 20, # define the number of lines (draws) you want\n    replace = T\n  )\n\n## Step 2: Create a vector of possible scores for your X variable\n\n# here I make a vector of potential values for km4week_z (set a sensible range!)\n# our km4week_z is a z score so I choose numbers between -3 and 3\nX <- seq(-15, 15, by = .1)\n\n## Step 3: Create an empty tibble that will be filled with predictions\n\nPredictions <- tibble(\n  draw = NULL,\n  X = NULL,\n  Pred1 = NULL  \n)\n\n## Step 4: For each of our n (here 20) samples of parameter values calculate a prediction of Y\n\nfor(i in 1:20){\n\n  Si <- S[i,]\n  \n  Pred1 <- Si$b_Intercept + Si$b_FirstVersion_GM*X\n  \n  draw <- rep(i,length(X))\n  \n  Pred <- tibble(\n    draw,\n    X,  \n    Pred1, \n  )\n  \n  Predictions <- rbind(Predictions, Pred)\n}\n\n# Check the result of our predictions\n\nhead(Predictions)\n\n## Step 5: Make a plot!\n\nP1 <- Predictions %>%\n  select(draw, X, Pred1) %>%\n  ggplot(aes(x = X,\n             y = Pred1,\n             group = draw)) +\n  geom_line(color = \"gray60\", alpha = .6) +\n  scale_y_continuous(\"predicted values\") +\n  scale_x_continuous(\"first version (centred around the mean)\")\n\nP1\n```\n\n# Visualizing Random Effects\n\n## Plotting the residuals\n\nTo plot differences between classes we can use class-specific residuals:\n\n```{r}\n#| message: false\n#| warning: false\n#| error: false\n#| echo: true\nhead(as_draws_df(M3) %>% \n  select(ends_with(\",Intercept]\")) %>%\n  select(1:3),\n  5\n)\n```\n\n## Plotting the residuals\n\n```{r}\n#| message: false\n#| warning: false\n#| error: false\n#| echo: true\n#| output-location: slide\nas_draws_df(M3) %>% \n  select(ends_with(\",Intercept]\")) %>%\n  pivot_longer(starts_with(\"r_Class\")) %>% \n  mutate(sigma_i = value) %>%\n  ggplot(aes(x = sigma_i, y = reorder(name, sigma_i))) +\n  stat_pointinterval(\n    point_interval = mean_qi, \n    .width = .89, \n    size = 1/6) +\n  scale_y_discrete(expression(italic(j)), breaks = NULL) +\n  labs(x = expression(italic(u)[italic(j)])) +\n  coord_flip()\n```\n\n## ICC estimation\n\n```{r}\n#| message: false\n#| warning: false\n#| error: false\n#| echo: true\nhead(\n  as_draws_df(M3) %>%\n    mutate(\n      ICC = (sd_Class__Intercept^2/(sd_Class__Intercept^2 + sigma^2))) %>%\n    select(sigma, sd_Class__Intercept, ICC), \n  5\n  ) \n```\n\n## ICC estimation\n\n```{r}\n#| message: false\n#| warning: false\n#| error: false\n#| echo: true\n#| output-location: slide\n\nas_draws_df(M3) %>%\n  mutate(\n    ICC = (sd_Class__Intercept^2/(sd_Class__Intercept^2 + sigma^2))\n    ) %>%\n  select(ICC) %>%                           \n  ggplot(aes(x = ICC)) +                    \n   stat_dotsinterval(\n     quantiles = 100,\n     .width = c(.50,.89)\n   ) +\n   scale_x_continuous(\"marginal posterior\", \n                      breaks = seq(.00,.60,by =.05)) + \n   scale_y_continuous(\"ICC\", breaks = NULL)\n```\n\n## HOP per higher level unit\n\nCode: see separate script called `HOP_MixedEffects_script.R`\n\n```{r}\n\n\n### CREATING HYPOTHETICAL OUTCOME PLOTS FOR BRMS MODEL ###\n\n# make sure your model is loaded in your R session\n# here we will apply it to the model M3\n\n## Step 1: sample n number of parameter values from the posterior\n\n# here we sample n = 20\nlibrary(posterior)\nS <- as_draws_df(M3) %>% \n  select(\n    # select the necessary parameters to calculate the predicted scores\n    b_Intercept, \n    b_FirstVersion_GM,\n    ends_with(\",Intercept]\"),       # select class specific intercept residuals\n    ends_with(\",FirstVersion_GM]\")  # select class specific slope residuals\n  ) %>%\n  slice_sample(\n    n = 20, # define the number of lines (draws) you want per class\n    replace = T\n  ) %>%\n  mutate(\n    draw = seq(1:20)\n  )\n\n## Create long dataframe\n\nS_Long <- S %>%\n\n    ## Pivot longer\n\n    pivot_longer(\n    cols = c(\n      ends_with(\",Intercept]\"),       # select class specific intercept residuals\n      ends_with(\",FirstVersion_GM]\")  # select class specific slope residuals\n    ),\n    names_sep = \",\",\n    names_to = c(\"Class\", \"Parameter\"),\n    values_to = \"residual\"\n  ) %>%\n  \n  ## remove parts of text variables to get good identifiers for Class and Parameter\n\n  mutate(\n    Class = str_remove(\n      Class,\n      pattern = \".*\\\\[\"\n      ),\n    Parameter = str_remove(\n      Parameter,\n      pattern = \"\\\\]\"\n      )\n  ) %>% \n  \n  ## Pivot wider again to have column for each random parameter\n  \n  pivot_wider(\n    names_from = Parameter,\n    values_from = residual\n  )\n  \n\n  \n## Step 2: Create a vector of possible scores for your X variable\n\n# here I make a vector of potential values for km4week_z (set a sensible range!)\n# our km4week_z is a z score so I choose numbers between -3 and 3\nX <- seq(-15, 15, by = .1)\n\n## Step 3: Create an empty tibble that will be filled with predictions\n\nPredictions <- tibble(\n  draw = NULL,\n  X = NULL,\n  Pred1 = NULL,\n  Class = NULL\n)\n\n## Step 4: For each of our n (here 20) samples of parameter values calculate a prediction of Y\n\nfor(i in 1:400){\n\n  Si <- S_Long[i,]\n  \n  # Calculate a predicted score based on the fixed and random estimates of that draw\n  \n  Pred1 <- Si$b_Intercept + Si$Intercept + (Si$b_FirstVersion_GM + Si$FirstVersion_GM )*X  \n  \n  draw <- Si$draw\n  \n  Class <- as.factor(Si$Class)\n  \n  Pred <- tibble(\n    draw,\n    X,  \n    Pred1, \n    Class\n  )\n  \n  Predictions <- rbind(Predictions, Pred)\n}\n\n\n## Step 5: Make a plot!\n\nP1 <- Predictions %>%\n  select(draw, X, Pred1, Class) %>%\n  ggplot(aes(x = X,\n             y = Pred1,\n             group = draw)) +\n  geom_line(color = \"gray60\", alpha = .6) +\n  facet_wrap(~Class, labeller = label_both) +\n  scale_y_continuous(\"predicted values\") +\n  scale_x_continuous(\"first version (centred around the mean)\")\n\nP1\n\n\n```\n\n# Reporting stats about the posterior\n\n## Credible Intervals\n\n::::: columns\n::: {.column width=\"60%\"}\n```{r}\n# Generate a gamma distribution (that is skew)\nlibrary(bayestestR)\nset.seed(1975)\nposterior <- distribution_gamma(1000, 2.5, 2)\n\n# Compute HDI and Quantile CI\nci_hdi <- ci(posterior, method = \"HDI\")\nci_eti <- ci(posterior, method = \"ETI\")\n\n# Plot the distribution and add the limits of the two CIs\nposterior %>% \n  estimate_density(extend = TRUE) %>% \n  ggplot(aes(x = x, y = y)) +\n  geom_area(fill = \"#E6AB02\") +\n\n  # HDI in green\n  geom_vline(xintercept = ci_hdi$CI_low, color = \"#66A61E\", size = 2) +\n  geom_vline(xintercept = ci_hdi$CI_high, color = \"#66A61E\", size = 2) +\n  # ETI in purple\n  geom_vline(xintercept = ci_eti$CI_low, color = \"#7570B3\", size = 2) +\n  geom_vline(xintercept = ci_eti$CI_high, color = \"#7570B3\", size = 2) +\n  scale_y_continuous(\"posterior probability density\") +\n  scale_x_continuous(\"possible parameter values\") +\n  ggtitle(\"Skew posterior with a 89% HDI (green lines) and a 89% ETI (purple lines)\")\n```\n:::\n\n::: {.column width=\"40%\"}\n<br>\n\n-   ETI: Equal Tailed Interval\n\n<br>\n\n-   HDI: Highest Density Interval\n:::\n:::::\n\n## Concept of ROPE\n\n::::: columns\n::: {.column width=\"40%\"}\n```{r, out.height = \"99%\", out.width=\"99%\", echo = FALSE}\nknitr::include_graphics(\"Kruchke_2018.jpg\")\n```\n:::\n\n::: {.column width=\"60%\"}\n<b>ROPE</b>: Region Of Practical Equivalence\n\n<i> [Set a region of parameter values that can be considered equivalent to having no effect]{style=\"background-color: yellow\"} </i>\n\n-   in standard effect sizes the advised default is a range of -0.1 to 0.1\n\n-   this equals [1/2 of a small effect size]{style=\"color: red\"} (Cohen, 1988)\n\n-   all parameter values in that range are set equal to [no effect]{style=\"color: red\"}\n:::\n:::::\n\n## ROPE + HDI\n\n::::: columns\n::: {.column width=\"40%\"}\n```{r, out.height = \"99%\", out.width=\"99%\", echo = FALSE}\nknitr::include_graphics(\"Kruchke_2018.jpg\")\n```\n:::\n\n::: {.column width=\"60%\"}\n<b>ROPE + HDI rule</b>\n\n<br>\n\n-   95% of HDI out of ROPE $\\rightarrow$ $H_0$ rejected\n\n-   95% of HDI all in ROPE $\\rightarrow$ $H_0$ not rejected\n\n-   95% of HDI partially out of ROPE $\\rightarrow$ undecided\n:::\n:::::\n\n## Applying the HDI + ROPE rule with `bayestestR` package\n\n<br>\n\nWe can use the `equivalence_test()` function of the `bayestestR` package\n\n<br>\n\n```{r}\n#| echo: true\nlibrary(bayestestR)\nequivalence_test(M3)\n```\n\n## Visualizing the HDI + ROPE rule\n\n<br>\n\nWe visualize the `equivalence_test()` by adding `plot( )`\n\n<br>\n\n```{r}\n#| echo: true\n#| output-location: column\nequivalence_test(M3) %>%\n  plot()\n\n```\n\n## Probability of Direction (PD) with `parameters` package\n\n```{r}\n#| echo: true\nlibrary(parameters)\nmodel_parameters(\n  M3,\n  ci_method = \"hdi\",\n  rope_range = c(-1.8,1.8), #sd MarathonTimeM = 17.76 so 17.76*0.1 \n  test = c(\"rope\", \"pd\")\n  )\n```\n\n# Outro\n\n## Some books `r fontawesome::fa(\"book\")`\n\n```{r, out.height = \"90%\", out.width=\"90%\", echo = FALSE}\nknitr::include_graphics(\"cover_Lambert.jpg\")\n```\n\n## Some books `r fontawesome::fa(\"book\")`\n\n```{r, out.height = \"90%\", out.width=\"90%\", echo = FALSE}\nknitr::include_graphics(\"cover_rethinking2.jpg\")\n```\n\n## Some free online books `r fontawesome::fa(\"book\")`\n\n-   Bayes Rules!:\n\n<https://www.bayesrulesbook.com/>\n\n-   Or this book:\n\n<https://vasishth.github.io/bayescogsci/book/>\n\n## Rens van de Schoot `r fontawesome::fa(\"book\")`\n\nIn <i>Nature Reviews</i>\n\n```{r, out.height = \"90%\", out.width=\"90%\", echo = FALSE}\nknitr::include_graphics(\"Rens_NatureReviews.jpg\")\n```\n\n## THE Podcast `r fontawesome::fa(\"podcast\")`\n\nIf you like running - like I do - this could be a great companion on your run!\n\n<https://www.learnbayesstats.com/>\n\n## Site on creating the graphs `r fontawesome::fa(\"newspaper\")`\n\nThere are many blogs and websites that you can consult if you want to find out more about making graphs. <br>\n\nOne that I often fall back to is:\n\n<br>\n\n<http://mjskay.github.io/tidybayes/>\n\n## Questions?\n\n<br>\n\nDo not hesitate to contact me!\n\n<br>\n\n[sven.demaeyer\\@uantwerpen.be](mailto:sven.demaeyer@uantwerpen.be){.email}\n\n# `r fontawesome::fa(\"thumbs-up\", \"white\")` THANK YOU! {background-color=\"#447099\" transition=\"slide-in\"}","srcMarkdownNoYaml":"\n\n```{r}\nlibrary(here)\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(bayesplot)\nlibrary(ggmcmc)\nlibrary(patchwork)\nlibrary(priorsense)\n\nload(\n  file = here(\n    \"Presentations\", \n    \"WritingData.RData\")\n)\n\nM3 <-readRDS(file = \n  here(\"Presentations\",\n        \"Part3\",\n        \"M3.RDS\"\n       )\n  )\n\n# Setting a plotting theme\ntheme_set(theme_linedraw() +\n            theme(\n              text = element_text(family = \"Times\", size = 14),\n              panel.grid = element_blank()\n              )\n)\n\n```\n\n## New example data `WritingData.RData`\n\n-   Experimental study on Writing instructions\n\n-   2 conditions:\n\n    -   Control condition (Business as usual)\n    -   Experimental condition (Observational learning)\n\n```{r, out.height = \"50%\", out.width=\"50%\", echo = FALSE}\nknitr::include_graphics(here(\"Presentations\", \"Part3\",\"WritingDataDesc.jpg\"))\n```\n\n## `r fontawesome::fa(\"laptop-code\", \"white\")` Your Turn {background-color=\"#447099\" transition=\"slide-in\"}\n\n-   Open `WritingData.RData`\n\n-   Estimate 3 models with `SecondVersion` as dependent variable\n\n    -   M1: fixed effect of `FirstVersion_GM` + random effect of `Class` (`(1|Class)`)\n    -   M2: M1 + random effect of `FirstVersion_GM` (`(1 + FirstVersion_GM |Class)`)\n    -   M3: M2 + fixed effect of `Experimental_condition`\n\n-   Compare the models on their fit\n\n-   What do we learn?\n\n-   Make a summary of the best fitting model\n\n::: aside\n[Note:]{style=\"color: white\"} `FirstVersion_GM`[is the score of the pretest centred around the mean, so a score 0 for this variable implies scoring on average for the pretest]{style=\"color: white\"}\n:::\n\n## Divergent transitions...\n\n![Exploring the parameter space...](ChatGPT_explorer.png){fig-align=\"center\"}\n\n## \n\n::: center\n```{r}\n#| eval: true\n#| warning: false\n#| message: false\n#| echo: false\n\nlibrary(plotly)\nlibrary(dplyr)\n\n# Create data for a 3D valley\nset.seed(123)\nx <- seq(-5, 5, length.out = 50)\ny <- seq(-5, 5, length.out = 50)\ngrid <- expand.grid(x = x, y = y)\ngrid$height <- with(grid, 10 - sqrt(x^2 + y^2) + rnorm(length(x), 0, 0.5))\n\n# Meetpunten van drie verschillende ontdekkingsreizigers in 3D\nExplorers <- data.frame(\n  Explorer = rep(c(\"A\", \"B\", \"C\"), each = 6),\n  x = c(-4, -3, -2, -1, 0, 1,\n        4, 3, 2, 1, 0, -1,\n        0, -1, -2, -3, -4, -5),\n  y = c(-4, -3, -1.5, 0, 1, 2,\n        4, 3.5, 2.5, 1.5, 0.5, -0.5,\n        -4, -3, -2, -1, 0, 1)\n)\nExplorers$height <- 10 - sqrt(Explorers$x^2 + Explorers$y^2)\n\nColors <- c(\"A\" = \"red\", \"B\" = \"blue\", \"C\" = \"black\")\n\n# Maak 3D-plot\nfig <- plot_ly(x = ~grid$x, y = ~grid$y, z = ~grid$height,\n               type = 'mesh3d', opacity = 0.4, colorscale = 'Viridis')\n\nfor(o in unique(Explorers$Explorer)) {\n  subset <- Explorers %>% filter(Explorer == o)\n  fig <- fig %>%\n    add_trace(data = subset, x = ~x, y = ~y, z = ~height,\n              type = 'scatter3d', mode = 'lines+markers',\n              line = list(width = 4, color = Colors[o]),\n              marker = list(size = 4, color = Colors[o]),\n              name = paste(\"Explorer\", o))\n}\n\nfig <- fig %>%\n  layout(scene = list(\n    xaxis = list(title = \"Parameter X\"),\n    yaxis = list(title = \"Parameter Y\"),\n    zaxis = list(title = \"Height (posterior density)\")\n  ),\n  title = \"3D-visualisation of MCMC-sampling by multiple Explorers\")\n\nfig <- fig %>% layout(width = 900, height = 600)\n\nfig\n\n```\n:::\n\n## Divergent transitions...\n\n-   Something to worry about!\n\n-   Essentially: sampling of parameter estimate values went wrong\n\n-   Fixes:\n\n    -   sometimes fine-tuning the sampling algorithm (e.g., `control = list(adapt_delta = 0.9)`) works\n    -   sometimes you need more informative priors\n    -   sometimes the model is just not a good model\n\n## Our fix here for model 3\n\n```{r}\n#| eval: false\n#| echo: true\n\nM3 <- brm(\n  SecondVersion ~ FirstVersion_GM + Experimental_condition + (1 + FirstVersion_GM |Class),\n  data = WritingData,\n  backend = \"cmdstanr\",\n  cores = 4,\n  control = list(adapt_delta = 0.9),\n  seed = 1975 \n)\n```\n\n## Our fix here for model 3\n\n```{r, out.height = \"50%\", out.width=\"50%\", echo = FALSE}\nknitr::include_graphics(\"M3_summary.jpg\")\n```\n\n## Interpretation of results...\n\nDifferent ways to summarize our results:\n\n-   visually\n\n-   credible intervals (eti & hdi)\n\n-   rope + hdi rule\n\n-   hypothesis tests\n\n# Visually summarizing the posterior distribution\n\n## Functions in `bayesplot` package\n\n-   `mcmc_areas()` function\n\n-   `mcmc_areas_ridges()` function\n\n-   `mcmc_intervals()` function\n\n## The `mcmc_areas()` function\n\nGives a posterior distribution including a certain credible interval that you can set manually with the `prob` argument:\n\n```{r}\n#| eval: true\n#| echo: true\n#| output-location: slide\nmcmc_areas(\n  M3,\n  pars = c(\n    \"b_FirstVersion_GM\",\n    \"b_Experimental_condition\"\n  ),\n  prob = .89\n)\n```\n\n## The `mcmc_areas_ridges()` function\n\n<br>\n\nAlmost similar to the previous, only the horizontal spacing changes a bit...\n\n<br>\n\nMeanwhile, see how you can easily change the color scheme for `bayesplot` graphs\n\n<br>\n\n```{r}\n#| eval: true\n#| echo: true\n#| output-location: slide\n\ncolor_scheme_set(scheme = \"red\")\n\nmcmc_areas_ridges(\n  M3,\n  pars = c(\n    \"b_FirstVersion_GM\",\n    \"b_Experimental_condition\"\n  ),\n  prob = .89\n)\n```\n\n## The `mcmc_intervals()` function\n\nSummarizes the posterior as a horizontal bar with identifiers for two CI.\n\nHere we set one for a 50% and one for a 89% CI\n\n```{r}\n#| eval: true\n#| echo: true\n#| output-location: slide\ncolor_scheme_set(scheme = \"gray\")\n\nmcmc_intervals(\n  M3,\n  pars = c(\n    \"b_FirstVersion_GM\",\n    \"b_Experimental_condition\"\n  ),\n  prob = .5,\n  prob_outer = .89\n)\n```\n\n## Manually create visualizations\n\n<br>\n\nPowercombo: `as_draws_df()` + `ggplot2` + `ggdist`\n\n<br>\n\nWhat does `as_draws_df()` do?\n\n<br>\n\n```{r}\n#| eval: true\n#| echo: true\n#| output-location: slide\nposterior_PD <- as_draws_df(M3)\nhead(posterior_PD)\n```\n\n## Use draws to create a plot using `ggdist` geoms\n\n::::: columns\n::: {.column width=\"60%\"}\n```{r, out.height = \"98%\", out.width=\"98%\", echo = FALSE}\nknitr::include_graphics(\"ggdist_geoms.jpg\")\n```\n:::\n\n::: {.column width=\"40%\"}\n`ggdist` package has a set of functions to visualize a distribution\n:::\n:::::\n\n## An example\n\n<br>\n\nBefore we start, set our own plot theme (not so necessary)\n\n<br>\n\n```{r}\n#| echo: true\n\n# Setting a plotting theme\ntheme_set(theme_linedraw() +\n            theme(\n              text = element_text(family = \"Times\", size = 14),\n              panel.grid = element_blank()\n              )\n)\n\n```\n\n## An example\n\n<br>\n\nWe use `posterior_PD` as a starting point (our draws)\n\n```{r}\n#| message: false\n#| warning: false\n#| error: false\n#| echo: true\n#| output-location: slide\n\nlibrary(ggdist)\n\nPlot <- ggplot(\n  posterior_PD,\n  aes(x = b_Experimental_condition)\n  ) +\n  stat_halfeye()\n\nPlot + scale_y_continuous(name = \"\", breaks = NULL)\n```\n\n## Change the CI's\n\n<br>\n\nChange the CI's to 50% and 89%\n\n<br>\n\n```{r}\n#| message: false\n#| warning: false\n#| error: false\n#| echo: true\n#| code-line-numbers: \"6\"\n#| output-location: slide\nPlot <- ggplot(\n  posterior_PD,\n  aes(x = b_Experimental_condition)\n  ) +\n  stat_halfeye(\n    .width = c(.50,.89)\n  )\n\nPlot + scale_y_continuous(name = \"\", breaks = NULL)\n```\n\n## Use another visualization\n\n<br>\n\nLet's make a dotplot... (research shows this is best interpreted) with 100 dots\n\n<br>\n\n```{r}\n#| message: false\n#| warning: false\n#| error: false\n#| echo: true\n#| code-line-numbers: \"5|6\"\n#| output-location: slide\nPlot <- ggplot(\n  posterior_PD,\n  aes(x = b_Experimental_condition)\n  ) +\n  stat_dotsinterval(\n    quantiles = 100,\n    .width = c(.50,.89)\n  )\n\nPlot + scale_y_continuous(name = \"\", breaks = NULL)\n```\n\n## Plot two parameters each in a facet\n\n<br>\n\nWe use `pivot_longer(everything())` to stack information on multiple parameters\n\n<br>\n\n```{r}\n#| message: false\n#| warning: false\n#| error: false\n#| echo: true\n#| output-location: slide\nposterior_PD %>% \n  select(\n    b_Experimental_condition, b_FirstVersion_GM\n  ) %>% \n  pivot_longer(everything()) %>%\n  ggplot(\n    aes(x = value)\n  ) +\n  stat_halfeye(\n    .width = c(.50,.89)\n  ) +\nfacet_wrap(name ~ .) +\nscale_y_continuous(name = \"\", breaks = NULL)\n\n```\n\n## Visualize calculated predictions based on posterior\n\nOur example: 2 groups according to `Experimental_condition`\n\nHow to visualize the posterior probability of averages for both groups?\n\n```{r}\n#| message: false\n#| warning: false\n#| error: false\n#| echo: true\n#| code-line-numbers: \"6|7|10\"\n#| output-location: slide\nposterior_PD %>% \n  select(\n    b_Intercept, b_Experimental_condition\n  ) %>% \n  mutate(\n    Mean_Control_condition = b_Intercept,\n    Mean_Experimental_condition = b_Intercept + b_Experimental_condition\n  ) %>% \n  select(\n    Mean_Control_condition, Mean_Experimental_condition\n  ) %>% \n  pivot_longer(everything()) %>%\n  ggplot(\n    aes(x = value, color = name, fill = name)\n  ) +\n  stat_halfeye(\n    .width = c(.50,.89),\n    alpha = .40\n  ) + \n  scale_y_continuous(name = \"\", breaks = NULL)\n```\n\n## Hypothetical Outcome Plots (HOPs)\n\nCode: see separate script called `HOP_script.R`\n\n```{r}\n### CREATING HYPOTHETICAL OUTCOME PLOTS FOR BRMS MODEL ###\n\n# make sure your model is loaded in your R session\n# here we will apply it to the model M3\n\n## Step 1: sample n number of parameter values from the posterior\n\n# here we sample n = 20\n\nS <- as_draws_df(M3) %>% \n  select(\n    # select the necessary parameters to calculate the predicted scores\n    b_Intercept, \n    b_FirstVersion_GM,\n  ) %>%\n  slice_sample(\n    n = 20, # define the number of lines (draws) you want\n    replace = T\n  )\n\n## Step 2: Create a vector of possible scores for your X variable\n\n# here I make a vector of potential values for km4week_z (set a sensible range!)\n# our km4week_z is a z score so I choose numbers between -3 and 3\nX <- seq(-15, 15, by = .1)\n\n## Step 3: Create an empty tibble that will be filled with predictions\n\nPredictions <- tibble(\n  draw = NULL,\n  X = NULL,\n  Pred1 = NULL  \n)\n\n## Step 4: For each of our n (here 20) samples of parameter values calculate a prediction of Y\n\nfor(i in 1:20){\n\n  Si <- S[i,]\n  \n  Pred1 <- Si$b_Intercept + Si$b_FirstVersion_GM*X\n  \n  draw <- rep(i,length(X))\n  \n  Pred <- tibble(\n    draw,\n    X,  \n    Pred1, \n  )\n  \n  Predictions <- rbind(Predictions, Pred)\n}\n\n# Check the result of our predictions\n\nhead(Predictions)\n\n## Step 5: Make a plot!\n\nP1 <- Predictions %>%\n  select(draw, X, Pred1) %>%\n  ggplot(aes(x = X,\n             y = Pred1,\n             group = draw)) +\n  geom_line(color = \"gray60\", alpha = .6) +\n  scale_y_continuous(\"predicted values\") +\n  scale_x_continuous(\"first version (centred around the mean)\")\n\nP1\n```\n\n# Visualizing Random Effects\n\n## Plotting the residuals\n\nTo plot differences between classes we can use class-specific residuals:\n\n```{r}\n#| message: false\n#| warning: false\n#| error: false\n#| echo: true\nhead(as_draws_df(M3) %>% \n  select(ends_with(\",Intercept]\")) %>%\n  select(1:3),\n  5\n)\n```\n\n## Plotting the residuals\n\n```{r}\n#| message: false\n#| warning: false\n#| error: false\n#| echo: true\n#| output-location: slide\nas_draws_df(M3) %>% \n  select(ends_with(\",Intercept]\")) %>%\n  pivot_longer(starts_with(\"r_Class\")) %>% \n  mutate(sigma_i = value) %>%\n  ggplot(aes(x = sigma_i, y = reorder(name, sigma_i))) +\n  stat_pointinterval(\n    point_interval = mean_qi, \n    .width = .89, \n    size = 1/6) +\n  scale_y_discrete(expression(italic(j)), breaks = NULL) +\n  labs(x = expression(italic(u)[italic(j)])) +\n  coord_flip()\n```\n\n## ICC estimation\n\n```{r}\n#| message: false\n#| warning: false\n#| error: false\n#| echo: true\nhead(\n  as_draws_df(M3) %>%\n    mutate(\n      ICC = (sd_Class__Intercept^2/(sd_Class__Intercept^2 + sigma^2))) %>%\n    select(sigma, sd_Class__Intercept, ICC), \n  5\n  ) \n```\n\n## ICC estimation\n\n```{r}\n#| message: false\n#| warning: false\n#| error: false\n#| echo: true\n#| output-location: slide\n\nas_draws_df(M3) %>%\n  mutate(\n    ICC = (sd_Class__Intercept^2/(sd_Class__Intercept^2 + sigma^2))\n    ) %>%\n  select(ICC) %>%                           \n  ggplot(aes(x = ICC)) +                    \n   stat_dotsinterval(\n     quantiles = 100,\n     .width = c(.50,.89)\n   ) +\n   scale_x_continuous(\"marginal posterior\", \n                      breaks = seq(.00,.60,by =.05)) + \n   scale_y_continuous(\"ICC\", breaks = NULL)\n```\n\n## HOP per higher level unit\n\nCode: see separate script called `HOP_MixedEffects_script.R`\n\n```{r}\n\n\n### CREATING HYPOTHETICAL OUTCOME PLOTS FOR BRMS MODEL ###\n\n# make sure your model is loaded in your R session\n# here we will apply it to the model M3\n\n## Step 1: sample n number of parameter values from the posterior\n\n# here we sample n = 20\nlibrary(posterior)\nS <- as_draws_df(M3) %>% \n  select(\n    # select the necessary parameters to calculate the predicted scores\n    b_Intercept, \n    b_FirstVersion_GM,\n    ends_with(\",Intercept]\"),       # select class specific intercept residuals\n    ends_with(\",FirstVersion_GM]\")  # select class specific slope residuals\n  ) %>%\n  slice_sample(\n    n = 20, # define the number of lines (draws) you want per class\n    replace = T\n  ) %>%\n  mutate(\n    draw = seq(1:20)\n  )\n\n## Create long dataframe\n\nS_Long <- S %>%\n\n    ## Pivot longer\n\n    pivot_longer(\n    cols = c(\n      ends_with(\",Intercept]\"),       # select class specific intercept residuals\n      ends_with(\",FirstVersion_GM]\")  # select class specific slope residuals\n    ),\n    names_sep = \",\",\n    names_to = c(\"Class\", \"Parameter\"),\n    values_to = \"residual\"\n  ) %>%\n  \n  ## remove parts of text variables to get good identifiers for Class and Parameter\n\n  mutate(\n    Class = str_remove(\n      Class,\n      pattern = \".*\\\\[\"\n      ),\n    Parameter = str_remove(\n      Parameter,\n      pattern = \"\\\\]\"\n      )\n  ) %>% \n  \n  ## Pivot wider again to have column for each random parameter\n  \n  pivot_wider(\n    names_from = Parameter,\n    values_from = residual\n  )\n  \n\n  \n## Step 2: Create a vector of possible scores for your X variable\n\n# here I make a vector of potential values for km4week_z (set a sensible range!)\n# our km4week_z is a z score so I choose numbers between -3 and 3\nX <- seq(-15, 15, by = .1)\n\n## Step 3: Create an empty tibble that will be filled with predictions\n\nPredictions <- tibble(\n  draw = NULL,\n  X = NULL,\n  Pred1 = NULL,\n  Class = NULL\n)\n\n## Step 4: For each of our n (here 20) samples of parameter values calculate a prediction of Y\n\nfor(i in 1:400){\n\n  Si <- S_Long[i,]\n  \n  # Calculate a predicted score based on the fixed and random estimates of that draw\n  \n  Pred1 <- Si$b_Intercept + Si$Intercept + (Si$b_FirstVersion_GM + Si$FirstVersion_GM )*X  \n  \n  draw <- Si$draw\n  \n  Class <- as.factor(Si$Class)\n  \n  Pred <- tibble(\n    draw,\n    X,  \n    Pred1, \n    Class\n  )\n  \n  Predictions <- rbind(Predictions, Pred)\n}\n\n\n## Step 5: Make a plot!\n\nP1 <- Predictions %>%\n  select(draw, X, Pred1, Class) %>%\n  ggplot(aes(x = X,\n             y = Pred1,\n             group = draw)) +\n  geom_line(color = \"gray60\", alpha = .6) +\n  facet_wrap(~Class, labeller = label_both) +\n  scale_y_continuous(\"predicted values\") +\n  scale_x_continuous(\"first version (centred around the mean)\")\n\nP1\n\n\n```\n\n# Reporting stats about the posterior\n\n## Credible Intervals\n\n::::: columns\n::: {.column width=\"60%\"}\n```{r}\n# Generate a gamma distribution (that is skew)\nlibrary(bayestestR)\nset.seed(1975)\nposterior <- distribution_gamma(1000, 2.5, 2)\n\n# Compute HDI and Quantile CI\nci_hdi <- ci(posterior, method = \"HDI\")\nci_eti <- ci(posterior, method = \"ETI\")\n\n# Plot the distribution and add the limits of the two CIs\nposterior %>% \n  estimate_density(extend = TRUE) %>% \n  ggplot(aes(x = x, y = y)) +\n  geom_area(fill = \"#E6AB02\") +\n\n  # HDI in green\n  geom_vline(xintercept = ci_hdi$CI_low, color = \"#66A61E\", size = 2) +\n  geom_vline(xintercept = ci_hdi$CI_high, color = \"#66A61E\", size = 2) +\n  # ETI in purple\n  geom_vline(xintercept = ci_eti$CI_low, color = \"#7570B3\", size = 2) +\n  geom_vline(xintercept = ci_eti$CI_high, color = \"#7570B3\", size = 2) +\n  scale_y_continuous(\"posterior probability density\") +\n  scale_x_continuous(\"possible parameter values\") +\n  ggtitle(\"Skew posterior with a 89% HDI (green lines) and a 89% ETI (purple lines)\")\n```\n:::\n\n::: {.column width=\"40%\"}\n<br>\n\n-   ETI: Equal Tailed Interval\n\n<br>\n\n-   HDI: Highest Density Interval\n:::\n:::::\n\n## Concept of ROPE\n\n::::: columns\n::: {.column width=\"40%\"}\n```{r, out.height = \"99%\", out.width=\"99%\", echo = FALSE}\nknitr::include_graphics(\"Kruchke_2018.jpg\")\n```\n:::\n\n::: {.column width=\"60%\"}\n<b>ROPE</b>: Region Of Practical Equivalence\n\n<i> [Set a region of parameter values that can be considered equivalent to having no effect]{style=\"background-color: yellow\"} </i>\n\n-   in standard effect sizes the advised default is a range of -0.1 to 0.1\n\n-   this equals [1/2 of a small effect size]{style=\"color: red\"} (Cohen, 1988)\n\n-   all parameter values in that range are set equal to [no effect]{style=\"color: red\"}\n:::\n:::::\n\n## ROPE + HDI\n\n::::: columns\n::: {.column width=\"40%\"}\n```{r, out.height = \"99%\", out.width=\"99%\", echo = FALSE}\nknitr::include_graphics(\"Kruchke_2018.jpg\")\n```\n:::\n\n::: {.column width=\"60%\"}\n<b>ROPE + HDI rule</b>\n\n<br>\n\n-   95% of HDI out of ROPE $\\rightarrow$ $H_0$ rejected\n\n-   95% of HDI all in ROPE $\\rightarrow$ $H_0$ not rejected\n\n-   95% of HDI partially out of ROPE $\\rightarrow$ undecided\n:::\n:::::\n\n## Applying the HDI + ROPE rule with `bayestestR` package\n\n<br>\n\nWe can use the `equivalence_test()` function of the `bayestestR` package\n\n<br>\n\n```{r}\n#| echo: true\nlibrary(bayestestR)\nequivalence_test(M3)\n```\n\n## Visualizing the HDI + ROPE rule\n\n<br>\n\nWe visualize the `equivalence_test()` by adding `plot( )`\n\n<br>\n\n```{r}\n#| echo: true\n#| output-location: column\nequivalence_test(M3) %>%\n  plot()\n\n```\n\n## Probability of Direction (PD) with `parameters` package\n\n```{r}\n#| echo: true\nlibrary(parameters)\nmodel_parameters(\n  M3,\n  ci_method = \"hdi\",\n  rope_range = c(-1.8,1.8), #sd MarathonTimeM = 17.76 so 17.76*0.1 \n  test = c(\"rope\", \"pd\")\n  )\n```\n\n# Outro\n\n## Some books `r fontawesome::fa(\"book\")`\n\n```{r, out.height = \"90%\", out.width=\"90%\", echo = FALSE}\nknitr::include_graphics(\"cover_Lambert.jpg\")\n```\n\n## Some books `r fontawesome::fa(\"book\")`\n\n```{r, out.height = \"90%\", out.width=\"90%\", echo = FALSE}\nknitr::include_graphics(\"cover_rethinking2.jpg\")\n```\n\n## Some free online books `r fontawesome::fa(\"book\")`\n\n-   Bayes Rules!:\n\n<https://www.bayesrulesbook.com/>\n\n-   Or this book:\n\n<https://vasishth.github.io/bayescogsci/book/>\n\n## Rens van de Schoot `r fontawesome::fa(\"book\")`\n\nIn <i>Nature Reviews</i>\n\n```{r, out.height = \"90%\", out.width=\"90%\", echo = FALSE}\nknitr::include_graphics(\"Rens_NatureReviews.jpg\")\n```\n\n## THE Podcast `r fontawesome::fa(\"podcast\")`\n\nIf you like running - like I do - this could be a great companion on your run!\n\n<https://www.learnbayesstats.com/>\n\n## Site on creating the graphs `r fontawesome::fa(\"newspaper\")`\n\nThere are many blogs and websites that you can consult if you want to find out more about making graphs. <br>\n\nOne that I often fall back to is:\n\n<br>\n\n<http://mjskay.github.io/tidybayes/>\n\n## Questions?\n\n<br>\n\nDo not hesitate to contact me!\n\n<br>\n\n[sven.demaeyer\\@uantwerpen.be](mailto:sven.demaeyer@uantwerpen.be){.email}\n\n# `r fontawesome::fa(\"thumbs-up\", \"white\")` THANK YOU! {background-color=\"#447099\" transition=\"slide-in\"}"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","self-contained":true,"output-file":"Slides_Part3.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.6.39","auto-stretch":true,"title":"Applied Bayesian Analyses in R","subtitle":"Part 3","author":"Sven De Maeyer","editor":"visual","code":{"code-copy":true,"code-line-numbers":true},"code-annotations":"hover","theme":["default","zurich2026-slides-clean.scss"],"width":1422,"height":805,"slideNumber":true}}},"projectFormats":["html","revealjs"]}