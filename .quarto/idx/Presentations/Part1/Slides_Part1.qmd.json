{"title":"Applied Bayesian Analyses in R","markdown":{"yaml":{"title":"Applied Bayesian Analyses in R","subtitle":"Part1","author":"Sven De Maeyer","format":{"revealjs":{"theme":["default","zurich2026-slides-clean.scss"],"width":1422,"height":805,"slide-number":true,"include-in-header":{"text":"<style>\n.center-xy {\n  margin: 0;\n  position: absolute;\n  top: 50%;\n  -ms-transform: translateY(-50%);\n  transform: translateY(-50%);\n}\n</style>\n"}}},"editor":"visual","self-contained":true,"execute":{"echo":false,"include":true,"output":true},"code":{"code-copy":true,"code-line-numbers":true},"code-annotations":"hover"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\n```{css echo=FALSE}\n.small-code{\n  font-size: 75%  \n}\n```\n\n## Welcome\n\nLet's get to know each other first!\n\n-   what's your name?\n-   a 1/2-minute pitch on your research\n-   any experience with Bayesian inference?\n-   what do you hope to learn?\n\n## Outline\n\nPart 1: Introduction to Bayesian inferences and some basics in `brms`\n\nPart 2: Checking the model with the WAMBS and how to do it in `R`\n\nPart 3: Reporting and interpreting Bayesian models\n\nPart 4: Analysing your own data or integrated exercise\n\nThroughout the different presentations we will also increase model complexity\n\n## Practical stuff\n\nAll material is on the on-line dashboard:\n\n<https://abar-turku-2025.netlify.app/>\n\nYou can find:\n\n-   an overview of the course\n\n-   how to prepare\n\n-   for each day the slides and datasets\n\n-   html slides can be exported to pdf in your browser (use the menu button bottom right)\n\n# Statistical Inference\n\n## Why statistical inference?\n\n::: center-xy\nWe want to learn more about [a population]{style=\"color: #d28e43\"} but we have [sample data]{style=\"color: #d28e43\"}!\n\nTherefore, we have [uncertainty]{style=\"color: #d28e43\"}\n:::\n\n## Why statistical inference? an example\n\n> What is the effect of average number of kilometers ran per week on the marathon time?\n\nWe can estimate the effect by using a regression model based on sample data:\n\n$$\\text{MarathonTime}_i = \\beta_0 + \\beta_1 * \\text{n_km}_i + \\epsilon_i$$\n\nOf course, we are not interested in the value of $\\beta_1$ in the sample, but we want to learn more on $\\beta_1$ in the population!\n\n## Frequentistic statistical inference (with data...)\n\nMaximal Likelihood estimation:\n\n::: small-code\n```{r}\n## Libraries\n\nlibrary(here)\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(bayesplot)\n\n## Load the data\n\nMarathonData <- read_csv(\n  file = here(\"Presentations\", \"MarathonData.csv\")\n)\n\nMarathonData <- MarathonData |>\n  mutate(\n    MarathonTimeM = MarathonTime * 60,\n    km4week = km4week - mean(km4week, na.rm = T)\n  )\n\nModel1 <- lm(MarathonTimeM ~ km4week, data = MarathonData)\n\nsummary(Model1)\n```\n:::\n\n::: aside\nData comes from <https://www.kaggle.com/datasets/girardi69/marathon-time-predictions?resource=download>\n:::\n\n## Frequentistic statistical inference (interpretation ...)\n\n```{r}\n#| echo: false\nlower <- -0.509 - (1.96*.07)\nhigher <- -0.509 + (1.96*.07)\n```\n\nSample: for each km more a week approx half a minute faster\n\nPopulation:\n\n-   p-value = prob to observe a \\|0.509\\| estimate in our sample if effect is zero in the population is lower than 0.05\n-   CI = in 95% of possible samples the calculation of a CI would result in an interval containing the true population value\n\n==\\> \\[`r lower` ; `r higher`\\] so: -0.6 is as equally probable as -0.4 ...\n\n## Bayesian statistical inference\n\n```{r}\nModel1_running <- readRDS(\n  here(\"Presentations\", \"Output\", \"Model1_running.RDS\")\n)\n\n\nmcmc_areas(\n  Model1_running,\n  pars = c(\"b_km4week\"),\n  prob = 0.89\n) + theme_minimal()\n```\n\n## Frequentist compared to Bayesian inference\n\n<br> <br> You can explore a tutorial App of J. Krushke (2019) <br> <br> <https://iupbsapps.shinyapps.io/KruschkeFreqAndBayesApp/>\n\n<br> <br> He has also written a nice tutorial, linked to this App: <br> <br> <https://jkkweb.sitehost.iu.edu/KruschkeFreqAndBayesAppTutorial.html>\n\n## Advantages & Disadvantages of Bayesian analyses\n\n::::: columns\n::: {.column width=\"50%\"}\nAdvantages:\n\n-   Natural approach to express uncertainty\n-   Ability to incorporate prior knowledge\n-   Increased model flexibility\n-   Full posterior distribution of the parameters\n-   Natural propagation of uncertainty\n:::\n\n::: {.column width=\"50%\"}\nDisadvantage:\n\n-   Slow speed of model estimation\n-   Some reviewers don't understand you (<i>\"give me the p-value\"</i>)\n:::\n:::::\n\n::: aside\nSlight adaptation of a slide from Paul B√ºrkner's presentation available on YouTube: <https://www.youtube.com/watch?v=FRs1iribZME>\n:::\n\n# Bayesian Inference\n\n## Bayesian Theorem\n\n::::: columns\n::: {.column width=\"50%\"}\n```{r, out.height = \"50%\", out.width=\"50%\", echo = FALSE}\nknitr::include_graphics(\"prior_data_posterior.png\")\n```\n:::\n\n::: {.column width=\"50%\"}\n$$\nP(\\theta|data) = \\frac{P(data|\\theta)P(\\theta)}{P(data)}\n$$ with\n\n-   $P(data|\\theta)$ : the [likelihood]{style=\"color: #d28e43\"} of the data given our model $\\theta$\n-   $P(\\theta)$ : our [prior]{style=\"color: #d28e43\"} belief about values for the model parameters\n-   $P(\\theta|data)$: the [posterior]{style=\"color: #d28e43\"} probability for model parameter values\n:::\n:::::\n\n::: aside\nmeme from <https://twitter.com/ChelseaParlett/status/1421291716229746689?s=20>\n:::\n\n## Likelihood\n\n$P(data|\\theta)$ : the [likelihood]{style=\"color: #d28e43\"} of the data given our model $\\theta$\n\nThis part is about our [MODEL]{style=\"background-color: yellow; color: red\"} and the parameters (aka the unknowns) in that model\n\n> Example: a model on \"time needed to run a marathon\" could be a normal distribution: $y_i \\backsim N(\\mu, \\sigma)$\n\nSo: for a certain average ($\\mu$) and standard deviation ($\\sigma$) we can calculate the probability of observing a marathon time of 240 minutes\n\n## Prior\n\nExpression of our [prior knowledge (belief)]{style=\"background-color: yellow; color: red\"} on the parameter values\n\n> Example: a model on \"time needed to run a marathon\" could be a normal distribution $y_i \\backsim N(\\mu, \\sigma)$\n\ne.g., How probable is an average marathon time of 120 minutes ($P(\\mu=120)$) and how probable is a standard deviation of 40 minutes ($P(\\sigma=40)$)?\n\n## Prior as a distribution\n\nHow probable are different values as average marathon time in a population?\n\n```{r}\nX <- seq(120, 300, by = 0.1)\n\nProb <- dnorm(\n  X, \n  210,\n  30\n  )\n\ndata_frame(X, Prob) %>%\n  ggplot(aes(x = X, y = Prob)) + \n    geom_line() +\n    labs(\n      x = expression(mu),\n      y = \"density\"\n    ) +\n  theme_linedraw()\n```\n\n## Types of priors\n\n::::: columns\n::: {.column width=\"50%\"}\n[Uninformative/Weakly informative]{style=\"color: #d28e43\"}\n\nWhen objectivity is crucial and you want *let the data speak for itself...*\n:::\n\n::: {.column width=\"50%\"}\n[Informative]{style=\"color: #d28e43\"}\n\nWhen including significant information is crucial\n\n-   previously collected data\n-   results from former research/analyses\n-   data of another source\n-   theoretical considerations\n-   elicitation\n:::\n:::::\n\n## Type of priors (visually)\n\n```{r}\nX <- seq(120, 300, by = 0.1)\n\nWeakly_Informative <- dnorm(\n  X, \n  210,\n  30\n  )\n\nUninformative <- dunif(\n  X,\n  min = 130,\n  max = 290\n)\n\n\ndata_frame(X, Weakly_Informative, Uninformative) %>%\n  pivot_longer(c(Weakly_Informative, Uninformative)) %>%\n  rename(Prior = name) %>%\n  ggplot(aes(x = X, y = value, lty = Prior)) + \n    geom_line() +\n    labs(\n      x = expression(mu),\n      y = \"density\"\n    ) +\n  theme_linedraw()\n```\n\nTwo potential priors for the *mean marathon time* in the population\n\n## Bayesian theorem in stats\n\n<br> <br> *\"For Bayesians, the data are treated as fixed and the parameters vary. \\[...\\] Bayes' rule tells us that to calculate the posterior probability distribution we must combine a likelihood with a prior probability distribution over parameter values.\"* <br> (Lambert, 2018, p.88)\n\n## Let's apply this idea\n\nOur [**Model**]{style=\"color: #d28e43\"} for marathon times\n\n$y_i \\backsim N(\\mu, \\sigma)$\n\n## Let's apply this idea\n\nOur [**Priors**]{style=\"color: #d28e43\"} related to mu and sigma:\n\n```{r, echo = TRUE}\npar(mfrow=c(2,2))\ncurve( dnorm( x , 210 , 30 ) , from=120 , to=300 ,xlab=\"mu\", main=\"Prior for mu\")\ncurve( dunif( x , 1 , 40 ) , from=-5 , to=50 ,xlab=\"sigma\", main=\"Prior for sigma\")\n```\n\n## Let's apply this idea\n\nOur [**data**]{style=\"color: #d28e43\"}:\n\n```{r}\nMT <- c(\n  185,\n  193,\n  240,\n  245,\n  155, ## whoow; flying!\n  234,\n  189,\n  196,\n  206,\n  263)\nMT\n```\n\n## Let's apply this idea\n\nWhat is the [**posterior probability**]{style=\"color: #d28e43\"} of [mu = 180]{style=\"background-color: yellow; color: red\"} combined with a [sigma = 15]{style=\"background-color: yellow; color: red\"}?\n\n```{r}\n#| echo: false\n\noptions(scipen = 1000000)\n```\n\n::::: columns\n::: {.column width=\"50%\"}\n```{r}\n#| echo: true\n\n# Calculate the likelihood of the data\n# given these parameter values of\n# mu = 180 ; sigma = 15\n# remember MT contains our data\nLikelihood <- \n  sum(\n      dnorm(\n        MT ,\n        mean=180 ,\n        sd=15)\n      )\nLikelihood\n\n```\n:::\n\n::: {.column width=\"50%\"}\n```{r}\n#| echo: true\n\n# Calculate our Prior belief\n\nPrior <- dnorm(180, 210 , 30 ) * dunif(15 , 1 , 40 )\n\nPrior\n\n```\n:::\n:::::\n\n```{r}\n#| echo: true\n\n# Calculate posterior as product \n# of Likelihood and Prior\n\nProduct <- Likelihood * Prior\n\nProduct\n\n```\n\n## Let's apply this idea\n\nWhat is the [**posterior probability**]{style=\"color: #d28e43\"} of [mu = 210]{style=\"background-color: yellow; color: red\"} combined with a [sigma = 30]{style=\"background-color: yellow; color: red\"}?\n\n::::: columns\n::: {.column width=\"50%\"}\n```{r}\n#| echo: true\n\n# Calculate the likelihood of the data\n# given these parameter values of\n# mu = 210 ; sigma = 30\n# remember MT contains our data\nLikelihood <- \n  sum(\n      dnorm(\n        MT ,\n        mean=210 ,\n        sd=30)\n      )\nLikelihood\n\n```\n:::\n\n::: {.column width=\"50%\"}\n```{r}\n#| echo: true\n\n# Calculate our Prior belief\n\nPrior <- dnorm(210, 210 , 30 ) * dunif(30 , 1 , 40 )\n\nPrior\n\n```\n:::\n:::::\n\n```{r}\n#| echo: true\n\n# Calculate posterior as product \n# of Likelihood and Prior\n\nProduct <- Likelihood * Prior\n\nProduct\n\n```\n\n## Grid approximation\n\nWe calculated the posterior probability of a combination of 2 parameters\n\nWe could repeat this systematically for following values:\n\n```{r}\n#| echo: true\n\n# sample some values for mu and sigma\nmu.list <- seq(from = 135, \n               to = 300, \n               length.out=400)\nsigma.list <- seq(from = 1, \n                  to = 40, \n                  length.out = 400)\n\n```\n\naka: we create a grid of possible parameter value combinations and approx the distribution of posterior probabilities\n\n## Grid approximation applied\n\n```{r}\npost <- expand.grid(mu = mu.list, \n                    sigma = sigma.list)\n\npost$LL <- \n  sapply(1:nrow(post), \n    function(i) sum(\n      dnorm(\n        MT ,\n        mean=post$mu[i] ,\n        sd=post$sigma[i] ,\n        log=TRUE ) ) )\n\npost$prod <- post$LL + \n  dnorm(post$mu, 210 , 30 , TRUE) + \n  dunif(post$sigma , 1 , 40 , TRUE)\n\n# Re-scale the posterior \n# to the probability scale\npost$prob <- exp(post$prod - \n                   max(post$prod)\n                 )\n\nset.seed(1975)\n\npost %>%\n  # sample 10000 rows\n  # with replacement\n  # higher prob higher prob to \n  # be sampled\n  sample_n(size = 15000, \n           replace = TRUE, \n           weight = prob) %>%\n  \n  # create the plot\n  ggplot(aes(x = mu, y = sigma)) +\n  geom_density_2d_filled() + \n  annotate(geom=\"text\", x=180, y=15, label=\"C1\",\n              color=\"red\") +\n  annotate(geom=\"text\", x=210, y=30, label=\"C2\",\n              color=\"red\") +\n  scale_y_continuous(limits = c(10,40)) +\n  theme_minimal()\n```\n\nC1: first combo of mu and sigma\n\n& C2: second combo of mu and sigma\n\n## Sampling parameter values\n\n-   Instead of a fixed grid of combinations of parameter values,\n\n-   sample pairs/triplets/... of parameter values\n\n-   reconstruct the posterior probability distribution\n\n# Why `brms`?\n\n## Imagine\n\nA 'simple' linear model\n\n<br>\n\n$$\\begin{aligned}\n  & MT_{i}  \\sim N(\\mu,\\sigma_{e_{i}})\\\\\n  & \\mu = \\beta_0 + \\beta_1*\\text{sp4week}_{i} + \\beta_2*\\text{km4week}_{i} + \\\\ \n  & \\beta_3*\\text{Age}_{i} + \\beta_4*\\text{Gender}_{i} + \\beta_5*\\text{CrossTraining}_{i} \\\\\n\\end{aligned}$$\n\n<br>\n\nSo you can get a REALLY LARGE number of parameters!\n\n## Markov Chain Monte Carlo - Why?\n\nComplex models $\\rightarrow$ Large number of parameters $\\rightarrow$ exponentionally large number of combinations!\n\n<br>\n\nPosterior gets unsolvable by grid approximation\n\n<br>\n\nWe will approximate the 'joint posterior' by ['smart' sampling]{style=\"color: #d28e43\"}\n\n<br>\n\nSamples of combinations of parameter values are drawn\n\n<br>\n\nBUT: samples will not be random!\n\n## MCMC - demonstrated\n\n<br> <br> Following link brings you to an interactive tool that let's you get the basic idea behind MCMC sampling:\n\n<br>\n\n<https://chi-feng.github.io/mcmc-demo/app.html#HamiltonianMC,standard>\n\n<br>\n\n## Software\n\n<br>\n\n-   different dedicated software/packages are available: JAGS / BUGS / Stan\n\n-   most powerful is [Stan]{style=\"color: #d28e43\"}! Specifically the *Hamiltonian Monte Carlo* algorithm makes it the best choice at the moment\n\n-   [Stan]{style=\"color: #d28e43\"} is a probabilistic programming language that uses C++\n\n## Example of Stan code\n\n```{r echo = F}\nload(here(\"Presentations\", \"Part1\", \"Model_math_naive.R\"))\nstancode(Model_math_naive)\n```\n\n## How `brms` works\n\n![](brms_procedure.jpeg)\n\n# The basics of `brms`\n\n## `brms` syntax\n\nVery very similar to `lm` or `lme4` and in line with typical R-style writing up of a model ...\n\n::::: columns\n::: {.column width=\"50%\"}\n`lme4`\n\n```{r}\n#| echo: true\n#| eval: false\n\nModel <- lmer(\n  y ~ x1 + x2 + (1|Group),\n  data = Data,\n\n  ...\n)\n```\n:::\n\n::: {.column width=\"50%\"}\n`brms`\n\n```{r}\n#| echo: true\n#| eval: false\n\nModel <- brm(\n  y ~ x1 + x2 + (1|Group),\n  data = Data,\n  family = \"gaussian\",\n  backend = \"cmdstanr\",\n  \n  ...\n)\n```\n:::\n:::::\n\nNotice:\n\n-   `backend = \"cmdstanr\"` indicates the way we want to interact with Stan and C++\n\n## Let's retake the example on running\n\nThe simplest model looked like:\n\n$$\nMT_i \\sim N(\\mu,\\sigma_e)\n$$\n\nIn `brms` this model is:\n\n```{r echo = TRUE, eval = FALSE}\nMT <- c(185, 193, 240, 245, 155, 234, 189, 196, 206, 263) # <1>\n\n# First make a dataset from our MT vector\nDataMT <- data_frame(MT)  # <2>\n\nMod_MT1 <- brm(                        # <3>\n  MT ~ 1, # We only model an intercept # <4>\n  data = DataMT,                       # <5>  \n  backend = \"cmdstanr\",                # <6>\n  seed = 1975                          # <7>\n)\n\n```\n\n1.  create a dataset with potential marathon times\n2.  change it to a data frame\n3.  the brm() commando runs the model\n4.  define the model\n5.  define the dataset to us\n6.  how brms and stan communicate\n7.  make the estimation reproducible\n\n<b> üèÉ Try it yourself and run the model ... (don't forget to load the necessary packages: `brms` & `tidyverse`) </b>\n\n## Basic output `brms`\n\n```{r}\nMod_MT1<- readRDS(\n  file = here(\"Presentations\",\n              \"Part1\",\n              \"Mod_MT1.RDS\")\n  )\n```\n\n```{r, highlight.output = c(10,14)}\n#| echo: true\n#| output-location: slide\nsummary(Mod_MT1)\n```\n\n## About chains and iterations\n\nMarkov [Chain]{style=\"color: #d28e43\"} Monte Carlo\n\n-   A chain of samples of parameter values\n-   It is advised to run [multiple chains]{style=\"color: #d28e43\"}\n-   Each sample of parameter values is called an [iteration]{style=\"color: #d28e43\"}\n-   First X iterations are used to tune the sampler but not used to interpret the results: X burn-in iterations\n\n`brms` defaults with **4 chains** each of **2000 iterations** of which **1000 iterations** are used as burn-in\n\n## Changing `brms` defaults\n\n```{r}\n#| eval: false\n#| echo: true\n#| code-line-numbers: \"5|6|7|8\"\nMod_MT1 <- brm(                        \n  MT ~ 1, \n  data = DataMT,   \n  backend = \"cmdstanr\",\n  chains = 5,\n  iter = 6000,\n  warmup = 1500,\n  cores = 4,\n  seed = 1975           \n)\n```\n\n## Good old `plot()` function\n\n```{r}\n#| echo: true\n#| output-location: slide\nplot(Mod_MT1)\n```\n\n## `r fontawesome::fa(\"laptop-code\", \"white\")` Your Turn {background-color=\"#82654f\" transition=\"slide-in\"}\n\n-   Open `MarathonData.RData`\n-   Estimate your first Bayesian Models\n-   Dependent variable: `MarathonTimeM`\n-   Model1: only an intercept\n-   Model2: introduce the effect of `km4week` and `sp4week` on `MarathonTimeM`\n-   Change the `brms` defaults (4 chains of 6000 iterations)\n-   Make plots with the `plot()` function\n-   What do we learn? (rough interpretation)\n\n## Results Exercise\n\n*Model1: only an intercept (brms defaults)*\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\n\nload(file = here(\n    \"Presentations\",\n    \"MarathonData.RData\")\n    )\n\nMod_MT1 <- brm(                        \n  MarathonTimeM ~ 1, \n  data = MarathonData,   \n  backend = \"cmdstanr\",\n  seed = 1975           \n)\n```\n\n## Results Exercise\n\n*Model1: only an intercept (brms defaults)*\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\nsummary(Mod_MT1)\n```\n\n## Results Exercise\n\n*Model1: only an intercept (brms defaults)*\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\nplot(Mod_MT1)\n```\n\n## Results Exercise\n\n*Model2: add the effect of* `km4week` *and* `sp4week`\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\n\nMod_MT2 <- brm(                        \n  MarathonTimeM ~ 1 + km4week + sp4week, \n  data = MarathonData,   \n  backend = \"cmdstanr\",\n  seed = 1975           \n)\n```\n\n## Results Exercise\n\n*Model2: add the effect of* `km4week` *and* `sp4week`\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\nsummary(Mod_MT2)\n```\n\n## Results Exercise\n\n*Model2: add the effect of* `km4week` *and* `sp4week`\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\nplot(Mod_MT2)\n```\n\n## Results Exercise\n\n*Model3: change the brms defaults*\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\n\nMod_MT3 <- brm(                        \n  MarathonTimeM ~ 1 + km4week + sp4week, \n  data = MarathonData,\n  chains = 4,\n  iter = 4000,\n  cores = 4,\n  backend = \"cmdstanr\",\n  seed = 1975           \n)\n```\n\n## Results Exercise\n\n*Model3: change the brms defaults*\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\nsummary(Mod_MT3)\n```\n\n## Results Exercise\n\n*Model3: change the brms defaults*\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\nplot(Mod_MT3)\n```\n\n## About convergence\n\n::::: columns\n::: {.column width=\"50%\"}\n```{r, out.height = \"99%\", out.width=\"99%\", echo = FALSE}\nknitr::include_graphics(\"Vethari_paper.jpg\")\n```\n:::\n\n::: {.column width=\"50%\"}\n-   $\\widehat R$ \\< 1.015 for each parameter estimate\n\n-   at least 4 chains are recommended\n\n-   Effective Sample Size (ESS) \\> 400 to rely on $\\widehat R$\n:::\n:::::\n\n## But is it a good model?\n\n<br> <br> Two complementary procedures: <br> <br>\n\n-   posterior-predictive check\n\n-   compare models with <i>[leave one out cross-validation]{style=\"color:#d28e43\"}</i>\n\n## Posterior-predictive check\n\nA visual check that can be performed with `pp_check()` from `brms`\n\n```{r, fig.width = 5, fig.height = 5}\n#| echo: true\n#| output-location: slide\n\nMarathonTimes_Mod2 <-\n  readRDS(file = \n            here(\"Presentations\",\n              \"Output\",\n              \"MarathonTimes_Mod2.RDS\")\n          )\n\npp_check(MarathonTimes_Mod2) + theme_minimal()\n```\n\n## Model comparison with loo cross-validation\n\n$\\sim$ AIC or BIC in Frequentist statistics\n\n$\\widehat{elpd}$: \"expected log predictive density\" (higher $\\widehat{elpd}$ implies better model fit without being sensitive for overfitting!)\n\n```{r}\nMarathonTimes_Mod1 <-\n  readRDS(file = \n            here(\"Presentations\",\n              \"Output\",\n              \"MarathonTimes_Mod1.RDS\")\n          )\n```\n\n```{r}\n#| eval: true\n#| echo: true\n#| output-location: slide\n\nloo_Mod1 <- loo(MarathonTimes_Mod1)\nloo_Mod2 <- loo(MarathonTimes_Mod2)\n\nComparison<- \n  loo_compare(\n    loo_Mod1, \n    loo_Mod2\n    )\n\nprint(Comparison, simplify = F)\n```\n\n## `r fontawesome::fa(\"laptop-code\", \"white\")` Your Turn {background-color=\"#82654f\" transition=\"slide-in\"}\n\n-   Time to switch to `your dataset`\n-   Think of `two alternative models` for a certain variable\n-   Be naive! Let's `assume normality` and\n-   `Keep it simple`\n-   Estimate the two models\n-   Compare the models\n-   Interpret the best fitting model\n-   Check the fit with `pp_check()`\n\n## Family time\n\n`brms` defaults to `family = gaussian(link = \"identity\")`\n\n```{r}\n#| eval: false\n#| echo: true\n#| code-line-numbers: \"3\"\nMod_MT1 <- brm(                        \n  MT ~ 1, \n  family = gaussian(link = \"identity\"),\n  data = DataMT,   \n  backend = \"cmdstanr\",\n  chains = 5,\n  iter = 6000,\n  warmup = 1500,\n  cores = 4,\n  seed = 1975           \n)\n```\n\n## Family time\n\n<br>\n\nBut many alternatives are available!\n\n<br>\n\nThe default `family` types known in `R` (e.g, `binomial(link = \"logit\")`, `Gamma(link = \"inverse\")`, `poisson(link = \"log\")`, ...)\n\n<br>\n\nsee `help(family)`\n\n<br>\n\n```{r}\n#| eval: false\n#| echo: true\n#| code-line-numbers: \"3\"\nModX <- brm(                        \n  Y ~ 1, \n  family = binomial(link = \"logit\"),\n  ...\n)\n```\n\n## Family time\n\n<br>\n\nAnd even more!\n\n<br>\n\n`brmsfamily(...)` has a lot of possible models\n\n<br>\n\nsee `help(brmsfamily)`\n\n<br>\n\n```{r}\n#| eval: false\n#| echo: true\n#| code-line-numbers: \"3\"\nModX <- brm(                        \n  Y ~ 1, \n  family = brmsfamily(skew_normal, \n                      link = \"identity\", \n                      link_sigma = \"log\", \n                      link_alpha = \"identity\"),\n  ...\n)\n```\n\n## Mixed effects models\n\n<br>\n\n`brms` can estimate models for more complex designs like `multilevel models` or more generally `mixed effects models`\n\n<br> Random intercepts...\n\n```{r}\n#| eval: false\n#| echo: true\n#| code-line-numbers: \"3\"\nModX <- brm(                        \n  Y ~ 1 + x + \n    (1 | Groupvariable), \n  ...\n)\n```\n\nRandom intercepts and slopes...\n\n```{r}\n#| eval: false\n#| echo: true\n#| code-line-numbers: \"3\"\nModX <- brm(                        \n  Y ~ 1 + x + \n    (1 + x | Groupvariable), \n  ...\n)\n```\n\n# `r fontawesome::fa(\"laptop-code\", \"white\")` Homework {background-color=\"#82654f\" transition=\"slide-in\"}\n\nTime to think about `your data` and research question\n\n-   Define a simple model?\n-   What are the parameters in that model?\n-   What are your `prior beliefs` about each of the parameters?\n-   Minimum and maximum values assumed for each parameter?\n-   Do all potential parameter values have a similar probability?\n-   Bring along your notes to the next session!","srcMarkdownNoYaml":"\n\n# Introduction\n\n```{css echo=FALSE}\n.small-code{\n  font-size: 75%  \n}\n```\n\n## Welcome\n\nLet's get to know each other first!\n\n-   what's your name?\n-   a 1/2-minute pitch on your research\n-   any experience with Bayesian inference?\n-   what do you hope to learn?\n\n## Outline\n\nPart 1: Introduction to Bayesian inferences and some basics in `brms`\n\nPart 2: Checking the model with the WAMBS and how to do it in `R`\n\nPart 3: Reporting and interpreting Bayesian models\n\nPart 4: Analysing your own data or integrated exercise\n\nThroughout the different presentations we will also increase model complexity\n\n## Practical stuff\n\nAll material is on the on-line dashboard:\n\n<https://abar-turku-2025.netlify.app/>\n\nYou can find:\n\n-   an overview of the course\n\n-   how to prepare\n\n-   for each day the slides and datasets\n\n-   html slides can be exported to pdf in your browser (use the menu button bottom right)\n\n# Statistical Inference\n\n## Why statistical inference?\n\n::: center-xy\nWe want to learn more about [a population]{style=\"color: #d28e43\"} but we have [sample data]{style=\"color: #d28e43\"}!\n\nTherefore, we have [uncertainty]{style=\"color: #d28e43\"}\n:::\n\n## Why statistical inference? an example\n\n> What is the effect of average number of kilometers ran per week on the marathon time?\n\nWe can estimate the effect by using a regression model based on sample data:\n\n$$\\text{MarathonTime}_i = \\beta_0 + \\beta_1 * \\text{n_km}_i + \\epsilon_i$$\n\nOf course, we are not interested in the value of $\\beta_1$ in the sample, but we want to learn more on $\\beta_1$ in the population!\n\n## Frequentistic statistical inference (with data...)\n\nMaximal Likelihood estimation:\n\n::: small-code\n```{r}\n## Libraries\n\nlibrary(here)\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(bayesplot)\n\n## Load the data\n\nMarathonData <- read_csv(\n  file = here(\"Presentations\", \"MarathonData.csv\")\n)\n\nMarathonData <- MarathonData |>\n  mutate(\n    MarathonTimeM = MarathonTime * 60,\n    km4week = km4week - mean(km4week, na.rm = T)\n  )\n\nModel1 <- lm(MarathonTimeM ~ km4week, data = MarathonData)\n\nsummary(Model1)\n```\n:::\n\n::: aside\nData comes from <https://www.kaggle.com/datasets/girardi69/marathon-time-predictions?resource=download>\n:::\n\n## Frequentistic statistical inference (interpretation ...)\n\n```{r}\n#| echo: false\nlower <- -0.509 - (1.96*.07)\nhigher <- -0.509 + (1.96*.07)\n```\n\nSample: for each km more a week approx half a minute faster\n\nPopulation:\n\n-   p-value = prob to observe a \\|0.509\\| estimate in our sample if effect is zero in the population is lower than 0.05\n-   CI = in 95% of possible samples the calculation of a CI would result in an interval containing the true population value\n\n==\\> \\[`r lower` ; `r higher`\\] so: -0.6 is as equally probable as -0.4 ...\n\n## Bayesian statistical inference\n\n```{r}\nModel1_running <- readRDS(\n  here(\"Presentations\", \"Output\", \"Model1_running.RDS\")\n)\n\n\nmcmc_areas(\n  Model1_running,\n  pars = c(\"b_km4week\"),\n  prob = 0.89\n) + theme_minimal()\n```\n\n## Frequentist compared to Bayesian inference\n\n<br> <br> You can explore a tutorial App of J. Krushke (2019) <br> <br> <https://iupbsapps.shinyapps.io/KruschkeFreqAndBayesApp/>\n\n<br> <br> He has also written a nice tutorial, linked to this App: <br> <br> <https://jkkweb.sitehost.iu.edu/KruschkeFreqAndBayesAppTutorial.html>\n\n## Advantages & Disadvantages of Bayesian analyses\n\n::::: columns\n::: {.column width=\"50%\"}\nAdvantages:\n\n-   Natural approach to express uncertainty\n-   Ability to incorporate prior knowledge\n-   Increased model flexibility\n-   Full posterior distribution of the parameters\n-   Natural propagation of uncertainty\n:::\n\n::: {.column width=\"50%\"}\nDisadvantage:\n\n-   Slow speed of model estimation\n-   Some reviewers don't understand you (<i>\"give me the p-value\"</i>)\n:::\n:::::\n\n::: aside\nSlight adaptation of a slide from Paul B√ºrkner's presentation available on YouTube: <https://www.youtube.com/watch?v=FRs1iribZME>\n:::\n\n# Bayesian Inference\n\n## Bayesian Theorem\n\n::::: columns\n::: {.column width=\"50%\"}\n```{r, out.height = \"50%\", out.width=\"50%\", echo = FALSE}\nknitr::include_graphics(\"prior_data_posterior.png\")\n```\n:::\n\n::: {.column width=\"50%\"}\n$$\nP(\\theta|data) = \\frac{P(data|\\theta)P(\\theta)}{P(data)}\n$$ with\n\n-   $P(data|\\theta)$ : the [likelihood]{style=\"color: #d28e43\"} of the data given our model $\\theta$\n-   $P(\\theta)$ : our [prior]{style=\"color: #d28e43\"} belief about values for the model parameters\n-   $P(\\theta|data)$: the [posterior]{style=\"color: #d28e43\"} probability for model parameter values\n:::\n:::::\n\n::: aside\nmeme from <https://twitter.com/ChelseaParlett/status/1421291716229746689?s=20>\n:::\n\n## Likelihood\n\n$P(data|\\theta)$ : the [likelihood]{style=\"color: #d28e43\"} of the data given our model $\\theta$\n\nThis part is about our [MODEL]{style=\"background-color: yellow; color: red\"} and the parameters (aka the unknowns) in that model\n\n> Example: a model on \"time needed to run a marathon\" could be a normal distribution: $y_i \\backsim N(\\mu, \\sigma)$\n\nSo: for a certain average ($\\mu$) and standard deviation ($\\sigma$) we can calculate the probability of observing a marathon time of 240 minutes\n\n## Prior\n\nExpression of our [prior knowledge (belief)]{style=\"background-color: yellow; color: red\"} on the parameter values\n\n> Example: a model on \"time needed to run a marathon\" could be a normal distribution $y_i \\backsim N(\\mu, \\sigma)$\n\ne.g., How probable is an average marathon time of 120 minutes ($P(\\mu=120)$) and how probable is a standard deviation of 40 minutes ($P(\\sigma=40)$)?\n\n## Prior as a distribution\n\nHow probable are different values as average marathon time in a population?\n\n```{r}\nX <- seq(120, 300, by = 0.1)\n\nProb <- dnorm(\n  X, \n  210,\n  30\n  )\n\ndata_frame(X, Prob) %>%\n  ggplot(aes(x = X, y = Prob)) + \n    geom_line() +\n    labs(\n      x = expression(mu),\n      y = \"density\"\n    ) +\n  theme_linedraw()\n```\n\n## Types of priors\n\n::::: columns\n::: {.column width=\"50%\"}\n[Uninformative/Weakly informative]{style=\"color: #d28e43\"}\n\nWhen objectivity is crucial and you want *let the data speak for itself...*\n:::\n\n::: {.column width=\"50%\"}\n[Informative]{style=\"color: #d28e43\"}\n\nWhen including significant information is crucial\n\n-   previously collected data\n-   results from former research/analyses\n-   data of another source\n-   theoretical considerations\n-   elicitation\n:::\n:::::\n\n## Type of priors (visually)\n\n```{r}\nX <- seq(120, 300, by = 0.1)\n\nWeakly_Informative <- dnorm(\n  X, \n  210,\n  30\n  )\n\nUninformative <- dunif(\n  X,\n  min = 130,\n  max = 290\n)\n\n\ndata_frame(X, Weakly_Informative, Uninformative) %>%\n  pivot_longer(c(Weakly_Informative, Uninformative)) %>%\n  rename(Prior = name) %>%\n  ggplot(aes(x = X, y = value, lty = Prior)) + \n    geom_line() +\n    labs(\n      x = expression(mu),\n      y = \"density\"\n    ) +\n  theme_linedraw()\n```\n\nTwo potential priors for the *mean marathon time* in the population\n\n## Bayesian theorem in stats\n\n<br> <br> *\"For Bayesians, the data are treated as fixed and the parameters vary. \\[...\\] Bayes' rule tells us that to calculate the posterior probability distribution we must combine a likelihood with a prior probability distribution over parameter values.\"* <br> (Lambert, 2018, p.88)\n\n## Let's apply this idea\n\nOur [**Model**]{style=\"color: #d28e43\"} for marathon times\n\n$y_i \\backsim N(\\mu, \\sigma)$\n\n## Let's apply this idea\n\nOur [**Priors**]{style=\"color: #d28e43\"} related to mu and sigma:\n\n```{r, echo = TRUE}\npar(mfrow=c(2,2))\ncurve( dnorm( x , 210 , 30 ) , from=120 , to=300 ,xlab=\"mu\", main=\"Prior for mu\")\ncurve( dunif( x , 1 , 40 ) , from=-5 , to=50 ,xlab=\"sigma\", main=\"Prior for sigma\")\n```\n\n## Let's apply this idea\n\nOur [**data**]{style=\"color: #d28e43\"}:\n\n```{r}\nMT <- c(\n  185,\n  193,\n  240,\n  245,\n  155, ## whoow; flying!\n  234,\n  189,\n  196,\n  206,\n  263)\nMT\n```\n\n## Let's apply this idea\n\nWhat is the [**posterior probability**]{style=\"color: #d28e43\"} of [mu = 180]{style=\"background-color: yellow; color: red\"} combined with a [sigma = 15]{style=\"background-color: yellow; color: red\"}?\n\n```{r}\n#| echo: false\n\noptions(scipen = 1000000)\n```\n\n::::: columns\n::: {.column width=\"50%\"}\n```{r}\n#| echo: true\n\n# Calculate the likelihood of the data\n# given these parameter values of\n# mu = 180 ; sigma = 15\n# remember MT contains our data\nLikelihood <- \n  sum(\n      dnorm(\n        MT ,\n        mean=180 ,\n        sd=15)\n      )\nLikelihood\n\n```\n:::\n\n::: {.column width=\"50%\"}\n```{r}\n#| echo: true\n\n# Calculate our Prior belief\n\nPrior <- dnorm(180, 210 , 30 ) * dunif(15 , 1 , 40 )\n\nPrior\n\n```\n:::\n:::::\n\n```{r}\n#| echo: true\n\n# Calculate posterior as product \n# of Likelihood and Prior\n\nProduct <- Likelihood * Prior\n\nProduct\n\n```\n\n## Let's apply this idea\n\nWhat is the [**posterior probability**]{style=\"color: #d28e43\"} of [mu = 210]{style=\"background-color: yellow; color: red\"} combined with a [sigma = 30]{style=\"background-color: yellow; color: red\"}?\n\n::::: columns\n::: {.column width=\"50%\"}\n```{r}\n#| echo: true\n\n# Calculate the likelihood of the data\n# given these parameter values of\n# mu = 210 ; sigma = 30\n# remember MT contains our data\nLikelihood <- \n  sum(\n      dnorm(\n        MT ,\n        mean=210 ,\n        sd=30)\n      )\nLikelihood\n\n```\n:::\n\n::: {.column width=\"50%\"}\n```{r}\n#| echo: true\n\n# Calculate our Prior belief\n\nPrior <- dnorm(210, 210 , 30 ) * dunif(30 , 1 , 40 )\n\nPrior\n\n```\n:::\n:::::\n\n```{r}\n#| echo: true\n\n# Calculate posterior as product \n# of Likelihood and Prior\n\nProduct <- Likelihood * Prior\n\nProduct\n\n```\n\n## Grid approximation\n\nWe calculated the posterior probability of a combination of 2 parameters\n\nWe could repeat this systematically for following values:\n\n```{r}\n#| echo: true\n\n# sample some values for mu and sigma\nmu.list <- seq(from = 135, \n               to = 300, \n               length.out=400)\nsigma.list <- seq(from = 1, \n                  to = 40, \n                  length.out = 400)\n\n```\n\naka: we create a grid of possible parameter value combinations and approx the distribution of posterior probabilities\n\n## Grid approximation applied\n\n```{r}\npost <- expand.grid(mu = mu.list, \n                    sigma = sigma.list)\n\npost$LL <- \n  sapply(1:nrow(post), \n    function(i) sum(\n      dnorm(\n        MT ,\n        mean=post$mu[i] ,\n        sd=post$sigma[i] ,\n        log=TRUE ) ) )\n\npost$prod <- post$LL + \n  dnorm(post$mu, 210 , 30 , TRUE) + \n  dunif(post$sigma , 1 , 40 , TRUE)\n\n# Re-scale the posterior \n# to the probability scale\npost$prob <- exp(post$prod - \n                   max(post$prod)\n                 )\n\nset.seed(1975)\n\npost %>%\n  # sample 10000 rows\n  # with replacement\n  # higher prob higher prob to \n  # be sampled\n  sample_n(size = 15000, \n           replace = TRUE, \n           weight = prob) %>%\n  \n  # create the plot\n  ggplot(aes(x = mu, y = sigma)) +\n  geom_density_2d_filled() + \n  annotate(geom=\"text\", x=180, y=15, label=\"C1\",\n              color=\"red\") +\n  annotate(geom=\"text\", x=210, y=30, label=\"C2\",\n              color=\"red\") +\n  scale_y_continuous(limits = c(10,40)) +\n  theme_minimal()\n```\n\nC1: first combo of mu and sigma\n\n& C2: second combo of mu and sigma\n\n## Sampling parameter values\n\n-   Instead of a fixed grid of combinations of parameter values,\n\n-   sample pairs/triplets/... of parameter values\n\n-   reconstruct the posterior probability distribution\n\n# Why `brms`?\n\n## Imagine\n\nA 'simple' linear model\n\n<br>\n\n$$\\begin{aligned}\n  & MT_{i}  \\sim N(\\mu,\\sigma_{e_{i}})\\\\\n  & \\mu = \\beta_0 + \\beta_1*\\text{sp4week}_{i} + \\beta_2*\\text{km4week}_{i} + \\\\ \n  & \\beta_3*\\text{Age}_{i} + \\beta_4*\\text{Gender}_{i} + \\beta_5*\\text{CrossTraining}_{i} \\\\\n\\end{aligned}$$\n\n<br>\n\nSo you can get a REALLY LARGE number of parameters!\n\n## Markov Chain Monte Carlo - Why?\n\nComplex models $\\rightarrow$ Large number of parameters $\\rightarrow$ exponentionally large number of combinations!\n\n<br>\n\nPosterior gets unsolvable by grid approximation\n\n<br>\n\nWe will approximate the 'joint posterior' by ['smart' sampling]{style=\"color: #d28e43\"}\n\n<br>\n\nSamples of combinations of parameter values are drawn\n\n<br>\n\nBUT: samples will not be random!\n\n## MCMC - demonstrated\n\n<br> <br> Following link brings you to an interactive tool that let's you get the basic idea behind MCMC sampling:\n\n<br>\n\n<https://chi-feng.github.io/mcmc-demo/app.html#HamiltonianMC,standard>\n\n<br>\n\n## Software\n\n<br>\n\n-   different dedicated software/packages are available: JAGS / BUGS / Stan\n\n-   most powerful is [Stan]{style=\"color: #d28e43\"}! Specifically the *Hamiltonian Monte Carlo* algorithm makes it the best choice at the moment\n\n-   [Stan]{style=\"color: #d28e43\"} is a probabilistic programming language that uses C++\n\n## Example of Stan code\n\n```{r echo = F}\nload(here(\"Presentations\", \"Part1\", \"Model_math_naive.R\"))\nstancode(Model_math_naive)\n```\n\n## How `brms` works\n\n![](brms_procedure.jpeg)\n\n# The basics of `brms`\n\n## `brms` syntax\n\nVery very similar to `lm` or `lme4` and in line with typical R-style writing up of a model ...\n\n::::: columns\n::: {.column width=\"50%\"}\n`lme4`\n\n```{r}\n#| echo: true\n#| eval: false\n\nModel <- lmer(\n  y ~ x1 + x2 + (1|Group),\n  data = Data,\n\n  ...\n)\n```\n:::\n\n::: {.column width=\"50%\"}\n`brms`\n\n```{r}\n#| echo: true\n#| eval: false\n\nModel <- brm(\n  y ~ x1 + x2 + (1|Group),\n  data = Data,\n  family = \"gaussian\",\n  backend = \"cmdstanr\",\n  \n  ...\n)\n```\n:::\n:::::\n\nNotice:\n\n-   `backend = \"cmdstanr\"` indicates the way we want to interact with Stan and C++\n\n## Let's retake the example on running\n\nThe simplest model looked like:\n\n$$\nMT_i \\sim N(\\mu,\\sigma_e)\n$$\n\nIn `brms` this model is:\n\n```{r echo = TRUE, eval = FALSE}\nMT <- c(185, 193, 240, 245, 155, 234, 189, 196, 206, 263) # <1>\n\n# First make a dataset from our MT vector\nDataMT <- data_frame(MT)  # <2>\n\nMod_MT1 <- brm(                        # <3>\n  MT ~ 1, # We only model an intercept # <4>\n  data = DataMT,                       # <5>  \n  backend = \"cmdstanr\",                # <6>\n  seed = 1975                          # <7>\n)\n\n```\n\n1.  create a dataset with potential marathon times\n2.  change it to a data frame\n3.  the brm() commando runs the model\n4.  define the model\n5.  define the dataset to us\n6.  how brms and stan communicate\n7.  make the estimation reproducible\n\n<b> üèÉ Try it yourself and run the model ... (don't forget to load the necessary packages: `brms` & `tidyverse`) </b>\n\n## Basic output `brms`\n\n```{r}\nMod_MT1<- readRDS(\n  file = here(\"Presentations\",\n              \"Part1\",\n              \"Mod_MT1.RDS\")\n  )\n```\n\n```{r, highlight.output = c(10,14)}\n#| echo: true\n#| output-location: slide\nsummary(Mod_MT1)\n```\n\n## About chains and iterations\n\nMarkov [Chain]{style=\"color: #d28e43\"} Monte Carlo\n\n-   A chain of samples of parameter values\n-   It is advised to run [multiple chains]{style=\"color: #d28e43\"}\n-   Each sample of parameter values is called an [iteration]{style=\"color: #d28e43\"}\n-   First X iterations are used to tune the sampler but not used to interpret the results: X burn-in iterations\n\n`brms` defaults with **4 chains** each of **2000 iterations** of which **1000 iterations** are used as burn-in\n\n## Changing `brms` defaults\n\n```{r}\n#| eval: false\n#| echo: true\n#| code-line-numbers: \"5|6|7|8\"\nMod_MT1 <- brm(                        \n  MT ~ 1, \n  data = DataMT,   \n  backend = \"cmdstanr\",\n  chains = 5,\n  iter = 6000,\n  warmup = 1500,\n  cores = 4,\n  seed = 1975           \n)\n```\n\n## Good old `plot()` function\n\n```{r}\n#| echo: true\n#| output-location: slide\nplot(Mod_MT1)\n```\n\n## `r fontawesome::fa(\"laptop-code\", \"white\")` Your Turn {background-color=\"#82654f\" transition=\"slide-in\"}\n\n-   Open `MarathonData.RData`\n-   Estimate your first Bayesian Models\n-   Dependent variable: `MarathonTimeM`\n-   Model1: only an intercept\n-   Model2: introduce the effect of `km4week` and `sp4week` on `MarathonTimeM`\n-   Change the `brms` defaults (4 chains of 6000 iterations)\n-   Make plots with the `plot()` function\n-   What do we learn? (rough interpretation)\n\n## Results Exercise\n\n*Model1: only an intercept (brms defaults)*\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\n\nload(file = here(\n    \"Presentations\",\n    \"MarathonData.RData\")\n    )\n\nMod_MT1 <- brm(                        \n  MarathonTimeM ~ 1, \n  data = MarathonData,   \n  backend = \"cmdstanr\",\n  seed = 1975           \n)\n```\n\n## Results Exercise\n\n*Model1: only an intercept (brms defaults)*\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\nsummary(Mod_MT1)\n```\n\n## Results Exercise\n\n*Model1: only an intercept (brms defaults)*\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\nplot(Mod_MT1)\n```\n\n## Results Exercise\n\n*Model2: add the effect of* `km4week` *and* `sp4week`\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\n\nMod_MT2 <- brm(                        \n  MarathonTimeM ~ 1 + km4week + sp4week, \n  data = MarathonData,   \n  backend = \"cmdstanr\",\n  seed = 1975           \n)\n```\n\n## Results Exercise\n\n*Model2: add the effect of* `km4week` *and* `sp4week`\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\nsummary(Mod_MT2)\n```\n\n## Results Exercise\n\n*Model2: add the effect of* `km4week` *and* `sp4week`\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\nplot(Mod_MT2)\n```\n\n## Results Exercise\n\n*Model3: change the brms defaults*\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\n\nMod_MT3 <- brm(                        \n  MarathonTimeM ~ 1 + km4week + sp4week, \n  data = MarathonData,\n  chains = 4,\n  iter = 4000,\n  cores = 4,\n  backend = \"cmdstanr\",\n  seed = 1975           \n)\n```\n\n## Results Exercise\n\n*Model3: change the brms defaults*\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\nsummary(Mod_MT3)\n```\n\n## Results Exercise\n\n*Model3: change the brms defaults*\n\n```{r}\n#| eval: true\n#| message: false\n#| warning: false\n#| echo: true\nplot(Mod_MT3)\n```\n\n## About convergence\n\n::::: columns\n::: {.column width=\"50%\"}\n```{r, out.height = \"99%\", out.width=\"99%\", echo = FALSE}\nknitr::include_graphics(\"Vethari_paper.jpg\")\n```\n:::\n\n::: {.column width=\"50%\"}\n-   $\\widehat R$ \\< 1.015 for each parameter estimate\n\n-   at least 4 chains are recommended\n\n-   Effective Sample Size (ESS) \\> 400 to rely on $\\widehat R$\n:::\n:::::\n\n## But is it a good model?\n\n<br> <br> Two complementary procedures: <br> <br>\n\n-   posterior-predictive check\n\n-   compare models with <i>[leave one out cross-validation]{style=\"color:#d28e43\"}</i>\n\n## Posterior-predictive check\n\nA visual check that can be performed with `pp_check()` from `brms`\n\n```{r, fig.width = 5, fig.height = 5}\n#| echo: true\n#| output-location: slide\n\nMarathonTimes_Mod2 <-\n  readRDS(file = \n            here(\"Presentations\",\n              \"Output\",\n              \"MarathonTimes_Mod2.RDS\")\n          )\n\npp_check(MarathonTimes_Mod2) + theme_minimal()\n```\n\n## Model comparison with loo cross-validation\n\n$\\sim$ AIC or BIC in Frequentist statistics\n\n$\\widehat{elpd}$: \"expected log predictive density\" (higher $\\widehat{elpd}$ implies better model fit without being sensitive for overfitting!)\n\n```{r}\nMarathonTimes_Mod1 <-\n  readRDS(file = \n            here(\"Presentations\",\n              \"Output\",\n              \"MarathonTimes_Mod1.RDS\")\n          )\n```\n\n```{r}\n#| eval: true\n#| echo: true\n#| output-location: slide\n\nloo_Mod1 <- loo(MarathonTimes_Mod1)\nloo_Mod2 <- loo(MarathonTimes_Mod2)\n\nComparison<- \n  loo_compare(\n    loo_Mod1, \n    loo_Mod2\n    )\n\nprint(Comparison, simplify = F)\n```\n\n## `r fontawesome::fa(\"laptop-code\", \"white\")` Your Turn {background-color=\"#82654f\" transition=\"slide-in\"}\n\n-   Time to switch to `your dataset`\n-   Think of `two alternative models` for a certain variable\n-   Be naive! Let's `assume normality` and\n-   `Keep it simple`\n-   Estimate the two models\n-   Compare the models\n-   Interpret the best fitting model\n-   Check the fit with `pp_check()`\n\n## Family time\n\n`brms` defaults to `family = gaussian(link = \"identity\")`\n\n```{r}\n#| eval: false\n#| echo: true\n#| code-line-numbers: \"3\"\nMod_MT1 <- brm(                        \n  MT ~ 1, \n  family = gaussian(link = \"identity\"),\n  data = DataMT,   \n  backend = \"cmdstanr\",\n  chains = 5,\n  iter = 6000,\n  warmup = 1500,\n  cores = 4,\n  seed = 1975           \n)\n```\n\n## Family time\n\n<br>\n\nBut many alternatives are available!\n\n<br>\n\nThe default `family` types known in `R` (e.g, `binomial(link = \"logit\")`, `Gamma(link = \"inverse\")`, `poisson(link = \"log\")`, ...)\n\n<br>\n\nsee `help(family)`\n\n<br>\n\n```{r}\n#| eval: false\n#| echo: true\n#| code-line-numbers: \"3\"\nModX <- brm(                        \n  Y ~ 1, \n  family = binomial(link = \"logit\"),\n  ...\n)\n```\n\n## Family time\n\n<br>\n\nAnd even more!\n\n<br>\n\n`brmsfamily(...)` has a lot of possible models\n\n<br>\n\nsee `help(brmsfamily)`\n\n<br>\n\n```{r}\n#| eval: false\n#| echo: true\n#| code-line-numbers: \"3\"\nModX <- brm(                        \n  Y ~ 1, \n  family = brmsfamily(skew_normal, \n                      link = \"identity\", \n                      link_sigma = \"log\", \n                      link_alpha = \"identity\"),\n  ...\n)\n```\n\n## Mixed effects models\n\n<br>\n\n`brms` can estimate models for more complex designs like `multilevel models` or more generally `mixed effects models`\n\n<br> Random intercepts...\n\n```{r}\n#| eval: false\n#| echo: true\n#| code-line-numbers: \"3\"\nModX <- brm(                        \n  Y ~ 1 + x + \n    (1 | Groupvariable), \n  ...\n)\n```\n\nRandom intercepts and slopes...\n\n```{r}\n#| eval: false\n#| echo: true\n#| code-line-numbers: \"3\"\nModX <- brm(                        \n  Y ~ 1 + x + \n    (1 + x | Groupvariable), \n  ...\n)\n```\n\n# `r fontawesome::fa(\"laptop-code\", \"white\")` Homework {background-color=\"#82654f\" transition=\"slide-in\"}\n\nTime to think about `your data` and research question\n\n-   Define a simple model?\n-   What are the parameters in that model?\n-   What are your `prior beliefs` about each of the parameters?\n-   Minimum and maximum values assumed for each parameter?\n-   Do all potential parameter values have a similar probability?\n-   Bring along your notes to the next session!"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","self-contained":true,"include-in-header":{"text":"<style>\n.center-xy {\n  margin: 0;\n  position: absolute;\n  top: 50%;\n  -ms-transform: translateY(-50%);\n  transform: translateY(-50%);\n}\n</style>\n"},"output-file":"Slides_Part1.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.6.39","auto-stretch":true,"title":"Applied Bayesian Analyses in R","subtitle":"Part1","author":"Sven De Maeyer","editor":"visual","code":{"code-copy":true,"code-line-numbers":true},"code-annotations":"hover","theme":["default","zurich2026-slides-clean.scss"],"width":1422,"height":805,"slideNumber":true}}},"projectFormats":["html"]}