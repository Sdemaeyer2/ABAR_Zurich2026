[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Bayesian Analyses with R (Zurich 2026)",
    "section": "",
    "text": "In a broad range of scientific disciplines, Bayesian statistics are gaining popularity. Yet the basic training of most researchers only introduces the frequentist framework for statistical inference. This workshop aims to introduce Bayesian statistics in a practical way, laying a foundation for crucial concepts in the Bayesian realm.\nParticipants will be introduced to a workflow for Bayesian analyses in the open-source environment R. The starting point will be the linear model (aka regression model) and we can extend this to linear mixed models. The central piece of software will be the brms package in R, that bridges the typical R modelling language with Stan, which is a probabilistic programming language built to estimate models within the Bayesian framework. Given that a workflow in R will be introduced, it is advised that participants have some experience working with R. Moreover, packages will be used from the tidyverse (e.g., dplyr, ggplot2, …). Therefore, we advise participants to acknowledge themselves with these packages.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Introduction to Bayesian Analyses with R (Zurich 2026)",
    "section": "",
    "text": "In a broad range of scientific disciplines, Bayesian statistics are gaining popularity. Yet the basic training of most researchers only introduces the frequentist framework for statistical inference. This workshop aims to introduce Bayesian statistics in a practical way, laying a foundation for crucial concepts in the Bayesian realm.\nParticipants will be introduced to a workflow for Bayesian analyses in the open-source environment R. The starting point will be the linear model (aka regression model) and we can extend this to linear mixed models. The central piece of software will be the brms package in R, that bridges the typical R modelling language with Stan, which is a probabilistic programming language built to estimate models within the Bayesian framework. Given that a workflow in R will be introduced, it is advised that participants have some experience working with R. Moreover, packages will be used from the tidyverse (e.g., dplyr, ggplot2, …). Therefore, we advise participants to acknowledge themselves with these packages.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Introduction to Bayesian Analyses with R (Zurich 2026)",
    "section": "Schedule",
    "text": "Schedule\n\n\n\nWhen?\nWhere\n\n\n\n\nThursday 5th of February\n\n\n\nFriday 6th of February\n\n\n\n\nThis site bundles all the material used for the course. Or you can download all the materials on the github page .",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "Integrated_exercise/Exercise_response.html#subtask-2.1-what-about-the-priors",
    "href": "Integrated_exercise/Exercise_response.html#subtask-2.1-what-about-the-priors",
    "title": "Integrated excercise",
    "section": "Subtask 2.1: what about the priors?",
    "text": "Subtask 2.1: what about the priors?\nWhat are the default brms priors? Do they make sense? Do they generate impossible datasets? If necessary, specify your own (weakly informative) priors and approach them critically as well.\n\n Potential Solution \nLet’s start with a prior predictive check.\n\n\nCode\nM3_priors &lt;- brm(\n  fluency ~ 1 + Occ2*FL + Occ2*MT + Occ3*FL + Occ3*MT + (1|student),\n  data = Subtitles,\n  cores = 4,\n  backend = \"cmdstanr\",\n  seed = 1975,\n  sample_prior = \"only\"\n)\n\npp_check(M3_priors)\n\n\nAs you might notice, you will get an error message saying that sampling from the priors is not possible. This is due to the fact that brms by default uses flat priors which generates this error message.\nTo get the priors used by brms we use the get_prior() command.\n\n\nCode\nget_prior(\n  fluency ~ 1 + Occ2*FL + Occ2*MT + Occ3*FL + Occ3*MT + (1|student),\n  data = Subtitles\n)\n\n\n                    prior     class      coef   group resp dpar nlpar lb ub\n                   (flat)         b                                        \n                   (flat)         b        FL                              \n                   (flat)         b   FL:Occ3                              \n                   (flat)         b        MT                              \n                   (flat)         b   MT:Occ3                              \n                   (flat)         b      Occ2                              \n                   (flat)         b   Occ2:FL                              \n                   (flat)         b   Occ2:MT                              \n                   (flat)         b      Occ3                              \n student_t(3, 104.9, 6.1) Intercept                                        \n     student_t(3, 0, 6.1)        sd                                    0   \n     student_t(3, 0, 6.1)        sd           student                  0   \n     student_t(3, 0, 6.1)        sd Intercept student                  0   \n     student_t(3, 0, 6.1)     sigma                                    0   \n       source\n      default\n (vectorized)\n (vectorized)\n (vectorized)\n (vectorized)\n (vectorized)\n (vectorized)\n (vectorized)\n (vectorized)\n      default\n      default\n (vectorized)\n (vectorized)\n      default\n\n\nAs can be seen above, for all the beta’s (fixed effects) brms uses a flat prior. Actually, that is something that is better avoided. More appropriate would be to come up with our own priors. Let’s think about this. All the explanatory variables are dummy variables. So, they quantify differences between groups of observations (based on time or condition).\nAs we have no prior idea about the directions of the effects of condition nor of the effect of time, we could use a prior distribution centred around 0 (most probability assigned to no effect).\nNext, we have to think about setting the width of the prior. For instance, if we use a normal distribution to express our prior belief, we have to think about the sd for the normal distribution that captures our prior belief. In our case, the sd has to be high enough to assign some probability to very strong positive effects as well as to very strong negative effects. Here, it is important that we know our data well. I mean, we need to know the scale of our dependent variable (fluency). This variable has a standard deviation of 7.1. Now we can use Effect Sizes as a frame of reference to determine what a large positive and negative effect implies on the scale of the fluency variable. Remember, an effect size of 0.8 (or higher) indicates a strong effect (this is based on the Cohen’s d rules of thumb). So, on our scale of the fluency variable an effect of 5.7 (= 7.1 * 0.8) indicates a strong effect. If we use the value 5.7 as our sd for priors for the effects of our dummy variables, this would imply that we think that the 95% most probable parameter values for the effect of the dummy variables would be situated between -11.4 (= -2 * 5.7) and 11.4 (= 2 * 5.7). Visually the prior would look like this:\n\n\nCode\n# Setting a plotting theme\n\nlibrary(ggplot2)\nlibrary(ggtext) # to be able to change the fonts etc in graphs\n\ntheme_set(theme_linedraw() +\n            theme(text = element_text(family = \"Times\", size = 8),\n                  panel.grid = element_blank(),\n                  plot.title = element_markdown())\n)\n\nPrior_betas &lt;- ggplot( ) +\n  stat_function(\n    fun = dnorm,    # We use the normal distribution\n    args = list(mean = 0, sd = 5.7), # \n    xlim = c(-15,15)\n  ) +\n  scale_y_continuous(name = \"density\") +\n  labs(title = \"Prior for the effects of independent variables\",\n       subtitle = \"N(0,5.7)\")\n\nPrior_betas\n\n\n\n\n\n\n\n\n\nNotice that even effects of -11 and 11 (almost effect sizes of -2 and 2) still get a decent amount of probability in our prior density function.\nLet’s set these priors and try to apply a pp_check(). Notice that I set the priors for all slopes (class = \"b\") at once.\n\n\nCode\nCustom_prior &lt;- c(\n  set_prior(\n    \"normal(0,5.7)\",\n    class = \"b\"\n  )\n)\n\nM3_priors &lt;- brm(\n  fluency ~ 1 + Occ2*FL + Occ2*MT + Occ3*FL + Occ3*MT + (1|student),\n  data = Subtitles,\n  cores = 4,\n  backend = \"cmdstanr\",\n  seed = 1975,\n  prior = Custom_prior,\n  sample_prior = \"only\"\n)\n\npp_check(M3_priors)\n\n\n\n\n\n\n\n\n\n\n\nThe simulated data goes all the way (light blue lines)! But it doesn’t generate extremely high or low observations and from this check we also learn that we have set quiet broad priors as they result in big differences between distributions of fluency based on the simulated datasets coming from our model with these priors.\nTime to apply these priors (that we somehow understand now) to estimate the real model.\n\n\nCode\nCustom_prior &lt;- c(\n  set_prior(\n    \"normal(0,5.7)\",\n    class = \"b\"\n  )\n)\n\nM3 &lt;- brm(\n  fluency ~ 1 + Occ2*FL + Occ2*MT + Occ3*FL + Occ3*MT + (1|student),\n  data = Subtitles,\n  cores = 4,\n  backend = \"cmdstanr\",\n  seed = 1975,\n  prior = Custom_prior\n  )"
  },
  {
    "objectID": "Integrated_exercise/Exercise_response.html#subtask-2.2-did-the-model-converge-properly",
    "href": "Integrated_exercise/Exercise_response.html#subtask-2.2-did-the-model-converge-properly",
    "title": "Integrated excercise",
    "section": "Subtask 2.2: did the model converge properly?",
    "text": "Subtask 2.2: did the model converge properly?\nPerform different checks on the convergence of the model.\n\n Possible solution \nLet’s start by checking the trace-plots.\n\n\nCode\nlibrary(ggmcmc)\n\nModel_chains &lt;- ggs(M3b)\n\nModel_chains %&gt;%\n  filter(Parameter %in% c(\n    \"b_Intercept\",\n    \"b_FL\", \n    \"b_MT\", \n    \"b_Occ2\", \n    \"b_Occ3\",\n    \"b_FL:Occ2\",\n    \"b_FL:Occ3\",\n    \"b_MT:Occ2\",\n    \"b_MT:Occ3\"\n    )\n  ) %&gt;%\n  ggplot(aes(\n    x   = Iteration,\n    y   = value, \n    col = as.factor(Chain)))+\n  geom_line() +\n  facet_grid(Parameter ~ .,\n             scale  = 'free_y',\n             switch = 'y') +\n  labs(title = \"Caterpillar Plots for the parameters\",\n       col   = \"Chains\")\n\n\n\n\n\n\n\n\n\nLooking at the trace-plots, we can conclude that all chains mixed very well. This is already a first indication of successful convergence.\nNext, we can check the R-hat statistic for each of the parameters. With the following plot we get a visual overview of all the R-hat statistics (notice the large number of parameters, because we also have random effects in our model):\n\n\n\n\n\n\nNote\n\n\n\nIn the code below you can see that I first create a vector called Rhats to use in the mcmc_rhat() function. To create the vector I call the brms::rhat() function. By writing explicitly brms:: before the rhat() I make sure that the rhat() function from the package brms is used. I do this to avoid the use of another rhat() function that might be loaded by activating other packages and that results in incompatible results with the mcmc_rhat() function.\n\n\n\n\nCode\nlibrary(brms)\nlibrary(bayesplot)\n\n# Extract posterior draws as array (robust method)\nRhats &lt;- brms::rhat(M3b)\n\n# Plot with bayesplot\nmcmc_rhat(Rhats, size = 2) +\n  yaxis_text(hjust = 1)\n\n\n\n\n\n\n\n\n\nNone of the parameters shows a high R-hat statistic. They all are below the threshold of 1.05, indicating that all parameters converged well.\nTime to get insight in the amount of autocorrelation. A first check is plotting the ratio of the number of Effective Sample Sizes to the Total Sampel Sizes for all the parameters. Remember that this ratio should be above 0.1 to be sure that the amount of autocorrelation is acceptable. Following code gives a visual overview of these ratios.\n\n\nCode\nmcmc_neff(\n  neff_ratio(M3b)\n  ) + \n  yaxis_text(hjust = 1)\n\n\n\n\n\n\n\n\n\nFrom the plot we learn that for all the parameters the ratio is above 0.1. So, we can conclude that the amount of autocorrelation is not problematic for the estimation of any of the parameters."
  },
  {
    "objectID": "Integrated_exercise/Exercise_response.html#subtask-2.3-does-the-posterior-distribution-histogram-have-enough-information",
    "href": "Integrated_exercise/Exercise_response.html#subtask-2.3-does-the-posterior-distribution-histogram-have-enough-information",
    "title": "Integrated excercise",
    "section": "Subtask 2.3: does the posterior distribution histogram have enough information?",
    "text": "Subtask 2.3: does the posterior distribution histogram have enough information?\nCheck if the posterior distribution histograms of the different parameters are informative enough to substantiate our inferences.\n\n Possible solution \nTo evaluate this, we create histograms based on the draws for our parameter values based on our model. We will apply this first for all main fixed effects.\n\n\nCode\nlibrary(patchwork)\n\nposterior_PD &lt;- as_draws_df(M3b)\n\npost_intercept &lt;- \n  posterior_PD %&gt;%\n  select(b_Intercept) %&gt;%\n  ggplot(aes(x = b_Intercept)) +\n  geom_histogram() +\n  ggtitle(\"Intercept\") \n\npost_Occ2 &lt;- \n  posterior_PD %&gt;%\n  select(b_Occ2) %&gt;%\n  ggplot(aes(x = b_Occ2)) +\n  geom_histogram() +\n  ggtitle(\"Beta Occ2\") \n\npost_Occ3 &lt;- \n  posterior_PD %&gt;%\n  select(b_Occ3) %&gt;%\n  ggplot(aes(x = b_Occ3)) +\n  geom_histogram() +\n  ggtitle(\"Beta Occ3\") \n\npost_FL &lt;- \n  posterior_PD %&gt;%\n  select(b_FL) %&gt;%\n  ggplot(aes(x = b_FL)) +\n  geom_histogram() +\n  ggtitle(\"Beta FL\") \n\npost_MT &lt;- \n  posterior_PD %&gt;%\n  select(b_MT) %&gt;%\n  ggplot(aes(x = b_MT)) +\n  geom_histogram() +\n  ggtitle(\"Beta MT\") \n\n\npost_intercept + post_Occ2 + post_Occ3 + post_FL + post_MT +\n  plot_layout(ncol = 3)\n\n\n\n\n\n\n\n\n\nThese plots show clear slopes and a peak, indicating that the posterior is informative enough for each of these parameters.\nNow, let’s do the same for the interaction effects.\n\n\nCode\npost_Occ2_FL &lt;- \n  posterior_PD %&gt;%\n  select(`b_Occ2:FL`) %&gt;%\n  ggplot(aes(x = `b_Occ2:FL`)) +\n  geom_histogram() +\n  ggtitle(\"Beta Occ2:FL\") \n\npost_Occ2_MT &lt;- \n  posterior_PD %&gt;%\n  select(`b_Occ2:MT`) %&gt;%\n  ggplot(aes(x = `b_Occ2:MT`)) +\n  geom_histogram() +\n  ggtitle(\"Beta Occ2:MT\") \n\npost_Occ3_FL &lt;- \n  posterior_PD %&gt;%\n  select(`b_FL:Occ3`) %&gt;%\n  ggplot(aes(x = `b_FL:Occ3`)) +\n  geom_histogram() +\n  ggtitle(\"Beta Occ3:FL\") \n\npost_Occ3_MT &lt;- \n  posterior_PD %&gt;%\n  select(`b_MT:Occ3`) %&gt;%\n  ggplot(aes(x = `b_MT:Occ3`)) +\n  geom_histogram() +\n  ggtitle(\"Beta Occ3:MT\") \n\npost_Occ2_FL + post_Occ2_MT + post_Occ3_FL + post_Occ3_MT +\n  plot_layout(ncol = 3)\n\n\n\n\n\n\n\n\n\nHere the conclusion is the same. These histograms show no problematic cases."
  },
  {
    "objectID": "Integrated_exercise/Exercise_response.html#subtask-2.4-how-well-does-the-model-predict-the-observed-data",
    "href": "Integrated_exercise/Exercise_response.html#subtask-2.4-how-well-does-the-model-predict-the-observed-data",
    "title": "Integrated excercise",
    "section": "Subtask 2.4: how well does the model predict the observed data?",
    "text": "Subtask 2.4: how well does the model predict the observed data?\nPerform posterior predictive checks based on the model.\n\n Possible solutions \n\n\nCode\npp_check(M3b)\n\n\n\n\n\n\n\n\n\nLooking at the posterior probability checks, we can see that the distributions of simulated data show a strong resemblance to the distribution of the observed data. So, the conclusion could be that our model is performing quiet well."
  },
  {
    "objectID": "Integrated_exercise/Exercise_response.html#subtask-2.5-what-about-prior-sensitivity-of-the-results",
    "href": "Integrated_exercise/Exercise_response.html#subtask-2.5-what-about-prior-sensitivity-of-the-results",
    "title": "Integrated excercise",
    "section": "Subtask 2.5: what about prior sensitivity of the results?",
    "text": "Subtask 2.5: what about prior sensitivity of the results?\nFinally, we have to check if the results of our model are not too dependent on the priors we specified in the model.\n\n\nCode\nlibrary(priorsense)\n\npowerscale_sensitivity(M3b)\n\n\nSensitivity based on cjs_dist:\n# A tibble: 47 × 4\n   variable               prior likelihood diagnosis          \n   &lt;chr&gt;                  &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;              \n 1 b_Intercept           0.0880      0.150 prior-data conflict\n 2 b_Occ2                0.101       0.242 prior-data conflict\n 3 b_FL                  0.110       0.166 prior-data conflict\n 4 b_MT                  0.0808      0.138 prior-data conflict\n 5 b_Occ3                0.127       0.276 prior-data conflict\n 6 b_Occ2:FL             0.129       0.285 prior-data conflict\n 7 b_Occ2:MT             0.108       0.271 prior-data conflict\n 8 b_FL:Occ3             0.153       0.372 prior-data conflict\n 9 b_MT:Occ3             0.116       0.272 prior-data conflict\n10 sd_student__Intercept 0.0192      0.719 -                  \n# ℹ 37 more rows\n\n\nWe learn from this prior sensitivity analyses that there is a prior-data conflict.\n\n\n\n\n\n\nNote\n\n\n\nIf we delve into the paper on the priorsense package we can read this paragraph:\n\nPrior-data conflict (Evans & Moshonov, 2006; Nott, Wang, et al., 2020; Walter & Augustin, 2009) can arise due to intentionally or unintentionally informative priors disagreeing with, but not being dominated by, the likelihood. When this is the case, the posterior will be sensitive to power-scaling both the prior and the likelihood, as illustrated in Figure 5. When prior-data conflict has been detected, the modeller may wish to modify the model by using a less informative prior (e.g., Evans & Jang, 2011; Nott, Seah, et al., 2020) or using heavy-tailed distributions (e.g., Gagnon, 2022; O’Hagan & Pericchi, 2012).\n\nReference: Kallioinen, N., Paananen, T., Bürkner, P.-C., & Vehtari, A. (2023). Detecting and diagnosing prior and likelihood sensitivity with power-scaling. Statistics and Computing, 34(1), 57. https://doi.org/10.1007/s11222-023-10366-5\n\n\nSo, it might imply that our priors are “unintentionally informative”. Let’s see what happens if we would stick to the default priors of brms. Given that the object called M3 contains the estimation of the model using the default priors we can quickly learn about the prior sensitivity of this model.\n\n\nCode\npowerscale_sensitivity(M3)\n\n\nSensitivity based on cjs_dist:\n# A tibble: 47 × 4\n   variable                 prior likelihood diagnosis\n   &lt;chr&gt;                    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;    \n 1 b_Intercept           0.00283      0.0603 -        \n 2 b_Occ2                0.000612     0.128  -        \n 3 b_FL                  0.00104      0.0596 -        \n 4 b_MT                  0.00118      0.0606 -        \n 5 b_Occ3                0.000393     0.124  -        \n 6 b_Occ2:FL             0.000629     0.147  -        \n 7 b_Occ2:MT             0.000578     0.142  -        \n 8 b_FL:Occ3             0.000427     0.137  -        \n 9 b_MT:Occ3             0.000400     0.127  -        \n10 sd_student__Intercept 0.00617      0.502  -        \n# ℹ 37 more rows\n\n\nLooking at this prior sensitivity check we learn that using the default priors makes the model less sensitive to the priors used!\n“What to do now?”\nWell, in my opinion there is not a single best way to deal with this situation. What I would do first is compare the results based on both models (M3 and M3b). Let’s print the parameter estimates of both models. To do this I rely on the tab_model() function from the very helpful package sjPlot (so, do not forget to install that one if you want to use it!), as it allows me to show the results of both models side-to-side.\n\n\n\n\n \nM3 (brms default priors)\nM3b (custom priors)\n\n\nPredictors\nEstimates\nCI (95%)\nEstimates\nCI (95%)\n\n\nIntercept\n102.60\n100.31 – 104.92\n101.75\n99.68 – 103.80\n\n\nOcc2\n-0.29\n-2.83 – 2.16\n0.79\n-1.58 – 3.12\n\n\nFL\n-2.39\n-5.77 – 0.73\n-0.99\n-3.96 – 1.94\n\n\nMT\n-1.40\n-4.61 – 1.85\n-0.22\n-3.14 – 2.78\n\n\nOcc3\n2.50\n-0.00 – 4.94\n3.78\n1.42 – 6.17\n\n\nOcc2:FL\n7.08\n3.47 – 10.59\n5.30\n1.87 – 8.66\n\n\nOcc2:MT\n8.07\n4.57 – 11.51\n6.44\n3.12 – 9.57\n\n\nFL:Occ3\n17.15\n13.53 – 20.66\n14.87\n11.46 – 18.20\n\n\nMT:Occ3\n8.84\n5.32 – 12.49\n7.09\n3.68 – 10.26\n\n\nRandom Effects\n\n\n\nσ2\n5.35\n5.13\n\n\n\nτ00\n46.66\n44.83\n\n\nICC\n0.10\n0.10\n\n\nN\n36 student\n36 student\n\nObservations\n108\n108\n\n\nMarginal R2 / Conditional R2\n0.709 / 0.816\n0.695 / 0.803\n\n\n\n\n\n\nThis table clearly demonstrates how the model making use of the default priors has different results than the model with the priors we had set ourselves!\nIf we think about this more profoundly this doesn’t have to come as a surprise! The priors used for the effect of Occasion was maybe nonsense. Why not assuming that students got more fluent over the time course of 26 weeks? To be fairly honest, I deliberately used these priors that seemed uninformative so that we would - for didactical reasons - run into this issue!\nFor the rest of the exercise we will stick to the results of the model with default priors. But, another option could be to change our custom priors, mimicking the idea that we expect students to become more fluent on the course of 26 weeks. That’s what I actually did in a blogpost that I wrote about using the priorsense package that can be found here: https://sdemaeyer.quarto.pub/posts/2024-02-PriorSense/PriorSensePost.html"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html",
    "href": "Presentations/Part3/Slides_Part3.html",
    "title": "Applied Bayesian Analyses in R",
    "section": "",
    "text": "here() starts at /Users/svendemaeyer/Library/CloudStorage/OneDrive-UniversiteitAntwerpen/Praatjes/Workshops/Zurich_2026/ABAR_Zurich26_Web\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nLoading required package: Rcpp\n\nLoading 'brms' package (version 2.22.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n\n\nAttaching package: 'brms'\n\n\nThe following object is masked from 'package:stats':\n\n    ar\n\n\nThis is bayesplot version 1.12.0\n\n- Online documentation and vignettes at mc-stan.org/bayesplot\n\n- bayesplot theme set to bayesplot::theme_default()\n\n   * Does _not_ affect other ggplot2 plots\n\n   * See ?bayesplot_theme_set for details on theme setting\n\n\nAttaching package: 'bayesplot'\n\n\nThe following object is masked from 'package:brms':\n\n    rhat\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#new-example-data-writingdata.rdata",
    "href": "Presentations/Part3/Slides_Part3.html#new-example-data-writingdata.rdata",
    "title": "Applied Bayesian Analyses in R",
    "section": "New example data WritingData.RData",
    "text": "New example data WritingData.RData\n\nExperimental study on Writing instructions\n2 conditions:\n\nControl condition (Business as usual)\nExperimental condition (Observational learning)"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#your-turn",
    "href": "Presentations/Part3/Slides_Part3.html#your-turn",
    "title": "Applied Bayesian Analyses in R",
    "section": " Your Turn",
    "text": "Your Turn\n\nOpen WritingData.RData\nEstimate 3 models with SecondVersion as dependent variable\n\nM1: fixed effect of FirstVersion_GM + random effect of Class ((1|Class))\nM2: M1 + random effect of FirstVersion_GM ((1 + FirstVersion_GM |Class))\nM3: M2 + fixed effect of Experimental_condition\n\nCompare the models on their fit\nWhat do we learn?\nMake a summary of the best fitting model\n\n\n\nNote: FirstVersion_GMis the score of the pretest centred around the mean, so a score 0 for this variable implies scoring on average for the pretest"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#divergent-transitions",
    "href": "Presentations/Part3/Slides_Part3.html#divergent-transitions",
    "title": "Applied Bayesian Analyses in R",
    "section": "Divergent transitions…",
    "text": "Divergent transitions…\n\n\n\nExploring the parameter space…"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#divergent-transitions-1",
    "href": "Presentations/Part3/Slides_Part3.html#divergent-transitions-1",
    "title": "Applied Bayesian Analyses in R",
    "section": "Divergent transitions…",
    "text": "Divergent transitions…\n\nSomething to worry about!\nEssentially: sampling of parameter estimate values went wrong\nFixes:\n\nsometimes fine-tuning the sampling algorithm (e.g., control = list(adapt_delta = 0.9)) works\nsometimes you need more informative priors\nsometimes the model is just not a good model"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#our-fix-here-for-model-3",
    "href": "Presentations/Part3/Slides_Part3.html#our-fix-here-for-model-3",
    "title": "Applied Bayesian Analyses in R",
    "section": "Our fix here for model 3",
    "text": "Our fix here for model 3\n\nM3 &lt;- brm(\n  SecondVersion ~ FirstVersion_GM + Experimental_condition + (1 + FirstVersion_GM |Class),\n  data = WritingData,\n  backend = \"cmdstanr\",\n  cores = 4,\n  control = list(adapt_delta = 0.9),\n  seed = 1975 \n)"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#our-fix-here-for-model-3-1",
    "href": "Presentations/Part3/Slides_Part3.html#our-fix-here-for-model-3-1",
    "title": "Applied Bayesian Analyses in R",
    "section": "Our fix here for model 3",
    "text": "Our fix here for model 3"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#interpretation-of-results",
    "href": "Presentations/Part3/Slides_Part3.html#interpretation-of-results",
    "title": "Applied Bayesian Analyses in R",
    "section": "Interpretation of results…",
    "text": "Interpretation of results…\nDifferent ways to summarize our results:\n\nvisually\ncredible intervals (eti & hdi)\nrope + hdi rule\nhypothesis tests"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#functions-in-bayesplot-package",
    "href": "Presentations/Part3/Slides_Part3.html#functions-in-bayesplot-package",
    "title": "Applied Bayesian Analyses in R",
    "section": "Functions in bayesplot package",
    "text": "Functions in bayesplot package\n\nmcmc_areas() function\nmcmc_areas_ridges() function\nmcmc_intervals() function"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#the-mcmc_areas-function",
    "href": "Presentations/Part3/Slides_Part3.html#the-mcmc_areas-function",
    "title": "Applied Bayesian Analyses in R",
    "section": "The mcmc_areas() function",
    "text": "The mcmc_areas() function\nGives a posterior distribution including a certain credible interval that you can set manually with the prob argument:\n\nmcmc_areas(\n  M3,\n  pars = c(\n    \"b_FirstVersion_GM\",\n    \"b_Experimental_condition\"\n  ),\n  prob = .89\n)\n\nLoading required package: rstan\n\n\nLoading required package: StanHeaders\n\n\nError: package or namespace load failed for 'rstan' in .doLoadActions(where, attach):\n error in load action .__A__.1 for package rstan: Rcpp::loadModule(module = \"class_model_base\", what = TRUE, env = ns, : Unable to load module \"class_model_base\": attempt to apply non-function"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#the-mcmc_areas_ridges-function",
    "href": "Presentations/Part3/Slides_Part3.html#the-mcmc_areas_ridges-function",
    "title": "Applied Bayesian Analyses in R",
    "section": "The mcmc_areas_ridges() function",
    "text": "The mcmc_areas_ridges() function\n\nAlmost similar to the previous, only the horizontal spacing changes a bit…\n\nMeanwhile, see how you can easily change the color scheme for bayesplot graphs\n\n\ncolor_scheme_set(scheme = \"red\")\n\nmcmc_areas_ridges(\n  M3,\n  pars = c(\n    \"b_FirstVersion_GM\",\n    \"b_Experimental_condition\"\n  ),\n  prob = .89\n)"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#the-mcmc_intervals-function",
    "href": "Presentations/Part3/Slides_Part3.html#the-mcmc_intervals-function",
    "title": "Applied Bayesian Analyses in R",
    "section": "The mcmc_intervals() function",
    "text": "The mcmc_intervals() function\nSummarizes the posterior as a horizontal bar with identifiers for two CI.\nHere we set one for a 50% and one for a 89% CI\n\ncolor_scheme_set(scheme = \"gray\")\n\nmcmc_intervals(\n  M3,\n  pars = c(\n    \"b_FirstVersion_GM\",\n    \"b_Experimental_condition\"\n  ),\n  prob = .5,\n  prob_outer = .89\n)"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#manually-create-visualizations",
    "href": "Presentations/Part3/Slides_Part3.html#manually-create-visualizations",
    "title": "Applied Bayesian Analyses in R",
    "section": "Manually create visualizations",
    "text": "Manually create visualizations\n\nPowercombo: as_draws_df() + ggplot2 + ggdist\n\nWhat does as_draws_df() do?\n\n\nposterior_PD &lt;- as_draws_df(M3)\nhead(posterior_PD)\n\n# A draws_df: 6 iterations, 1 chains, and 49 variables\n  b_Intercept b_FirstVersion_GM b_Experimental_condition sd_Class__Intercept\n1         113              0.98                      4.7                 5.2\n2         112              1.47                      5.1                 2.8\n3         113              1.07                      3.5                 2.8\n4         111              1.09                      4.7                 4.2\n5         112              1.08                      4.0                 1.7\n6         111              1.04                      5.0                 2.4\n  sd_Class__FirstVersion_GM cor_Class__Intercept__FirstVersion_GM sigma\n1                      0.93                                  0.77   7.4\n2                      1.06                                  0.68   8.1\n3                      0.88                                  0.79   7.1\n4                      0.75                                  0.74   8.0\n5                      0.94                                  0.78   7.1\n6                      0.97                                  0.82   8.1\n  r_Class[1,Intercept]\n1                 1.84\n2                -1.59\n3                -0.60\n4                 0.95\n5                 0.81\n6                 0.38\n# ... with 41 more variables\n# ... hidden reserved variables {'.chain', '.iteration', '.draw'}"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#use-draws-to-create-a-plot-using-ggdist-geoms",
    "href": "Presentations/Part3/Slides_Part3.html#use-draws-to-create-a-plot-using-ggdist-geoms",
    "title": "Applied Bayesian Analyses in R",
    "section": "Use draws to create a plot using ggdist geoms",
    "text": "Use draws to create a plot using ggdist geoms\n\n\n\n\n\n\n\n\n\n\n\n\nggdist package has a set of functions to visualize a distribution"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#an-example",
    "href": "Presentations/Part3/Slides_Part3.html#an-example",
    "title": "Applied Bayesian Analyses in R",
    "section": "An example",
    "text": "An example\n\nBefore we start, set our own plot theme (not so necessary)\n\n\n# Setting a plotting theme\ntheme_set(theme_linedraw() +\n            theme(\n              text = element_text(family = \"Times\", size = 14),\n              panel.grid = element_blank()\n              )\n)"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#an-example-1",
    "href": "Presentations/Part3/Slides_Part3.html#an-example-1",
    "title": "Applied Bayesian Analyses in R",
    "section": "An example",
    "text": "An example\n\nWe use posterior_PD as a starting point (our draws)\n\nlibrary(ggdist)\n\nPlot &lt;- ggplot(\n  posterior_PD,\n  aes(x = b_Experimental_condition)\n  ) +\n  stat_halfeye()\n\nPlot + scale_y_continuous(name = \"\", breaks = NULL)"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#change-the-cis",
    "href": "Presentations/Part3/Slides_Part3.html#change-the-cis",
    "title": "Applied Bayesian Analyses in R",
    "section": "Change the CI’s",
    "text": "Change the CI’s\n\nChange the CI’s to 50% and 89%\n\n\nPlot &lt;- ggplot(\n  posterior_PD,\n  aes(x = b_Experimental_condition)\n  ) +\n  stat_halfeye(\n    .width = c(.50,.89)\n  )\n\nPlot + scale_y_continuous(name = \"\", breaks = NULL)"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#use-another-visualization",
    "href": "Presentations/Part3/Slides_Part3.html#use-another-visualization",
    "title": "Applied Bayesian Analyses in R",
    "section": "Use another visualization",
    "text": "Use another visualization\n\nLet’s make a dotplot… (research shows this is best interpreted) with 100 dots\n\n\nPlot &lt;- ggplot(\n  posterior_PD,\n  aes(x = b_Experimental_condition)\n  ) +\n  stat_dotsinterval(\n    quantiles = 100,\n    .width = c(.50,.89)\n  )\n\nPlot + scale_y_continuous(name = \"\", breaks = NULL)"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#plot-two-parameters-each-in-a-facet",
    "href": "Presentations/Part3/Slides_Part3.html#plot-two-parameters-each-in-a-facet",
    "title": "Applied Bayesian Analyses in R",
    "section": "Plot two parameters each in a facet",
    "text": "Plot two parameters each in a facet\n\nWe use pivot_longer(everything()) to stack information on multiple parameters\n\n\nposterior_PD %&gt;% \n  select(\n    b_Experimental_condition, b_FirstVersion_GM\n  ) %&gt;% \n  pivot_longer(everything()) %&gt;%\n  ggplot(\n    aes(x = value)\n  ) +\n  stat_halfeye(\n    .width = c(.50,.89)\n  ) +\nfacet_wrap(name ~ .) +\nscale_y_continuous(name = \"\", breaks = NULL)"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#visualize-calculated-predictions-based-on-posterior",
    "href": "Presentations/Part3/Slides_Part3.html#visualize-calculated-predictions-based-on-posterior",
    "title": "Applied Bayesian Analyses in R",
    "section": "Visualize calculated predictions based on posterior",
    "text": "Visualize calculated predictions based on posterior\nOur example: 2 groups according to Experimental_condition\nHow to visualize the posterior probability of averages for both groups?\n\nposterior_PD %&gt;% \n  select(\n    b_Intercept, b_Experimental_condition\n  ) %&gt;% \n  mutate(\n    Mean_Control_condition = b_Intercept,\n    Mean_Experimental_condition = b_Intercept + b_Experimental_condition\n  ) %&gt;% \n  select(\n    Mean_Control_condition, Mean_Experimental_condition\n  ) %&gt;% \n  pivot_longer(everything()) %&gt;%\n  ggplot(\n    aes(x = value, color = name, fill = name)\n  ) +\n  stat_halfeye(\n    .width = c(.50,.89),\n    alpha = .40\n  ) + \n  scale_y_continuous(name = \"\", breaks = NULL)"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#hypothetical-outcome-plots-hops",
    "href": "Presentations/Part3/Slides_Part3.html#hypothetical-outcome-plots-hops",
    "title": "Applied Bayesian Analyses in R",
    "section": "Hypothetical Outcome Plots (HOPs)",
    "text": "Hypothetical Outcome Plots (HOPs)\nCode: see separate script called HOP_script.R\n\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\n# A tibble: 6 × 3\n   draw     X Pred1\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1 -15    95.8\n2     1 -14.9  95.9\n3     1 -14.8  96.0\n4     1 -14.7  96.1\n5     1 -14.6  96.2\n6     1 -14.5  96.3"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#plotting-the-residuals",
    "href": "Presentations/Part3/Slides_Part3.html#plotting-the-residuals",
    "title": "Applied Bayesian Analyses in R",
    "section": "Plotting the residuals",
    "text": "Plotting the residuals\nTo plot differences between classes we can use class-specific residuals:\n\nhead(as_draws_df(M3) %&gt;% \n  select(ends_with(\",Intercept]\")) %&gt;%\n  select(1:3),\n  5\n)\n\n# A tibble: 5 × 3\n  `r_Class[1,Intercept]` `r_Class[2,Intercept]` `r_Class[3,Intercept]`\n                   &lt;dbl&gt;                  &lt;dbl&gt;                  &lt;dbl&gt;\n1                  1.84                  1.36                  -4.38  \n2                 -1.59                  3.12                   0.0290\n3                 -0.595                 2.02                  -0.347 \n4                  0.945                 2.50                  -2.55  \n5                  0.815                -0.0404                -1.59"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#plotting-the-residuals-1",
    "href": "Presentations/Part3/Slides_Part3.html#plotting-the-residuals-1",
    "title": "Applied Bayesian Analyses in R",
    "section": "Plotting the residuals",
    "text": "Plotting the residuals\n\nas_draws_df(M3) %&gt;% \n  select(ends_with(\",Intercept]\")) %&gt;%\n  pivot_longer(starts_with(\"r_Class\")) %&gt;% \n  mutate(sigma_i = value) %&gt;%\n  ggplot(aes(x = sigma_i, y = reorder(name, sigma_i))) +\n  stat_pointinterval(\n    point_interval = mean_qi, \n    .width = .89, \n    size = 1/6) +\n  scale_y_discrete(expression(italic(j)), breaks = NULL) +\n  labs(x = expression(italic(u)[italic(j)])) +\n  coord_flip()"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#icc-estimation",
    "href": "Presentations/Part3/Slides_Part3.html#icc-estimation",
    "title": "Applied Bayesian Analyses in R",
    "section": "ICC estimation",
    "text": "ICC estimation\n\nhead(\n  as_draws_df(M3) %&gt;%\n    mutate(\n      ICC = (sd_Class__Intercept^2/(sd_Class__Intercept^2 + sigma^2))) %&gt;%\n    select(sigma, sd_Class__Intercept, ICC), \n  5\n  ) \n\n# A tibble: 5 × 3\n  sigma sd_Class__Intercept    ICC\n  &lt;dbl&gt;               &lt;dbl&gt;  &lt;dbl&gt;\n1  7.36                5.24 0.336 \n2  8.07                2.77 0.106 \n3  7.10                2.76 0.131 \n4  8.01                4.18 0.214 \n5  7.11                1.71 0.0545"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#icc-estimation-1",
    "href": "Presentations/Part3/Slides_Part3.html#icc-estimation-1",
    "title": "Applied Bayesian Analyses in R",
    "section": "ICC estimation",
    "text": "ICC estimation\n\nas_draws_df(M3) %&gt;%\n  mutate(\n    ICC = (sd_Class__Intercept^2/(sd_Class__Intercept^2 + sigma^2))\n    ) %&gt;%\n  select(ICC) %&gt;%                           \n  ggplot(aes(x = ICC)) +                    \n   stat_dotsinterval(\n     quantiles = 100,\n     .width = c(.50,.89)\n   ) +\n   scale_x_continuous(\"marginal posterior\", \n                      breaks = seq(.00,.60,by =.05)) + \n   scale_y_continuous(\"ICC\", breaks = NULL)"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#hop-per-higher-level-unit",
    "href": "Presentations/Part3/Slides_Part3.html#hop-per-higher-level-unit",
    "title": "Applied Bayesian Analyses in R",
    "section": "HOP per higher level unit",
    "text": "HOP per higher level unit\nCode: see separate script called HOP_MixedEffects_script.R\n\n\nThis is posterior version 1.6.1\n\n\n\nAttaching package: 'posterior'\n\n\nThe following object is masked from 'package:bayesplot':\n\n    rhat\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\n\nWarning: Dropping 'draws_df' class as required metadata was removed."
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#credible-intervals",
    "href": "Presentations/Part3/Slides_Part3.html#credible-intervals",
    "title": "Applied Bayesian Analyses in R",
    "section": "Credible Intervals",
    "text": "Credible Intervals\n\n\n\n\n\nAttaching package: 'bayestestR'\n\n\nThe following object is masked from 'package:ggdist':\n\n    hdi\n\n\nThe following object is masked from 'package:ggmcmc':\n\n    ci\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n\nETI: Equal Tailed Interval\n\n\n\nHDI: Highest Density Interval"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#concept-of-rope",
    "href": "Presentations/Part3/Slides_Part3.html#concept-of-rope",
    "title": "Applied Bayesian Analyses in R",
    "section": "Concept of ROPE",
    "text": "Concept of ROPE\n\n\n\n\n\n\n\n\n\n\n\n\nROPE: Region Of Practical Equivalence\n Set a region of parameter values that can be considered equivalent to having no effect \n\nin standard effect sizes the advised default is a range of -0.1 to 0.1\nthis equals 1/2 of a small effect size (Cohen, 1988)\nall parameter values in that range are set equal to no effect"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#rope-hdi",
    "href": "Presentations/Part3/Slides_Part3.html#rope-hdi",
    "title": "Applied Bayesian Analyses in R",
    "section": "ROPE + HDI",
    "text": "ROPE + HDI\n\n\n\n\n\n\n\n\n\n\n\n\nROPE + HDI rule\n\n\n95% of HDI out of ROPE \\(\\rightarrow\\) \\(H_0\\) rejected\n95% of HDI all in ROPE \\(\\rightarrow\\) \\(H_0\\) not rejected\n95% of HDI partially out of ROPE \\(\\rightarrow\\) undecided"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#applying-the-hdi-rope-rule-with-bayestestr-package",
    "href": "Presentations/Part3/Slides_Part3.html#applying-the-hdi-rope-rule-with-bayestestr-package",
    "title": "Applied Bayesian Analyses in R",
    "section": "Applying the HDI + ROPE rule with bayestestR package",
    "text": "Applying the HDI + ROPE rule with bayestestR package\n\nWe can use the equivalence_test() function of the bayestestR package\n\n\nlibrary(bayestestR)\nequivalence_test(M3)\n\n# Test for Practical Equivalence\n\n  ROPE: [-1.68 1.68]\n\nParameter              |       H0 | inside ROPE |          95% HDI\n------------------------------------------------------------------\nIntercept              | Rejected |      0.00 % | [109.18, 113.43]\nFirstVersion_GM        | Accepted |    100.00 % |     [0.48, 1.39]\nExperimental_condition | Rejected |      0.00 % |     [1.71, 7.47]"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#visualizing-the-hdi-rope-rule",
    "href": "Presentations/Part3/Slides_Part3.html#visualizing-the-hdi-rope-rule",
    "title": "Applied Bayesian Analyses in R",
    "section": "Visualizing the HDI + ROPE rule",
    "text": "Visualizing the HDI + ROPE rule\n\nWe visualize the equivalence_test() by adding plot( )\n\n\nequivalence_test(M3) %&gt;%\n  plot()\n\nPicking joint bandwidth of 0.121"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#probability-of-direction-pd-with-parameters-package",
    "href": "Presentations/Part3/Slides_Part3.html#probability-of-direction-pd-with-parameters-package",
    "title": "Applied Bayesian Analyses in R",
    "section": "Probability of Direction (PD) with parameters package",
    "text": "Probability of Direction (PD) with parameters package\n\nlibrary(parameters)\n\nRegistered S3 methods overwritten by 'parameters':\n  method                           from      \n  display.parameters_distribution  datawizard\n  plot.parameters_distribution     datawizard\n  print_md.parameters_distribution datawizard\n\n\n\nAttaching package: 'parameters'\n\n\nThe following object is masked from 'package:ggmcmc':\n\n    ci\n\nmodel_parameters(\n  M3,\n  ci_method = \"hdi\",\n  rope_range = c(-1.8,1.8), #sd MarathonTimeM = 17.76 so 17.76*0.1 \n  test = c(\"rope\", \"pd\")\n  )\n\n# Fixed Effects\n\nParameter              | Median |           95% CI |     pd | % in ROPE |  Rhat |     ESS\n-----------------------------------------------------------------------------------------\n(Intercept)            | 111.36 | [109.34, 113.58] |   100% |        0% | 1.000 | 1925.00\nFirstVersion_GM        |   0.94 | [  0.49,   1.40] | 99.98% |      100% | 1.001 | 1718.00\nExperimental_condition |   4.54 | [  1.77,   7.52] | 99.75% |     0.21% | 0.999 | 1814.00\n\n# Sigma\n\nParameter | Median |       95% CI |   pd | % in ROPE |  Rhat |     ESS\n----------------------------------------------------------------------\nsigma     |   7.68 | [7.19, 8.20] | 100% |        0% | 1.001 | 5235.00\n\n\n\nUncertainty intervals (highest-density) and p-values (two-tailed)\n  computed using a MCMC distribution approximation."
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#some-books",
    "href": "Presentations/Part3/Slides_Part3.html#some-books",
    "title": "Applied Bayesian Analyses in R",
    "section": "Some books ",
    "text": "Some books"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#some-books-1",
    "href": "Presentations/Part3/Slides_Part3.html#some-books-1",
    "title": "Applied Bayesian Analyses in R",
    "section": "Some books ",
    "text": "Some books"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#some-free-online-books",
    "href": "Presentations/Part3/Slides_Part3.html#some-free-online-books",
    "title": "Applied Bayesian Analyses in R",
    "section": "Some free online books ",
    "text": "Some free online books \n\nBayes Rules!:\n\nhttps://www.bayesrulesbook.com/\n\nOr this book:\n\nhttps://vasishth.github.io/bayescogsci/book/"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#rens-van-de-schoot",
    "href": "Presentations/Part3/Slides_Part3.html#rens-van-de-schoot",
    "title": "Applied Bayesian Analyses in R",
    "section": "Rens van de Schoot ",
    "text": "Rens van de Schoot \nIn Nature Reviews"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#the-podcast",
    "href": "Presentations/Part3/Slides_Part3.html#the-podcast",
    "title": "Applied Bayesian Analyses in R",
    "section": "THE Podcast ",
    "text": "THE Podcast \nIf you like running - like I do - this could be a great companion on your run!\nhttps://www.learnbayesstats.com/"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#site-on-creating-the-graphs",
    "href": "Presentations/Part3/Slides_Part3.html#site-on-creating-the-graphs",
    "title": "Applied Bayesian Analyses in R",
    "section": "Site on creating the graphs ",
    "text": "Site on creating the graphs \nThere are many blogs and websites that you can consult if you want to find out more about making graphs. \nOne that I often fall back to is:\n\nhttp://mjskay.github.io/tidybayes/"
  },
  {
    "objectID": "Presentations/Part3/Slides_Part3.html#questions",
    "href": "Presentations/Part3/Slides_Part3.html#questions",
    "title": "Applied Bayesian Analyses in R",
    "section": "Questions?",
    "text": "Questions?\n\nDo not hesitate to contact me!\n\nsven.demaeyer@uantwerpen.be"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html",
    "href": "Presentations/Part1/Slides_Part1.html",
    "title": "Applied Bayesian Analyses in R",
    "section": "",
    "text": "Let’s get to know each other first!\n\nwhat’s your name?\na 1/2-minute pitch on your research\nany experience with Bayesian inference?\nwhat do you hope to learn?\n\n\n\n\nPart 1: Introduction to Bayesian inferences and some basics in brms\nPart 2: Checking the model with the WAMBS and how to do it in R\nPart 3: Reporting and interpreting Bayesian models\nPart 4: Analysing your own data or integrated exercise\nThroughout the different presentations we will also increase model complexity\n\n\n\nAll material is on the on-line dashboard:\nhttps://abar-turku-2025.netlify.app/\nYou can find:\n\nan overview of the course\nhow to prepare\nfor each day the slides and datasets\nhtml slides can be exported to pdf in your browser (use the menu button bottom right)"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#welcome",
    "href": "Presentations/Part1/Slides_Part1.html#welcome",
    "title": "Applied Bayesian Analyses in R",
    "section": "Welcome",
    "text": "Welcome\nLet’s get to know each other first!\n\nwhat’s your name?\na 1/2-minute pitch on your research\nany experience with Bayesian inference?\nwhat do you hope to learn?"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#outline",
    "href": "Presentations/Part1/Slides_Part1.html#outline",
    "title": "Applied Bayesian Analyses in R",
    "section": "Outline",
    "text": "Outline\nPart 1: Introduction to Bayesian inferences and some basics in brms\nPart 2: Checking the model with the WAMBS and how to do it in R\nPart 3: Reporting and interpreting Bayesian models\nPart 4: Analysing your own data or integrated exercise\nThroughout the different presentations we will also increase model complexity"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#practical-stuff",
    "href": "Presentations/Part1/Slides_Part1.html#practical-stuff",
    "title": "Applied Bayesian Analyses in R",
    "section": "Practical stuff",
    "text": "Practical stuff\nAll material is on the on-line dashboard:\nhttps://abar-turku-2025.netlify.app/\nYou can find:\n\nan overview of the course\nhow to prepare\nfor each day the slides and datasets\nhtml slides can be exported to pdf in your browser (use the menu button bottom right)"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#why-statistical-inference",
    "href": "Presentations/Part1/Slides_Part1.html#why-statistical-inference",
    "title": "Applied Bayesian Analyses in R",
    "section": "Why statistical inference?",
    "text": "Why statistical inference?\n\nWe want to learn more about a population but we have sample data!\nTherefore, we have uncertainty"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#why-statistical-inference-an-example",
    "href": "Presentations/Part1/Slides_Part1.html#why-statistical-inference-an-example",
    "title": "Applied Bayesian Analyses in R",
    "section": "Why statistical inference? an example",
    "text": "Why statistical inference? an example\n\nWhat is the effect of average number of kilometers ran per week on the marathon time?\n\nWe can estimate the effect by using a regression model based on sample data:\n\\[\\text{MarathonTime}_i = \\beta_0 + \\beta_1 * \\text{n_km}_i + \\epsilon_i\\]\nOf course, we are not interested in the value of \\(\\beta_1\\) in the sample, but we want to learn more on \\(\\beta_1\\) in the population!"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#frequentistic-statistical-inference-with-data",
    "href": "Presentations/Part1/Slides_Part1.html#frequentistic-statistical-inference-with-data",
    "title": "Applied Bayesian Analyses in R",
    "section": "Frequentistic statistical inference (with data…)",
    "text": "Frequentistic statistical inference (with data…)\nMaximal Likelihood estimation:\n\n\n\nhere() starts at /Users/svendemaeyer/Library/CloudStorage/OneDrive-UniversiteitAntwerpen/Praatjes/Workshops/Zurich_2026/ABAR_Zurich26_Web\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nLoading required package: Rcpp\n\nLoading 'brms' package (version 2.22.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n\n\nAttaching package: 'brms'\n\n\nThe following object is masked from 'package:stats':\n\n    ar\n\n\nThis is bayesplot version 1.12.0\n\n- Online documentation and vignettes at mc-stan.org/bayesplot\n\n- bayesplot theme set to bayesplot::theme_default()\n\n   * Does _not_ affect other ggplot2 plots\n\n   * See ?bayesplot_theme_set for details on theme setting\n\n\nAttaching package: 'bayesplot'\n\n\nThe following object is masked from 'package:brms':\n\n    rhat\n\n\nRows: 87 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): Marathon, Name, Category, CrossTraining, Wall21, CATEGORY\ndbl (4): id, km4week, sp4week, MarathonTime\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nCall:\nlm(formula = MarathonTimeM ~ km4week, data = MarathonData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-42.601 -12.469  -0.536  12.816  38.670 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 199.14483    1.93856 102.728  &lt; 2e-16 ***\nkm4week      -0.50907    0.07233  -7.038 4.68e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.08 on 85 degrees of freedom\nMultiple R-squared:  0.3682,    Adjusted R-squared:  0.3608 \nF-statistic: 49.53 on 1 and 85 DF,  p-value: 4.675e-10\n\n\n\n\n\nData comes from https://www.kaggle.com/datasets/girardi69/marathon-time-predictions?resource=download"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#frequentistic-statistical-inference-interpretation",
    "href": "Presentations/Part1/Slides_Part1.html#frequentistic-statistical-inference-interpretation",
    "title": "Applied Bayesian Analyses in R",
    "section": "Frequentistic statistical inference (interpretation …)",
    "text": "Frequentistic statistical inference (interpretation …)\nSample: for each km more a week approx half a minute faster\nPopulation:\n\np-value = prob to observe a |0.509| estimate in our sample if effect is zero in the population is lower than 0.05\nCI = in 95% of possible samples the calculation of a CI would result in an interval containing the true population value\n\n==&gt; [-0.6462 ; -0.3718] so: -0.6 is as equally probable as -0.4 …"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#bayesian-statistical-inference",
    "href": "Presentations/Part1/Slides_Part1.html#bayesian-statistical-inference",
    "title": "Applied Bayesian Analyses in R",
    "section": "Bayesian statistical inference",
    "text": "Bayesian statistical inference\n\n\nLoading required package: rstan\n\n\nLoading required package: StanHeaders\n\n\nError: package or namespace load failed for 'rstan' in .doLoadActions(where, attach):\n error in load action .__A__.1 for package rstan: Rcpp::loadModule(module = \"class_model_base\", what = TRUE, env = ns, : Unable to load module \"class_model_base\": attempt to apply non-function"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#frequentist-compared-to-bayesian-inference",
    "href": "Presentations/Part1/Slides_Part1.html#frequentist-compared-to-bayesian-inference",
    "title": "Applied Bayesian Analyses in R",
    "section": "Frequentist compared to Bayesian inference",
    "text": "Frequentist compared to Bayesian inference\n  You can explore a tutorial App of J. Krushke (2019)   https://iupbsapps.shinyapps.io/KruschkeFreqAndBayesApp/\n  He has also written a nice tutorial, linked to this App:   https://jkkweb.sitehost.iu.edu/KruschkeFreqAndBayesAppTutorial.html"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#advantages-disadvantages-of-bayesian-analyses",
    "href": "Presentations/Part1/Slides_Part1.html#advantages-disadvantages-of-bayesian-analyses",
    "title": "Applied Bayesian Analyses in R",
    "section": "Advantages & Disadvantages of Bayesian analyses",
    "text": "Advantages & Disadvantages of Bayesian analyses\n\n\nAdvantages:\n\nNatural approach to express uncertainty\nAbility to incorporate prior knowledge\nIncreased model flexibility\nFull posterior distribution of the parameters\nNatural propagation of uncertainty\n\n\nDisadvantage:\n\nSlow speed of model estimation\nSome reviewers don’t understand you (“give me the p-value”)\n\n\n\n\nSlight adaptation of a slide from Paul Bürkner’s presentation available on YouTube: https://www.youtube.com/watch?v=FRs1iribZME"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#bayesian-theorem",
    "href": "Presentations/Part1/Slides_Part1.html#bayesian-theorem",
    "title": "Applied Bayesian Analyses in R",
    "section": "Bayesian Theorem",
    "text": "Bayesian Theorem\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\nP(\\theta|data) = \\frac{P(data|\\theta)P(\\theta)}{P(data)}\n\\] with\n\n\\(P(data|\\theta)\\) : the likelihood of the data given our model \\(\\theta\\)\n\\(P(\\theta)\\) : our prior belief about values for the model parameters\n\\(P(\\theta|data)\\): the posterior probability for model parameter values\n\n\n\n\nmeme from https://twitter.com/ChelseaParlett/status/1421291716229746689?s=20"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#likelihood",
    "href": "Presentations/Part1/Slides_Part1.html#likelihood",
    "title": "Applied Bayesian Analyses in R",
    "section": "Likelihood",
    "text": "Likelihood\n\\(P(data|\\theta)\\) : the likelihood of the data given our model \\(\\theta\\)\nThis part is about our MODEL and the parameters (aka the unknowns) in that model\n\nExample: a model on “time needed to run a marathon” could be a normal distribution: \\(y_i \\backsim N(\\mu, \\sigma)\\)\n\nSo: for a certain average (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)) we can calculate the probability of observing a marathon time of 240 minutes"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#prior",
    "href": "Presentations/Part1/Slides_Part1.html#prior",
    "title": "Applied Bayesian Analyses in R",
    "section": "Prior",
    "text": "Prior\nExpression of our prior knowledge (belief) on the parameter values\n\nExample: a model on “time needed to run a marathon” could be a normal distribution \\(y_i \\backsim N(\\mu, \\sigma)\\)\n\ne.g., How probable is an average marathon time of 120 minutes (\\(P(\\mu=120)\\)) and how probable is a standard deviation of 40 minutes (\\(P(\\sigma=40)\\))?"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#prior-as-a-distribution",
    "href": "Presentations/Part1/Slides_Part1.html#prior-as-a-distribution",
    "title": "Applied Bayesian Analyses in R",
    "section": "Prior as a distribution",
    "text": "Prior as a distribution\nHow probable are different values as average marathon time in a population?\n\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nℹ Please use `tibble()` instead."
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#types-of-priors",
    "href": "Presentations/Part1/Slides_Part1.html#types-of-priors",
    "title": "Applied Bayesian Analyses in R",
    "section": "Types of priors",
    "text": "Types of priors\n\n\nUninformative/Weakly informative\nWhen objectivity is crucial and you want let the data speak for itself…\n\nInformative\nWhen including significant information is crucial\n\npreviously collected data\nresults from former research/analyses\ndata of another source\ntheoretical considerations\nelicitation"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#type-of-priors-visually",
    "href": "Presentations/Part1/Slides_Part1.html#type-of-priors-visually",
    "title": "Applied Bayesian Analyses in R",
    "section": "Type of priors (visually)",
    "text": "Type of priors (visually)\n\nTwo potential priors for the mean marathon time in the population"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#bayesian-theorem-in-stats",
    "href": "Presentations/Part1/Slides_Part1.html#bayesian-theorem-in-stats",
    "title": "Applied Bayesian Analyses in R",
    "section": "Bayesian theorem in stats",
    "text": "Bayesian theorem in stats\n  “For Bayesians, the data are treated as fixed and the parameters vary. […] Bayes’ rule tells us that to calculate the posterior probability distribution we must combine a likelihood with a prior probability distribution over parameter values.”  (Lambert, 2018, p.88)"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#lets-apply-this-idea",
    "href": "Presentations/Part1/Slides_Part1.html#lets-apply-this-idea",
    "title": "Applied Bayesian Analyses in R",
    "section": "Let’s apply this idea",
    "text": "Let’s apply this idea\nOur Model for marathon times\n\\(y_i \\backsim N(\\mu, \\sigma)\\)"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#lets-apply-this-idea-1",
    "href": "Presentations/Part1/Slides_Part1.html#lets-apply-this-idea-1",
    "title": "Applied Bayesian Analyses in R",
    "section": "Let’s apply this idea",
    "text": "Let’s apply this idea\nOur Priors related to mu and sigma:\n\npar(mfrow=c(2,2))\ncurve( dnorm( x , 210 , 30 ) , from=120 , to=300 ,xlab=\"mu\", main=\"Prior for mu\")\ncurve( dunif( x , 1 , 40 ) , from=-5 , to=50 ,xlab=\"sigma\", main=\"Prior for sigma\")"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#lets-apply-this-idea-2",
    "href": "Presentations/Part1/Slides_Part1.html#lets-apply-this-idea-2",
    "title": "Applied Bayesian Analyses in R",
    "section": "Let’s apply this idea",
    "text": "Let’s apply this idea\nOur data:\n\n\n [1] 185 193 240 245 155 234 189 196 206 263"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#lets-apply-this-idea-3",
    "href": "Presentations/Part1/Slides_Part1.html#lets-apply-this-idea-3",
    "title": "Applied Bayesian Analyses in R",
    "section": "Let’s apply this idea",
    "text": "Let’s apply this idea\nWhat is the posterior probability of mu = 180 combined with a sigma = 15?\n\n\n\n# Calculate the likelihood of the data\n# given these parameter values of\n# mu = 180 ; sigma = 15\n# remember MT contains our data\nLikelihood &lt;- \n  sum(\n      dnorm(\n        MT ,\n        mean=180 ,\n        sd=15)\n      )\nLikelihood\n\n[1] 0.09330546\n\n\n\n\n# Calculate our Prior belief\n\nPrior &lt;- dnorm(180, 210 , 30 ) * dunif(15 , 1 , 40 )\n\nPrior\n\n[1] 0.0002068126\n\n\n\n\n# Calculate posterior as product \n# of Likelihood and Prior\n\nProduct &lt;- Likelihood * Prior\n\nProduct\n\n[1] 0.00001929674"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#lets-apply-this-idea-4",
    "href": "Presentations/Part1/Slides_Part1.html#lets-apply-this-idea-4",
    "title": "Applied Bayesian Analyses in R",
    "section": "Let’s apply this idea",
    "text": "Let’s apply this idea\nWhat is the posterior probability of mu = 210 combined with a sigma = 30?\n\n\n\n# Calculate the likelihood of the data\n# given these parameter values of\n# mu = 210 ; sigma = 30\n# remember MT contains our data\nLikelihood &lt;- \n  sum(\n      dnorm(\n        MT ,\n        mean=210 ,\n        sd=30)\n      )\nLikelihood\n\n[1] 0.08596287\n\n\n\n\n# Calculate our Prior belief\n\nPrior &lt;- dnorm(210, 210 , 30 ) * dunif(30 , 1 , 40 )\n\nPrior\n\n[1] 0.0003409763\n\n\n\n\n# Calculate posterior as product \n# of Likelihood and Prior\n\nProduct &lt;- Likelihood * Prior\n\nProduct\n\n[1] 0.0000293113"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#grid-approximation",
    "href": "Presentations/Part1/Slides_Part1.html#grid-approximation",
    "title": "Applied Bayesian Analyses in R",
    "section": "Grid approximation",
    "text": "Grid approximation\nWe calculated the posterior probability of a combination of 2 parameters\nWe could repeat this systematically for following values:\n\n# sample some values for mu and sigma\nmu.list &lt;- seq(from = 135, \n               to = 300, \n               length.out=400)\nsigma.list &lt;- seq(from = 1, \n                  to = 40, \n                  length.out = 400)\n\naka: we create a grid of possible parameter value combinations and approx the distribution of posterior probabilities"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#grid-approximation-applied",
    "href": "Presentations/Part1/Slides_Part1.html#grid-approximation-applied",
    "title": "Applied Bayesian Analyses in R",
    "section": "Grid approximation applied",
    "text": "Grid approximation applied\n\nC1: first combo of mu and sigma\n& C2: second combo of mu and sigma"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#sampling-parameter-values",
    "href": "Presentations/Part1/Slides_Part1.html#sampling-parameter-values",
    "title": "Applied Bayesian Analyses in R",
    "section": "Sampling parameter values",
    "text": "Sampling parameter values\n\nInstead of a fixed grid of combinations of parameter values,\nsample pairs/triplets/… of parameter values\nreconstruct the posterior probability distribution"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#imagine",
    "href": "Presentations/Part1/Slides_Part1.html#imagine",
    "title": "Applied Bayesian Analyses in R",
    "section": "Imagine",
    "text": "Imagine\nA ‘simple’ linear model\n\n\\[\\begin{aligned}\n  & MT_{i}  \\sim N(\\mu,\\sigma_{e_{i}})\\\\\n  & \\mu = \\beta_0 + \\beta_1*\\text{sp4week}_{i} + \\beta_2*\\text{km4week}_{i} + \\\\\n  & \\beta_3*\\text{Age}_{i} + \\beta_4*\\text{Gender}_{i} + \\beta_5*\\text{CrossTraining}_{i} \\\\\n\\end{aligned}\\]\n\nSo you can get a REALLY LARGE number of parameters!"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#markov-chain-monte-carlo---why",
    "href": "Presentations/Part1/Slides_Part1.html#markov-chain-monte-carlo---why",
    "title": "Applied Bayesian Analyses in R",
    "section": "Markov Chain Monte Carlo - Why?",
    "text": "Markov Chain Monte Carlo - Why?\nComplex models \\(\\rightarrow\\) Large number of parameters \\(\\rightarrow\\) exponentionally large number of combinations!\n\nPosterior gets unsolvable by grid approximation\n\nWe will approximate the ‘joint posterior’ by ‘smart’ sampling\n\nSamples of combinations of parameter values are drawn\n\nBUT: samples will not be random!"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#mcmc---demonstrated",
    "href": "Presentations/Part1/Slides_Part1.html#mcmc---demonstrated",
    "title": "Applied Bayesian Analyses in R",
    "section": "MCMC - demonstrated",
    "text": "MCMC - demonstrated\n  Following link brings you to an interactive tool that let’s you get the basic idea behind MCMC sampling:\n\nhttps://chi-feng.github.io/mcmc-demo/app.html#HamiltonianMC,standard"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#software",
    "href": "Presentations/Part1/Slides_Part1.html#software",
    "title": "Applied Bayesian Analyses in R",
    "section": "Software",
    "text": "Software\n\n\ndifferent dedicated software/packages are available: JAGS / BUGS / Stan\nmost powerful is Stan! Specifically the Hamiltonian Monte Carlo algorithm makes it the best choice at the moment\nStan is a probabilistic programming language that uses C++"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#example-of-stan-code",
    "href": "Presentations/Part1/Slides_Part1.html#example-of-stan-code",
    "title": "Applied Bayesian Analyses in R",
    "section": "Example of Stan code",
    "text": "Example of Stan code\n\n\n// generated with brms 2.14.4\nfunctions {\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  vector[N] Y;  // response variable\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n}\nparameters {\n  real Intercept;  // temporary intercept for centered predictors\n  real&lt;lower=0&gt; sigma;  // residual SD\n}\ntransformed parameters {\n}\nmodel {\n  // likelihood including all constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = Intercept + rep_vector(0.0, N);\n    target += normal_lpdf(Y | mu, sigma);\n  }\n  // priors including all constants\n  target += normal_lpdf(Intercept | 500,50);\n  target += student_t_lpdf(sigma | 3, 0, 68.8)\n    - 1 * student_t_lccdf(0 | 3, 0, 68.8);\n}\ngenerated quantities {\n  // actual population-level intercept\n  real b_Intercept = Intercept;\n}"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#how-brms-works",
    "href": "Presentations/Part1/Slides_Part1.html#how-brms-works",
    "title": "Applied Bayesian Analyses in R",
    "section": "How brms works",
    "text": "How brms works"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#brms-syntax",
    "href": "Presentations/Part1/Slides_Part1.html#brms-syntax",
    "title": "Applied Bayesian Analyses in R",
    "section": "brms syntax",
    "text": "brms syntax\nVery very similar to lm or lme4 and in line with typical R-style writing up of a model …\n\n\nlme4\n\nModel &lt;- lmer(\n  y ~ x1 + x2 + (1|Group),\n  data = Data,\n\n  ...\n)\n\n\nbrms\n\nModel &lt;- brm(\n  y ~ x1 + x2 + (1|Group),\n  data = Data,\n  family = \"gaussian\",\n  backend = \"cmdstanr\",\n  \n  ...\n)\n\n\nNotice:\n\nbackend = \"cmdstanr\" indicates the way we want to interact with Stan and C++"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#lets-retake-the-example-on-running",
    "href": "Presentations/Part1/Slides_Part1.html#lets-retake-the-example-on-running",
    "title": "Applied Bayesian Analyses in R",
    "section": "Let’s retake the example on running",
    "text": "Let’s retake the example on running\nThe simplest model looked like:\n\\[\nMT_i \\sim N(\\mu,\\sigma_e)\n\\]\nIn brms this model is:\n\n1MT &lt;- c(185, 193, 240, 245, 155, 234, 189, 196, 206, 263)\n\n# First make a dataset from our MT vector\n2DataMT &lt;- data_frame(MT)\n\n3Mod_MT1 &lt;- brm(\n4  MT ~ 1, # We only model an intercept\n5  data = DataMT,\n6  backend = \"cmdstanr\",\n7  seed = 1975\n)\n\n\n1\n\ncreate a dataset with potential marathon times\n\n2\n\nchange it to a data frame\n\n3\n\nthe brm() commando runs the model\n\n4\n\ndefine the model\n\n5\n\ndefine the dataset to us\n\n6\n\nhow brms and stan communicate\n\n7\n\nmake the estimation reproducible\n\n\n\n\n 🏃 Try it yourself and run the model … (don’t forget to load the necessary packages: brms & tidyverse)"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#basic-output-brms",
    "href": "Presentations/Part1/Slides_Part1.html#basic-output-brms",
    "title": "Applied Bayesian Analyses in R",
    "section": "Basic output brms",
    "text": "Basic output brms\n\nsummary(Mod_MT1)"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#about-chains-and-iterations",
    "href": "Presentations/Part1/Slides_Part1.html#about-chains-and-iterations",
    "title": "Applied Bayesian Analyses in R",
    "section": "About chains and iterations",
    "text": "About chains and iterations\nMarkov Chain Monte Carlo\n\nA chain of samples of parameter values\nIt is advised to run multiple chains\nEach sample of parameter values is called an iteration\nFirst X iterations are used to tune the sampler but not used to interpret the results: X burn-in iterations\n\nbrms defaults with 4 chains each of 2000 iterations of which 1000 iterations are used as burn-in"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#changing-brms-defaults",
    "href": "Presentations/Part1/Slides_Part1.html#changing-brms-defaults",
    "title": "Applied Bayesian Analyses in R",
    "section": "Changing brms defaults",
    "text": "Changing brms defaults\n\nMod_MT1 &lt;- brm(                        \n  MT ~ 1, \n  data = DataMT,   \n  backend = \"cmdstanr\",\n  chains = 5,\n  iter = 6000,\n  warmup = 1500,\n  cores = 4,\n  seed = 1975           \n)"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#good-old-plot-function",
    "href": "Presentations/Part1/Slides_Part1.html#good-old-plot-function",
    "title": "Applied Bayesian Analyses in R",
    "section": "Good old plot() function",
    "text": "Good old plot() function\n\nplot(Mod_MT1)"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#your-turn",
    "href": "Presentations/Part1/Slides_Part1.html#your-turn",
    "title": "Applied Bayesian Analyses in R",
    "section": " Your Turn",
    "text": "Your Turn\n\nOpen MarathonData.RData\nEstimate your first Bayesian Models\nDependent variable: MarathonTimeM\nModel1: only an intercept\nModel2: introduce the effect of km4week and sp4week on MarathonTimeM\nChange the brms defaults (4 chains of 6000 iterations)\nMake plots with the plot() function\nWhat do we learn? (rough interpretation)"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#results-exercise",
    "href": "Presentations/Part1/Slides_Part1.html#results-exercise",
    "title": "Applied Bayesian Analyses in R",
    "section": "Results Exercise",
    "text": "Results Exercise\nModel1: only an intercept (brms defaults)\n\nload(file = here(\n    \"Presentations\",\n    \"MarathonData.RData\")\n    )\n\nMod_MT1 &lt;- brm(                        \n  MarathonTimeM ~ 1, \n  data = MarathonData,   \n  backend = \"cmdstanr\",\n  seed = 1975           \n)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.0 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.0 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.6 seconds."
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#results-exercise-1",
    "href": "Presentations/Part1/Slides_Part1.html#results-exercise-1",
    "title": "Applied Bayesian Analyses in R",
    "section": "Results Exercise",
    "text": "Results Exercise\nModel1: only an intercept (brms defaults)\n\nsummary(Mod_MT1)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: MarathonTimeM ~ 1 \n   Data: MarathonData (Number of observations: 87) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   199.12      2.42   194.38   203.76 1.00     2903     2125\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    22.84      1.73    19.72    26.39 1.00     3314     2946\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#results-exercise-2",
    "href": "Presentations/Part1/Slides_Part1.html#results-exercise-2",
    "title": "Applied Bayesian Analyses in R",
    "section": "Results Exercise",
    "text": "Results Exercise\nModel1: only an intercept (brms defaults)\n\nplot(Mod_MT1)"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#results-exercise-3",
    "href": "Presentations/Part1/Slides_Part1.html#results-exercise-3",
    "title": "Applied Bayesian Analyses in R",
    "section": "Results Exercise",
    "text": "Results Exercise\nModel2: add the effect of km4week and sp4week\n\nMod_MT2 &lt;- brm(                        \n  MarathonTimeM ~ 1 + km4week + sp4week, \n  data = MarathonData,   \n  backend = \"cmdstanr\",\n  seed = 1975           \n)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.0 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.0 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.5 seconds."
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#results-exercise-4",
    "href": "Presentations/Part1/Slides_Part1.html#results-exercise-4",
    "title": "Applied Bayesian Analyses in R",
    "section": "Results Exercise",
    "text": "Results Exercise\nModel2: add the effect of km4week and sp4week\n\nsummary(Mod_MT2)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: MarathonTimeM ~ 1 + km4week + sp4week \n   Data: MarathonData (Number of observations: 87) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   199.20      1.50   196.28   202.15 1.00     3852     2838\nkm4week      -0.42      0.06    -0.53    -0.31 1.00     4189     3157\nsp4week      -9.98      1.30   -12.46    -7.50 1.00     4402     3263\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    13.94      1.09    11.96    16.29 1.00     3991     3236\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#results-exercise-5",
    "href": "Presentations/Part1/Slides_Part1.html#results-exercise-5",
    "title": "Applied Bayesian Analyses in R",
    "section": "Results Exercise",
    "text": "Results Exercise\nModel2: add the effect of km4week and sp4week\n\nplot(Mod_MT2)"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#results-exercise-6",
    "href": "Presentations/Part1/Slides_Part1.html#results-exercise-6",
    "title": "Applied Bayesian Analyses in R",
    "section": "Results Exercise",
    "text": "Results Exercise\nModel3: change the brms defaults\n\nMod_MT3 &lt;- brm(                        \n  MarathonTimeM ~ 1 + km4week + sp4week, \n  data = MarathonData,\n  chains = 4,\n  iter = 4000,\n  cores = 4,\n  backend = \"cmdstanr\",\n  seed = 1975           \n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 1 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 1 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 1 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 1 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 1 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 1 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 1 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 1 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 1 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 1 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 1 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 1 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 1 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 1 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 1 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 1 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 1 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 1 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 1 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 1 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 1 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 1 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 1 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 1 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 1 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 1 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 1 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 1 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 1 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 1 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 1 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 1 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 1 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 1 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 1 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 1 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 1 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 1 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 1 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 1 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 2 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 2 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 2 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 2 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 2 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 2 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 2 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 2 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 2 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 2 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 2 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 2 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 2 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 2 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 2 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 2 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 2 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 2 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 2 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 2 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 2 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 2 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 2 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 2 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 2 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 2 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 2 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 2 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 2 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 2 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 2 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 2 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 2 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 2 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 2 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 2 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 2 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 2 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 2 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 2 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 2 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 3 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 3 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 3 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 3 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 3 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 3 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 3 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 3 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 3 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 3 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 3 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 3 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 3 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 3 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 3 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 3 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 3 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 3 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 3 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 3 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 3 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 3 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 3 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 3 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 3 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 3 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 3 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 3 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 3 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 3 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 3 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 3 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 3 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 3 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 3 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 3 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 3 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 3 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 3 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 3 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 3 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 4 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 4 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 4 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 4 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 4 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 4 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 4 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 4 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 4 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 4 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 4 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 4 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 4 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 4 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 4 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 4 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 4 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 4 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 4 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 4 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 4 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 4 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 4 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 4 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 4 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 4 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 4 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 4 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 4 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 4 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 4 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 4 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 4 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 4 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 4 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 4 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 4 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 4 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 4 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 4 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 4 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds."
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#results-exercise-7",
    "href": "Presentations/Part1/Slides_Part1.html#results-exercise-7",
    "title": "Applied Bayesian Analyses in R",
    "section": "Results Exercise",
    "text": "Results Exercise\nModel3: change the brms defaults\n\nsummary(Mod_MT3)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: MarathonTimeM ~ 1 + km4week + sp4week \n   Data: MarathonData (Number of observations: 87) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   199.15      1.51   196.21   202.14 1.00     8054     5568\nkm4week      -0.42      0.06    -0.53    -0.31 1.00     8127     5617\nsp4week      -9.98      1.29   -12.50    -7.48 1.00     8080     6304\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    13.92      1.07    12.02    16.22 1.00     7672     5845\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#results-exercise-8",
    "href": "Presentations/Part1/Slides_Part1.html#results-exercise-8",
    "title": "Applied Bayesian Analyses in R",
    "section": "Results Exercise",
    "text": "Results Exercise\nModel3: change the brms defaults\n\nplot(Mod_MT3)"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#about-convergence",
    "href": "Presentations/Part1/Slides_Part1.html#about-convergence",
    "title": "Applied Bayesian Analyses in R",
    "section": "About convergence",
    "text": "About convergence\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\widehat R\\) &lt; 1.015 for each parameter estimate\nat least 4 chains are recommended\nEffective Sample Size (ESS) &gt; 400 to rely on \\(\\widehat R\\)"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#but-is-it-a-good-model",
    "href": "Presentations/Part1/Slides_Part1.html#but-is-it-a-good-model",
    "title": "Applied Bayesian Analyses in R",
    "section": "But is it a good model?",
    "text": "But is it a good model?\n  Two complementary procedures:  \n\nposterior-predictive check\ncompare models with leave one out cross-validation"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#posterior-predictive-check",
    "href": "Presentations/Part1/Slides_Part1.html#posterior-predictive-check",
    "title": "Applied Bayesian Analyses in R",
    "section": "Posterior-predictive check",
    "text": "Posterior-predictive check\nA visual check that can be performed with pp_check() from brms\n\nMarathonTimes_Mod2 &lt;-\n  readRDS(file = \n            here(\"Presentations\",\n              \"Output\",\n              \"MarathonTimes_Mod2.RDS\")\n          )\n\npp_check(MarathonTimes_Mod2) + theme_minimal()"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#model-comparison-with-loo-cross-validation",
    "href": "Presentations/Part1/Slides_Part1.html#model-comparison-with-loo-cross-validation",
    "title": "Applied Bayesian Analyses in R",
    "section": "Model comparison with loo cross-validation",
    "text": "Model comparison with loo cross-validation\n\\(\\sim\\) AIC or BIC in Frequentist statistics\n\\(\\widehat{elpd}\\): “expected log predictive density” (higher \\(\\widehat{elpd}\\) implies better model fit without being sensitive for overfitting!)\n\nloo_Mod1 &lt;- loo(MarathonTimes_Mod1)\nloo_Mod2 &lt;- loo(MarathonTimes_Mod2)"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#your-turn-1",
    "href": "Presentations/Part1/Slides_Part1.html#your-turn-1",
    "title": "Applied Bayesian Analyses in R",
    "section": " Your Turn",
    "text": "Your Turn\n\nTime to switch to your dataset\nThink of two alternative models for a certain variable\nBe naive! Let’s assume normality and\nKeep it simple\nEstimate the two models\nCompare the models\nInterpret the best fitting model\nCheck the fit with pp_check()"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#family-time",
    "href": "Presentations/Part1/Slides_Part1.html#family-time",
    "title": "Applied Bayesian Analyses in R",
    "section": "Family time",
    "text": "Family time\nbrms defaults to family = gaussian(link = \"identity\")\n\nMod_MT1 &lt;- brm(                        \n  MT ~ 1, \n  family = gaussian(link = \"identity\"),\n  data = DataMT,   \n  backend = \"cmdstanr\",\n  chains = 5,\n  iter = 6000,\n  warmup = 1500,\n  cores = 4,\n  seed = 1975           \n)"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#family-time-1",
    "href": "Presentations/Part1/Slides_Part1.html#family-time-1",
    "title": "Applied Bayesian Analyses in R",
    "section": "Family time",
    "text": "Family time\n\nBut many alternatives are available!\n\nThe default family types known in R (e.g, binomial(link = \"logit\"), Gamma(link = \"inverse\"), poisson(link = \"log\"), …)\n\nsee help(family)\n\n\nModX &lt;- brm(                        \n  Y ~ 1, \n  family = binomial(link = \"logit\"),\n  ...\n)"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#family-time-2",
    "href": "Presentations/Part1/Slides_Part1.html#family-time-2",
    "title": "Applied Bayesian Analyses in R",
    "section": "Family time",
    "text": "Family time\n\nAnd even more!\n\nbrmsfamily(...) has a lot of possible models\n\nsee help(brmsfamily)\n\n\nModX &lt;- brm(                        \n  Y ~ 1, \n  family = brmsfamily(skew_normal, \n                      link = \"identity\", \n                      link_sigma = \"log\", \n                      link_alpha = \"identity\"),\n  ...\n)"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#mixed-effects-models",
    "href": "Presentations/Part1/Slides_Part1.html#mixed-effects-models",
    "title": "Applied Bayesian Analyses in R",
    "section": "Mixed effects models",
    "text": "Mixed effects models\n\nbrms can estimate models for more complex designs like multilevel models or more generally mixed effects models\n Random intercepts…\n\nModX &lt;- brm(                        \n  Y ~ 1 + x + \n    (1 | Groupvariable), \n  ...\n)\n\nRandom intercepts and slopes…\n\nModX &lt;- brm(                        \n  Y ~ 1 + x + \n    (1 + x | Groupvariable), \n  ...\n)"
  },
  {
    "objectID": "Links.html",
    "href": "Links.html",
    "title": "Links",
    "section": "",
    "text": "Many books are written that discuss Bayesian modelling. Here you can find two books that I would suggest for learning more about Bayesian analyses.\nA first book is the book by Ben Lambert: “A Student’s Guide to Bayesian Statistics”. This book is great if you want to learn more about the statistical theory behind Bayesian analyses and MCMC sampling. He succeeds in explaining these topics in a thorough conceptual way, without you getting lost in the mathematics and formulas. For me, this book was of great value!\nA big hit these days is the book by  Richard McElreath: “Statistical Rethinking. A Bayesian Course with examples in R and Stan” . In this course, Richard McElreath introduces statistical reasoning from scratch, integrating three key components for good statistical thinking: DAGs, Bayesian statistics, and multi-level models. So, this book is more than solely a book about Bayesian models. The lectures of Richard McElreath based on this book can be found for free on YouTube.",
    "crumbs": [
      "Links"
    ]
  },
  {
    "objectID": "Links.html#reading-tips",
    "href": "Links.html#reading-tips",
    "title": "Links",
    "section": "",
    "text": "Many books are written that discuss Bayesian modelling. Here you can find two books that I would suggest for learning more about Bayesian analyses.\nA first book is the book by Ben Lambert: “A Student’s Guide to Bayesian Statistics”. This book is great if you want to learn more about the statistical theory behind Bayesian analyses and MCMC sampling. He succeeds in explaining these topics in a thorough conceptual way, without you getting lost in the mathematics and formulas. For me, this book was of great value!\nA big hit these days is the book by  Richard McElreath: “Statistical Rethinking. A Bayesian Course with examples in R and Stan” . In this course, Richard McElreath introduces statistical reasoning from scratch, integrating three key components for good statistical thinking: DAGs, Bayesian statistics, and multi-level models. So, this book is more than solely a book about Bayesian models. The lectures of Richard McElreath based on this book can be found for free on YouTube.",
    "crumbs": [
      "Links"
    ]
  },
  {
    "objectID": "Links.html#some-free-online-books",
    "href": "Links.html#some-free-online-books",
    "title": "Links",
    "section": "Some free online books ",
    "text": "Some free online books \n\nJohn Kruschke’s book Doing Bayesian Data Analyses: a tutorial with R, Jags, and Stan :\n\nhttps://nyu-cdsc.github.io/learningr/assets/kruschke_bayesian_in_R.pdf\n\nBayes Rules!:\n\nhttps://www.bayesrulesbook.com/\n\nOr this book:\n\n https://vasishth.github.io/bayescogsci/book/",
    "crumbs": [
      "Links"
    ]
  },
  {
    "objectID": "Links.html#the-podcast",
    "href": "Links.html#the-podcast",
    "title": "Links",
    "section": "THE Podcast ",
    "text": "THE Podcast \nIf you like running - like I do - this could be a great companion on your run! In this podcast different guests from different backgrounds discuss the power and reasoning behind Bayesian analyses and how Bayesian statistics are used in their field from Astronomy, over Psychology to Sports Analyses (and many more).\nhttps://www.learnbayesstats.com/",
    "crumbs": [
      "Links"
    ]
  },
  {
    "objectID": "Links.html#online-learning-material",
    "href": "Links.html#online-learning-material",
    "title": "Links",
    "section": "Online learning material",
    "text": "Online learning material\nThe research group of Rens van de Schoot (Utrecht University) publishes a lot on Bayesian statistics. They also developed a summer school. Their materials are also openly available at (you will notice how their way of sharing materials has inspired me :-))\nhttps://utrechtuniversity.github.io/BayesianEstimation/#quick-overview",
    "crumbs": [
      "Links"
    ]
  },
  {
    "objectID": "Links.html#some-example-papers",
    "href": "Links.html#some-example-papers",
    "title": "Links",
    "section": "Some example papers",
    "text": "Some example papers\nHere are some references to articles reporting on Bayesian analyses.\nGijsen, M., Catrysse, L., De Maeyer, S., & Gijbels, D. (2024). Mapping cognitive processes in video-based learning by combining trace and think-aloud data. Learning and Instruction, 90, 101851. https://doi.org/10.1016/j.learninstruc.2023.101851 \nRoeser, J., De Maeyer, S., Leijten, M., & Van Waes, L. (2021). Modelling typing disfluencies as finite mixture process. Reading and Writing.  https://doi.org/10.1007/s11145-021-10203-z",
    "crumbs": [
      "Links"
    ]
  },
  {
    "objectID": "Links.html#article-on-bayesian-evidence-interval",
    "href": "Links.html#article-on-bayesian-evidence-interval",
    "title": "Links",
    "section": "Article on Bayesian Evidence Interval",
    "text": "Article on Bayesian Evidence Interval\nDuring the course I mentioned a paper that criticises the ROPE + HDI rule and proposes an alternative for making decisions on hypotheses in the Bayesian realm. This is the reference:\nKelter, R. (2022), The evidence interval and the Bayesian evidence value: On a unified theory for Bayesian hypothesis testing and interval estimation. Br J Math Stat Psychol, 75: 550-592.\n https://doi.org/10.1111/bmsp.12267",
    "crumbs": [
      "Links"
    ]
  },
  {
    "objectID": "Prerequisites/Prerequisites.html",
    "href": "Prerequisites/Prerequisites.html",
    "title": "Prerequisites for the workshop Applied Bayesian Analyses in R",
    "section": "",
    "text": "For the workshop we will be using R. To be able to participate properly, it is advisable to install a number of packages on your own PC beforehand, as well as to become familiar with the way I usually code in R. This document is a brief guide to help you prepare.",
    "crumbs": [
      "Prerequisites"
    ]
  },
  {
    "objectID": "Prerequisites/Prerequisites.html#cmdstan-cmdstanr",
    "href": "Prerequisites/Prerequisites.html#cmdstan-cmdstanr",
    "title": "Prerequisites for the workshop Applied Bayesian Analyses in R",
    "section": "CmdStan & CmdStanR",
    "text": "CmdStan & CmdStanR\nBayesian estimation and analyses require some dedicated samplers that should be installed on your machine. The workflow that we will cover in the workshop relies on Stan software. Stan stands alone from R but can be called through R making use of two different ways. We will cover this more in the workshop.\nFor the workshop I will use the CmdStan chain and the package cmdstanr. Therefore, I would encourage you to download and install a working cmdstan and cmdstanr. The following vignette can help you accomplish this: https://mc-stan.org/cmdstanr/articles/cmdstanr.html\nNormally if you follow the steps described in that vignette you will succeed in installing and testing your installation.",
    "crumbs": [
      "Prerequisites"
    ]
  },
  {
    "objectID": "Prerequisites/Prerequisites.html#brms",
    "href": "Prerequisites/Prerequisites.html#brms",
    "title": "Prerequisites for the workshop Applied Bayesian Analyses in R",
    "section": "brms",
    "text": "brms\nFor the analyses we will rely on the amazing package brms. Installing this package is not that difficult, but maybe install before the workshop and start some exploring ;-)",
    "crumbs": [
      "Prerequisites"
    ]
  },
  {
    "objectID": "Prerequisites/Prerequisites.html#tidyverse",
    "href": "Prerequisites/Prerequisites.html#tidyverse",
    "title": "Prerequisites for the workshop Applied Bayesian Analyses in R",
    "section": "tidyverse",
    "text": "tidyverse\nWhen I code I use the functional programming  approach and a lot of the tidy principles (see below). The package tidyverse bundles a set of packages that are needed for this approach to coding. If you want to learn more on the whole universe of tidyverse you can explore the book of Hadley Wickham: https://r4ds.had.co.nz/",
    "crumbs": [
      "Prerequisites"
    ]
  },
  {
    "objectID": "Prerequisites/Prerequisites.html#ggplot2",
    "href": "Prerequisites/Prerequisites.html#ggplot2",
    "title": "Prerequisites for the workshop Applied Bayesian Analyses in R",
    "section": "ggplot2",
    "text": "ggplot2\nPart of the tidyverse package is the ggplot2 package. Therefore it will be installed if you also install tidyverse normally. Nevertheless, I find it worth mentioning separately because I think knowing how to make graphs with ggplot2 is very handy even if you do not dive into bayesian analyses. A great resource to learn to visualize your data is the tutorial written by Cédric Scherer: https://www.cedricscherer.com/2019/08/05/a-ggplot2-tutorial-for-beautiful-plotting-in-r/!\nHe also blogs on creating great visualizations and has some nice talks that I think you can find on YouTube!",
    "crumbs": [
      "Prerequisites"
    ]
  },
  {
    "objectID": "Prerequisites/Prerequisites.html#patchwork",
    "href": "Prerequisites/Prerequisites.html#patchwork",
    "title": "Prerequisites for the workshop Applied Bayesian Analyses in R",
    "section": "patchwork",
    "text": "patchwork\nI mention this package separately as well. patchwork allows you to combine different plots created with ggplot2. This package should be installed on it’s own.",
    "crumbs": [
      "Prerequisites"
    ]
  },
  {
    "objectID": "Prerequisites/Prerequisites.html#tidybayes",
    "href": "Prerequisites/Prerequisites.html#tidybayes",
    "title": "Prerequisites for the workshop Applied Bayesian Analyses in R",
    "section": "tidybayes",
    "text": "tidybayes\nThis package is a dedicated package to make use of the tidy principles when applying it to (results of) Bayesian analyses.",
    "crumbs": [
      "Prerequisites"
    ]
  },
  {
    "objectID": "Prerequisites/Prerequisites.html#bayesplot",
    "href": "Prerequisites/Prerequisites.html#bayesplot",
    "title": "Prerequisites for the workshop Applied Bayesian Analyses in R",
    "section": "bayesplot",
    "text": "bayesplot\nWe will use visualisations a lot in Bayesian analyses to summarize the information of models and parameter estimation. bayesplot bundles a number of functions that make this visualizing much easier (although it uses ggplot2 under the hood so we could accomplish similar results just knowing ggplot2).",
    "crumbs": [
      "Prerequisites"
    ]
  },
  {
    "objectID": "Prerequisites/Prerequisites.html#the-pipe",
    "href": "Prerequisites/Prerequisites.html#the-pipe",
    "title": "Prerequisites for the workshop Applied Bayesian Analyses in R",
    "section": "The pipe",
    "text": "The pipe\nWhen coding I make use of what is called the pipe operator (notice that you can only use this when you loaded the tidyverse package).\nThe pipe in R code is the following: %&gt;%\nWhat is nice about using the pipe operator is that you can read a piece of code from left to right focussing on the verbs (as Hadley Wickham will call it sometimes).\nA short example.\nImagine you have a vector of numbers for which you want to calculate a mean: c(1,2,3,4,5).\nThere are different ways to do this in R.\nA first one is doing it in separate lines of code:\n\nx &lt;- c(1,2,3,4,5)\nmean(x)\n\n[1] 3\n\n\nFor this easy example this would work fine. But a typical side-effect of working this way is that we create a lot of objects along the way. Here we created the object x that stored the values of the vector.\nTo avoid this we could combine all the code in one line by embedding functions:\n\nmean(c(1,2,3,4,5))\n\n[1] 3\n\n\nOne line of code and no storage of a new object. But this can get complicated if you want to combine a set of functions resulting in almost non-readable code!\nThen there is the pipe operator:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nc(1,2,3,4,5) %&gt;%\n  mean()\n\n[1] 3\n\n\nThis is how to read this code:\n\nc(1,2,3,4,5) %&gt;% # create the vector and then\n  mean()         # calculate the mean\n\n[1] 3\n\n\nSo the pipe can be read as take (or create) A and then do B with the result. We could create a whole pipeline of functions to be applied in this way making use of the pipe operator (this becomes more clear in the next section when we shortly describe the dplyr verbs).\nA tweet that I encountered gives a great analogy. Using the pipe you could write:\n\nI %&gt;% woke up %&gt;% took a shower %&gt;% got breakfast %&gt;% took the metro %&gt;% arrived at work %&gt;% …\n\nIn a recent update of R they also introduced this idea of a pipe as a “native pipe”. This is written as |&gt; and functions in a similar way as %&gt;% with the advantage that it will also work outside the tidyverse. I’m slowly transforming all my material and code to make use of this native pipe. But as I am notoriously sloppy, the chances are high that you will encounter some old-fashioned tidyverse pipes in my code during the workshop…\n\nI |&gt; learned about the native pipe |&gt; transformed my code |&gt; potentially failed somewhere …",
    "crumbs": [
      "Prerequisites"
    ]
  },
  {
    "objectID": "Prerequisites/Prerequisites.html#dplyr-verbs",
    "href": "Prerequisites/Prerequisites.html#dplyr-verbs",
    "title": "Prerequisites for the workshop Applied Bayesian Analyses in R",
    "section": "dplyr verbs",
    "text": "dplyr verbs\nWithin the tidyverse universe we can make use of a great package called dplyr for a broad spectrum of data-management functions. The functions provided in that package are also sometimes called verbs:\n\nmutate( ) to do recoding etc;\nselect( ) to select a subset of columns/variables;\nfilter( ) to filter cases;\ngroup_by( ) to apply what’s coming afterwards for specific groups in the data by a grouping variable;\narrange( ) to sort the data;\nsummarize( ) to apply summarizing functions (like taking the mean etc.);\nrename( ) to rename columns;\n\nThese functions can be combined in a single statement making use of the pipe operator.\nHere are some examples on the use of these verbs applied to the built-in dataset starwars that is part of the tidyverse package.\nCalculate the mean birth_year for all the characters:\n\nstarwars |&gt; \n  summarize(\n    mean_birth_year = mean(birth_year, na.rm = TRUE)\n    ) |&gt;\n  print()\n\n# A tibble: 1 × 1\n  mean_birth_year\n            &lt;dbl&gt;\n1            87.6\n\n\nCalculate the average height and the average mass by gender:\n\nstarwars |&gt;\n  group_by(gender) |&gt;\n  summarize(\n    mean_height = mean(height, na.rm = T),\n    mean_mass = mean(mass, na.rm = T)\n  ) \n\n# A tibble: 3 × 3\n  gender    mean_height mean_mass\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;\n1 feminine         167.      54.7\n2 masculine        177.     107. \n3 &lt;NA&gt;             175       81  \n\n\nNow, let’s use the verbs to create a graph. We want to create a scatterplot with mass on the x-axis and height on the y-axis. Also, we want to add a third variable in the mix: whether the character’s birth year is hither than 50 or not (just for fun). Finally, we only want to show characters that have a mass lower than 200 and with a birth_year that is known.\n\nstarwars |&gt;\n  # First filter the cases with a mass higher than 200 and for which birth_year is not NA\n  filter(\n    mass &lt; 200,\n    !is.na(birth_year)\n  ) |&gt;\n\n  # Then we create a variable that has value yes if the character's birth year is above 50 and no otherwise\n  mutate(\n    birth_above_50 = if_else(birth_year &gt; 50, \"yes\", \"no\") \n  ) |&gt;\n  \n  # Select the necessary variables for the graph (not really necessary but this is to show this option)\n  select(\n    height, \n    mass, \n    birth_above_50) |&gt;\n\n  # Create the graph\n  ggplot(\n    aes(x = mass, y = height, color = birth_above_50)\n  ) + geom_point()",
    "crumbs": [
      "Prerequisites"
    ]
  },
  {
    "objectID": "WAMBS.html",
    "href": "WAMBS.html",
    "title": "WAMBS template",
    "section": "",
    "text": "During the course we introduced the WAMBS (when to Worry and how to Avoid the Misuse of Bayesian Statistics) check-list as a tool to structure your thinking and checking of your Bayesian model(s).\nTo see an example of this WAMBS checklist applied to the final model on the Marathon Data click here.\nThe Quarto document to make this report can be found here (right-click to save).\nIf you want to render the Quarto document and have the references in there, you also need to download the references.bib file and store it in the same folder as the Quarto document. The references file can be downloaded here (right-click to save).",
    "crumbs": [
      "WAMBS template"
    ]
  },
  {
    "objectID": "WAMBS.html#info",
    "href": "WAMBS.html#info",
    "title": "WAMBS template",
    "section": "",
    "text": "During the course we introduced the WAMBS (when to Worry and how to Avoid the Misuse of Bayesian Statistics) check-list as a tool to structure your thinking and checking of your Bayesian model(s).\nTo see an example of this WAMBS checklist applied to the final model on the Marathon Data click here.\nThe Quarto document to make this report can be found here (right-click to save).\nIf you want to render the Quarto document and have the references in there, you also need to download the references.bib file and store it in the same folder as the Quarto document. The references file can be downloaded here (right-click to save).",
    "crumbs": [
      "WAMBS template"
    ]
  },
  {
    "objectID": "WAMBS.html#references-and-resources",
    "href": "WAMBS.html#references-and-resources",
    "title": "WAMBS template",
    "section": "References and resources",
    "text": "References and resources\nDepaoli, S., & Van de Schoot, R. (2017). Improving transparency and replication in Bayesian statistics: The WAMBS-Checklist. Psychological methods, 22(2), 240.\nVan de Schoot, R., Veen, D., Smeets, L., Winter, S. D., & Depaoli, S. (2020). A tutorial on using the WAMBS checklist to avoid the misuse of Bayesian statistics. Small Sample Size Solutions: A Guide for Applied Researchers and Practitioners; van de Schoot, R., Miocevic, M., Eds, 30-49.",
    "crumbs": [
      "WAMBS template"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "I’m an R-enthusiast teaching statistics for behavioral researchers from an applied point of view. My main mission is to get people enthusiastic on the wonderful world of statistics.\nIn 2020, I started deep-diving into the world of Bayesian analyses during my sabbatical leave in full Covid times. The last 5 years I had the nice opportunity to share my learning with others in the form of different workshops (also here in Turku) and by writing some blog-posts to process what I learned myself.\nOne of my key values is Open Scholarship, sharing all research and teaching materials as open as possible for the broad (academic) audience. If you want to dive into my other materials and irregular blog-posts you can visit my website: Sven’s sharing place.\nI wish you a pleasant learning journey!",
    "crumbs": [
      "About me"
    ]
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About me",
    "section": "Contact",
    "text": "Contact\nQuestions or feedback, do not hesitate to contact me at sven.demaeyer@uantwerpen.be",
    "crumbs": [
      "About me"
    ]
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html",
    "href": "Presentations/Part2/Slides_Part2.html",
    "title": "Applied Bayesian Analyses in R",
    "section": "",
    "text": "here() starts at /Users/svendemaeyer/Library/CloudStorage/OneDrive-UniversiteitAntwerpen/Praatjes/Workshops/Zurich_2026/ABAR_Zurich26_Web\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nLoading required package: Rcpp\n\nLoading 'brms' package (version 2.22.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n\n\nAttaching package: 'brms'\n\n\nThe following object is masked from 'package:stats':\n\n    ar\n\n\nThis is bayesplot version 1.12.0\n\n- Online documentation and vignettes at mc-stan.org/bayesplot\n\n- bayesplot theme set to bayesplot::theme_default()\n\n   * Does _not_ affect other ggplot2 plots\n\n   * See ?bayesplot_theme_set for details on theme setting\n\n\nAttaching package: 'bayesplot'\n\n\nThe following object is masked from 'package:brms':\n\n    rhat\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#your-turn",
    "href": "Presentations/Part2/Slides_Part2.html#your-turn",
    "title": "Applied Bayesian Analyses in R",
    "section": " Your Turn",
    "text": "Your Turn\n\nOpen MarathonData.RData\nEstimate your first Bayesian Models\nDependent variable: MarathonTimeM\nModel1: only an intercept\nModel2: introduce the effect of km4week and sp4week on MarathonTimeM\nChange the brms defaults (4 chains of 6000 iterations)\nMake plots with the plot() function\nWhat do we learn? (rough interpretation)\n\n\n\nNote: I centred both km4weekand sp4week around their mean!"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#results-exercise",
    "href": "Presentations/Part2/Slides_Part2.html#results-exercise",
    "title": "Applied Bayesian Analyses in R",
    "section": "Results Exercise",
    "text": "Results Exercise\nModel1: only an intercept (brms defaults)\n\nload(file = here(\n    \"Presentations\",\n    \"MarathonData.RData\")\n    )\n\nMod_MT1 &lt;- brm(                        \n  MarathonTimeM ~ 1, \n  data = MarathonData,   \n  backend = \"cmdstanr\",\n  seed = 1975           \n)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.0 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.0 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.5 seconds."
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#results-exercise-1",
    "href": "Presentations/Part2/Slides_Part2.html#results-exercise-1",
    "title": "Applied Bayesian Analyses in R",
    "section": "Results Exercise",
    "text": "Results Exercise\nModel1: only an intercept (brms defaults)\n\nsummary(Mod_MT1)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: MarathonTimeM ~ 1 \n   Data: MarathonData (Number of observations: 87) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   199.12      2.42   194.38   203.76 1.00     2903     2125\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    22.84      1.73    19.72    26.39 1.00     3314     2946\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#results-exercise-2",
    "href": "Presentations/Part2/Slides_Part2.html#results-exercise-2",
    "title": "Applied Bayesian Analyses in R",
    "section": "Results Exercise",
    "text": "Results Exercise\nModel1: only an intercept (brms defaults)\n\nplot(Mod_MT1)"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#results-exercise-3",
    "href": "Presentations/Part2/Slides_Part2.html#results-exercise-3",
    "title": "Applied Bayesian Analyses in R",
    "section": "Results Exercise",
    "text": "Results Exercise\nModel2: add the effect of km4week and sp4week\n\nMod_MT2 &lt;- brm(                        \n  MarathonTimeM ~ 1 + km4week + sp4week, \n  data = MarathonData,   \n  backend = \"cmdstanr\",\n  seed = 1975           \n)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.0 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.0 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.5 seconds."
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#results-exercise-4",
    "href": "Presentations/Part2/Slides_Part2.html#results-exercise-4",
    "title": "Applied Bayesian Analyses in R",
    "section": "Results Exercise",
    "text": "Results Exercise\nModel2: add the effect of km4week and sp4week\n\nsummary(Mod_MT2)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: MarathonTimeM ~ 1 + km4week + sp4week \n   Data: MarathonData (Number of observations: 87) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   199.20      1.50   196.28   202.15 1.00     3852     2838\nkm4week      -0.42      0.06    -0.53    -0.31 1.00     4189     3157\nsp4week      -9.98      1.30   -12.46    -7.50 1.00     4402     3263\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    13.94      1.09    11.96    16.29 1.00     3991     3236\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#results-exercise-5",
    "href": "Presentations/Part2/Slides_Part2.html#results-exercise-5",
    "title": "Applied Bayesian Analyses in R",
    "section": "Results Exercise",
    "text": "Results Exercise\nModel2: add the effect of km4week and sp4week\n\nplot(Mod_MT2)"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#results-exercise-6",
    "href": "Presentations/Part2/Slides_Part2.html#results-exercise-6",
    "title": "Applied Bayesian Analyses in R",
    "section": "Results Exercise",
    "text": "Results Exercise\nModel3: change the brms defaults\n\nMod_MT3 &lt;- brm(                        \n  MarathonTimeM ~ 1 + km4week + sp4week, \n  data = MarathonData,\n  chains = 4,\n  iter = 4000,\n  cores = 4,\n  backend = \"cmdstanr\",\n  seed = 1975           \n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 1 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 1 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 1 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 1 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 1 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 1 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 1 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 1 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 1 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 1 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 1 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 1 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 1 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 1 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 1 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 1 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 1 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 1 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 1 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 1 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 1 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 1 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 1 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 1 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 1 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 1 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 1 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 1 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 1 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 1 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 1 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 1 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 1 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 1 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 1 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 1 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 1 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 1 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 1 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 1 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 2 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 2 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 2 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 2 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 2 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 2 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 2 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 2 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 2 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 2 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 2 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 2 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 2 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 2 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 2 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 2 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 2 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 2 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 2 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 2 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 2 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 2 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 2 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 2 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 2 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 2 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 2 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 2 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 2 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 2 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 2 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 2 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 2 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 2 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 2 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 2 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 2 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 2 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 2 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 2 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 2 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 3 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 3 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 3 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 3 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 3 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 3 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 3 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 3 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 3 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 3 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 3 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 3 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 3 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 3 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 3 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 3 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 3 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 3 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 3 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 3 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 3 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 3 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 3 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 3 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 3 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 3 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 3 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 3 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 3 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 3 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 3 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 3 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 3 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 3 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 3 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 3 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 3 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 3 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 3 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 3 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 3 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 4 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 4 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 4 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 4 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 4 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 4 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 4 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 4 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 4 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 4 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 4 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 4 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 4 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 4 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 4 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 4 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 4 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 4 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 4 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 4 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 4 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 4 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 4 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 4 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 4 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 4 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 4 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 4 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 4 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 4 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 4 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 4 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 4 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 4 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 4 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 4 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 4 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 4 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 4 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 4 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 4 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds."
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#results-exercise-7",
    "href": "Presentations/Part2/Slides_Part2.html#results-exercise-7",
    "title": "Applied Bayesian Analyses in R",
    "section": "Results Exercise",
    "text": "Results Exercise\nModel3: change the brms defaults\n\nsummary(Mod_MT3)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: MarathonTimeM ~ 1 + km4week + sp4week \n   Data: MarathonData (Number of observations: 87) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   199.15      1.51   196.21   202.14 1.00     8054     5568\nkm4week      -0.42      0.06    -0.53    -0.31 1.00     8127     5617\nsp4week      -9.98      1.29   -12.50    -7.48 1.00     8080     6304\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    13.92      1.07    12.02    16.22 1.00     7672     5845\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#results-exercise-8",
    "href": "Presentations/Part2/Slides_Part2.html#results-exercise-8",
    "title": "Applied Bayesian Analyses in R",
    "section": "Results Exercise",
    "text": "Results Exercise\nModel3: change the brms defaults\n\nplot(Mod_MT3)"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#about-convergence",
    "href": "Presentations/Part2/Slides_Part2.html#about-convergence",
    "title": "Applied Bayesian Analyses in R",
    "section": "About convergence",
    "text": "About convergence\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\widehat R\\) &lt; 1.015 for each parameter estimate\nat least 4 chains are recommended\nEffective Sample Size (ESS) &gt; 400 to rely on \\(\\widehat R\\)"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#but-is-it-a-good-model",
    "href": "Presentations/Part2/Slides_Part2.html#but-is-it-a-good-model",
    "title": "Applied Bayesian Analyses in R",
    "section": "But is it a good model?",
    "text": "But is it a good model?\n  Two complementary procedures:  \n\nposterior-predictive check\ncompare models with leave one out cross-validation"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#posterior-predictive-check",
    "href": "Presentations/Part2/Slides_Part2.html#posterior-predictive-check",
    "title": "Applied Bayesian Analyses in R",
    "section": "Posterior-predictive check",
    "text": "Posterior-predictive check\nA visual check that can be performed with pp_check() from brms\n\npp_check(Mod_MT2) + theme_minimal()\n\nUsing 10 posterior draws for ppc type 'dens_overlay' by default."
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#model-comparison-with-loo-cross-validation",
    "href": "Presentations/Part2/Slides_Part2.html#model-comparison-with-loo-cross-validation",
    "title": "Applied Bayesian Analyses in R",
    "section": "Model comparison with loo cross-validation",
    "text": "Model comparison with loo cross-validation\n\\(\\sim\\) AIC or BIC in Frequentist statistics\n\\(\\widehat{elpd}\\): “expected log predictive density” (higher \\(\\widehat{elpd}\\) implies better model fit without being sensitive for overfitting!)\n\nloo_Mod1 &lt;- loo(MarathonTimes_Mod1)\nloo_Mod2 &lt;- loo(MarathonTimes_Mod2)\n\nWarning: Found 1 observations with a pareto_k &gt; 0.7 in model\n'MarathonTimes_Mod2'. We recommend to set 'moment_match = TRUE' in order to\nperform moment matching for problematic observations.\n\nComparison&lt;- \n  loo_compare(\n    loo_Mod1, \n    loo_Mod2\n    )\n\nprint(Comparison, simplify = F)\n\n                   elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo\nMarathonTimes_Mod2    0.0       0.0  -356.3     12.0         7.2    4.4  \nMarathonTimes_Mod1  -39.9      12.6  -396.2      5.3         1.7    0.3  \n                   looic  se_looic\nMarathonTimes_Mod2  712.6   24.0  \nMarathonTimes_Mod1  792.3   10.6"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#when-to-worry-and-how-to-avoid-misuse-of-bayesian-statistics",
    "href": "Presentations/Part2/Slides_Part2.html#when-to-worry-and-how-to-avoid-misuse-of-bayesian-statistics",
    "title": "Applied Bayesian Analyses in R",
    "section": "When to Worry and How to Avoid Misuse of Bayesian Statistics",
    "text": "When to Worry and How to Avoid Misuse of Bayesian Statistics\nby Laurent Smeets and Rens van der Schoot\n\n\nBefore estimating the model:\n\nDo you understand the priors?\n\n\nAfter estimation before inspecting results:\n\nDoes the trace-plot exhibit convergence?\nDoes convergence remain after doubling the number of iterations?\nDoes the posterior distribution histogram have enough information?\nDo the chains exhibit a strong degree of autocorrelation?\nDo the posterior distributions make substantive sense?\n\n\nUnderstanding the exact influence of the priors\n\nDo different specification of the multivariate variance priors influence the results?\nIs there a notable effect of the prior when compared with non-informative priors?\nAre the results stable from a sensitivity analysis?\nIs the Bayesian way of interpreting and reporting model results used?\n\n\n\n\n\nTutorial source: https://www.rensvandeschoot.com/brms-wambs/ Alternatives exist as well like the BARG framework (Kruschke, J.K. Bayesian Analysis Reporting Guidelines. Nat Hum Behav 5, 1282–1291 (2021). https://doi.org/10.1038/s41562-021-01177-7)"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#wambs-template-to-use",
    "href": "Presentations/Part2/Slides_Part2.html#wambs-template-to-use",
    "title": "Applied Bayesian Analyses in R",
    "section": "WAMBS Template to use",
    "text": "WAMBS Template to use\n\nFile called WAMBS_workflow_MarathonData.qmd (quarto document)\nClick here  for the Quarto version\nCreate your own project and project folder\nCopy the template and rename it\nWe will go through the different parts in the slide show\nYou can apply/adapt the code in the template\nTo render the document properly with references, you also need the references.bib file"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#side-path-projects-in-rstudio-and-the-here-package",
    "href": "Presentations/Part2/Slides_Part2.html#side-path-projects-in-rstudio-and-the-here-package",
    "title": "Applied Bayesian Analyses in R",
    "section": "Side-path: projects in RStudio and the here package",
    "text": "Side-path: projects in RStudio and the here package\nIf you do not know how to use Projects in RStudio or the here package, these two sources might be helpfull:\nProjects: https://youtu.be/MdTtTN8PUqU?si=mmQGlU063EMt86B2\nhere package: https://youtu.be/oh3b3k5uM7E?si=0-heLJXfFVLtTohh"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#preparations-for-applying-it-to-marathon-model",
    "href": "Presentations/Part2/Slides_Part2.html#preparations-for-applying-it-to-marathon-model",
    "title": "Applied Bayesian Analyses in R",
    "section": "Preparations for applying it to Marathon model",
    "text": "Preparations for applying it to Marathon model\nPackages needed:\n\nlibrary(here)\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(bayesplot)\nlibrary(ggmcmc)\nlibrary(patchwork)\nlibrary(priorsense)"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#preparations-for-applying-it-to-marathon-model-1",
    "href": "Presentations/Part2/Slides_Part2.html#preparations-for-applying-it-to-marathon-model-1",
    "title": "Applied Bayesian Analyses in R",
    "section": "Preparations for applying it to Marathon model",
    "text": "Preparations for applying it to Marathon model\nLoad the dataset and the model:\n\nload(\n  file = here(\"Presentations\", \"MarathonData.RData\")\n)\n\nMarathonTimes_Mod2 &lt;-\n  readRDS(file = \n            here(\"Presentations\",\n              \"Output\",\n              \"MarathonTimes_Mod2.RDS\")\n          )"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#remember-priors-come-in-many-disguises",
    "href": "Presentations/Part2/Slides_Part2.html#remember-priors-come-in-many-disguises",
    "title": "Applied Bayesian Analyses in R",
    "section": "Remember: priors come in many disguises",
    "text": "Remember: priors come in many disguises\n\n\nUninformative/Weakly informative\nWhen objectivity is crucial and you want let the data speak for itself…\n\nInformative\nWhen including significant information is crucial\n\npreviously collected data\nresults from former research/analyses\ndata of another source\ntheoretical considerations\nelicitation"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#brms-defaults",
    "href": "Presentations/Part2/Slides_Part2.html#brms-defaults",
    "title": "Applied Bayesian Analyses in R",
    "section": "brms defaults",
    "text": "brms defaults\n\nWeakly informative priors\nIf dataset is big, impact of priors is minimal\nBut, always better to know what you are doing!\nComplex models might run into convergence issues \\(\\rightarrow\\) specifying more informative priors might help!\n\nSo, how to deviate from the defaults?"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#check-priors-used-by-brms",
    "href": "Presentations/Part2/Slides_Part2.html#check-priors-used-by-brms",
    "title": "Applied Bayesian Analyses in R",
    "section": "Check priors used by brms",
    "text": "Check priors used by brms\nFunction: get_prior( )\nRemember our model 2 for Marathon Times:\n\\[\\begin{aligned}\n& \\text{MarathonTimeM}_i \\sim N(\\mu,\\sigma_e)\\\\\n& \\mu = \\beta_0 + \\beta_1*\\text{km4week}_i + \\beta_2*\\text{sp4week}_i\n\\end{aligned}\\]\n\nget_prior(\n  MarathonTimeM ~ 1 + km4week + sp4week, \n  data = MarathonData\n)"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#check-priors-used-by-brms-1",
    "href": "Presentations/Part2/Slides_Part2.html#check-priors-used-by-brms-1",
    "title": "Applied Bayesian Analyses in R",
    "section": "Check priors used by brms",
    "text": "Check priors used by brms\n\n\n\n\n\n\n\n\n\n\nprior: type of prior distribution\nclass: parameter class (with b being population-effects)\ncoef: name of the coefficient within parameter class\ngroup: grouping factor for group-level parameters (when using mixed effects models)\nresp : name of the response variable when using multivariate models\nlb & ub: lower and upper bound for parameter restriction"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#visualizing-priors",
    "href": "Presentations/Part2/Slides_Part2.html#visualizing-priors",
    "title": "Applied Bayesian Analyses in R",
    "section": "Visualizing priors",
    "text": "Visualizing priors\nThe best way to make sense of the priors used is visualizing them!\nMany options:\n\nThe Zoo of Distributions https://ben18785.shinyapps.io/distribution-zoo/\nmaking your own visualizations\n\nSee WAMBS template!\nThere we demonstrate the use of ggplot2, metRology, ggtext and patchwork to visualize the priors."
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#visualizing-priors-1",
    "href": "Presentations/Part2/Slides_Part2.html#visualizing-priors-1",
    "title": "Applied Bayesian Analyses in R",
    "section": "Visualizing priors",
    "text": "Visualizing priors\n\nlibrary(metRology)\nlibrary(ggplot2)\nlibrary(ggtext)\nlibrary(patchwork)\n\n# Setting a plotting theme\ntheme_set(theme_linedraw() +\n            theme(text = element_text(family = \"Times\", size = 8),\n                  panel.grid = element_blank(),\n                  plot.title = element_markdown())\n)\n\n# Generate the plot for the prior of the Intercept (mu)\nPrior_mu &lt;- ggplot( ) +\n  stat_function(\n    fun = dt.scaled,    # We use the dt.scaled function of metRology\n    args = list(df = 3, mean = 199.2, sd = 24.9), # \n    xlim = c(120,300)\n  ) +\n  scale_y_continuous(name = \"density\") +\n  labs(title = \"Prior for the intercept\",\n       subtitle = \"student_t(3,199.2,24.9)\")\n\n# Generate the plot for the prior of the error variance (sigma)\nPrior_sigma &lt;- ggplot( ) +\n  stat_function(\n    fun = dt.scaled,    # We use the dt.scaled function of metRology\n    args = list(df = 3, mean = 0, sd = 24.9), # \n    xlim = c(0,6)\n  ) +\n  scale_y_continuous(name = \"density\") +\n  labs(title = \"Prior for the residual variance\",\n       subtitle = \"student_t(3,0,24.9)\")\n\n# Generate the plot for the prior of the effects of independent variables\nPrior_betas &lt;- ggplot( ) +\n  stat_function(\n    fun = dnorm,    # We use the normal distribution\n    args = list(mean = 0, sd = 10), # \n    xlim = c(-20,20)\n  ) +\n  scale_y_continuous(name = \"density\") +\n  labs(title = \"Prior for the effects of independent variables\",\n       subtitle = \"N(0,10)\")\n\nPrior_mu + Prior_sigma + Prior_betas +\n  plot_layout(ncol = 3)\n\n\n\n\nProbability density plots for the different priors used in the example model"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#understanding-priors-another-example",
    "href": "Presentations/Part2/Slides_Part2.html#understanding-priors-another-example",
    "title": "Applied Bayesian Analyses in R",
    "section": "Understanding priors… another example",
    "text": "Understanding priors… another example\nExperimental study (pretest - posttest design) with 3 conditions:\n\ncontrol group;\nexperimental group 1;\nexperimental group 2.\n\nModel:\n\\[\\begin{aligned}\n  & Posttest_{i}  \\sim N(\\mu,\\sigma_{e_{i}})\\\\\n  & \\mu = \\beta_0 + \\beta_1*\\text{Pretest}_{i} + \\beta_2*\\text{Exp_cond1}_{i} + \\beta_3*\\text{Exp_cond2}_{i}\n\\end{aligned}\\]\nOur job: coming up with priors that reflect that we expect both conditions to have a positive effect (hypothesis based on literature) and no indications that one experimental condition will outperform the other."
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#understanding-priors-another-example-1",
    "href": "Presentations/Part2/Slides_Part2.html#understanding-priors-another-example-1",
    "title": "Applied Bayesian Analyses in R",
    "section": "Understanding priors… another example",
    "text": "Understanding priors… another example\n\nAssuming pre- and posttest scores are standardized\nAssuming no increase between pre- and posttest in control condition"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#understanding-priors-another-example-2",
    "href": "Presentations/Part2/Slides_Part2.html#understanding-priors-another-example-2",
    "title": "Applied Bayesian Analyses in R",
    "section": "Understanding priors… another example",
    "text": "Understanding priors… another example\n\nAssuming a strong correlation between pre- and posttest"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#understanding-priors-another-example-3",
    "href": "Presentations/Part2/Slides_Part2.html#understanding-priors-another-example-3",
    "title": "Applied Bayesian Analyses in R",
    "section": "Understanding priors… another example",
    "text": "Understanding priors… another example\n\nAssuming a small effect of experimental conditions\nNo difference between both experimental conditions\n\n\n\n\n\n\n\n\n\n\nRemember Cohen’s d: 0.2 = small effect size; 0.5 = medium effect size; 0.8 or higher = large effect size"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#setting-custom-priors-in-brms",
    "href": "Presentations/Part2/Slides_Part2.html#setting-custom-priors-in-brms",
    "title": "Applied Bayesian Analyses in R",
    "section": "Setting custom priors in brms",
    "text": "Setting custom priors in brms\n\nSetting our custom priors can be done with set_prior( ) command\n\nE.g., change the priors for the beta’s (effects of km4week and sp4week):\n\n\nCustom_priors &lt;- \n  c(\n    set_prior(\n      \"normal(0,10)\", \n      class = \"b\", \n      coef = \"km4week\"),\n    set_prior(\n      \"normal(0,10)\", \n      class = \"b\", \n      coef = \"sp4week\")\n    )"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#prior-predictive-check",
    "href": "Presentations/Part2/Slides_Part2.html#prior-predictive-check",
    "title": "Applied Bayesian Analyses in R",
    "section": "Prior Predictive Check",
    "text": "Prior Predictive Check\n\nDid you set sensible priors?\n\n\nSimulate data based on the model and the priors\n\n\n\nVisualize the simulated data and compare with real data\n\n\n\nCheck if the plot shows impossible simulated datasets"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#prior-predictive-check-in-brms",
    "href": "Presentations/Part2/Slides_Part2.html#prior-predictive-check-in-brms",
    "title": "Applied Bayesian Analyses in R",
    "section": "Prior Predictive Check in brms",
    "text": "Prior Predictive Check in brms\n\nStep 1: Fit the model with custom priors with option sample_prior=\"only\"\n\n\nFit_Model_priors &lt;- \n  brm(\n    MarathonTimeM ~ 1 + km4week + sp4week, \n    data = MarathonData,\n    prior = Custom_priors,\n    backend = \"cmdstanr\",\n    cores = 4,\n    sample_prior = \"only\"\n    )"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#prior-predictive-check-in-brms-1",
    "href": "Presentations/Part2/Slides_Part2.html#prior-predictive-check-in-brms-1",
    "title": "Applied Bayesian Analyses in R",
    "section": "Prior Predictive Check in brms",
    "text": "Prior Predictive Check in brms\n\nStep 2: visualize the data with the pp_check( ) function\n\n\nset.seed(1975)\n\npp_check(\n  Fit_Model_priors, \n  ndraws = 300) # number of simulated datasets you wish for"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#check-some-summary-statistics",
    "href": "Presentations/Part2/Slides_Part2.html#check-some-summary-statistics",
    "title": "Applied Bayesian Analyses in R",
    "section": "Check some summary statistics",
    "text": "Check some summary statistics\n\nHow are summary statistics of simulated datasets (e.g., median, min, max, …) distributed over the datasets?\nHow does that compare to our real data?\nUse type = \"stat\" argument within pp_check()\n\n\npp_check(Fit_Model_priors, \n         type = \"stat\", \n         stat = \"median\")\n\nUsing all posterior draws for ppc type 'stat' by default.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#your-turn-1",
    "href": "Presentations/Part2/Slides_Part2.html#your-turn-1",
    "title": "Applied Bayesian Analyses in R",
    "section": " Your Turn",
    "text": "Your Turn\n\nYour data and model\nPerform a prior predictive check\nIf necessary re-think your priors and check again"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#does-the-trace-plot-exhibits-convergence",
    "href": "Presentations/Part2/Slides_Part2.html#does-the-trace-plot-exhibits-convergence",
    "title": "Applied Bayesian Analyses in R",
    "section": "Does the trace-plot exhibits convergence?",
    "text": "Does the trace-plot exhibits convergence?\n\nCreate custom trace-plots (aka caterpillar plots) with ggs( ) function from ggmcmc package\n\nModel_chains &lt;- ggs(MarathonTimes_Mod2)\n\nModel_chains %&gt;%\n  filter(Parameter %in% c(\n          \"b_Intercept\", \n          \"b_km4week\", \n          \"b_sp4week\", \n          \"sigma\"\n          )\n  ) %&gt;%\n  ggplot(aes(\n    x   = Iteration,\n    y   = value, \n    col = as.factor(Chain)))+\n  geom_line() +\n  facet_grid(Parameter ~ .,\n             scale  = 'free_y',\n             switch = 'y') +\n  labs(title = \"Caterpillar Plots for the parameters\",\n       col   = \"Chains\")\n\n\n\n\nCaterpillar plots for the parameters in the model"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#does-convergence-remain-after-doubling-the-number-of-iterations",
    "href": "Presentations/Part2/Slides_Part2.html#does-convergence-remain-after-doubling-the-number-of-iterations",
    "title": "Applied Bayesian Analyses in R",
    "section": "Does convergence remain after doubling the number of iterations?",
    "text": "Does convergence remain after doubling the number of iterations?\n\nRe-fit the model with more iterations\n\nCheck trace-plots again\n\n\n\n\n\n\n\nWarning\n\n\n\nFirst consider the need to do this! If you have a complex model that already took a long time to run, this check will take at least twice as much time…"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#your-turn-2",
    "href": "Presentations/Part2/Slides_Part2.html#your-turn-2",
    "title": "Applied Bayesian Analyses in R",
    "section": " Your Turn",
    "text": "Your Turn\n\nYour data and model\nDo the first checks on the model convergence"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#r-hat-statistics",
    "href": "Presentations/Part2/Slides_Part2.html#r-hat-statistics",
    "title": "Applied Bayesian Analyses in R",
    "section": "R-hat statistics",
    "text": "R-hat statistics\nSampling of parameters done by:\n\nmultiple chains\nmultiple iterations within chains\n\nIf variance between chains is big \\(\\rightarrow\\) NO CONVERGENCE\nR-hat (\\(\\widehat{R}\\)) : compares the between- and within-chain estimates for model parameters"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#r-hat-statistics-1",
    "href": "Presentations/Part2/Slides_Part2.html#r-hat-statistics-1",
    "title": "Applied Bayesian Analyses in R",
    "section": "R-hat statistics",
    "text": "R-hat statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\widehat{R}\\) &lt; 1.015 for each parameter estimate\nat least 4 chains are recommended\nEffective Sample Size (ESS) &gt; 400 to rely on \\(\\widehat{R}\\)"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#r-hat-in-brms",
    "href": "Presentations/Part2/Slides_Part2.html#r-hat-in-brms",
    "title": "Applied Bayesian Analyses in R",
    "section": "R-hat in brms",
    "text": "R-hat in brms\nmcmc_rhat() function from the bayesplot package\n\nmcmc_rhat(\n  brms::rhat(MarathonTimes_Mod2), \n  size = 3\n  )+ \n  yaxis_text(hjust = 1)  # to print parameter names"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#your-turn-3",
    "href": "Presentations/Part2/Slides_Part2.html#your-turn-3",
    "title": "Applied Bayesian Analyses in R",
    "section": " Your Turn",
    "text": "Your Turn\n\nYour data and model\nCheck the R-hat statistics"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#autocorrelation",
    "href": "Presentations/Part2/Slides_Part2.html#autocorrelation",
    "title": "Applied Bayesian Analyses in R",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nSampling of parameter values are not independent!\nSo there is autocorrelation\nBut you don’t want too much impact of autocorrelation\n2 approaches to check this:\n\nratio of the effective sample size to the total sample size\nplot degree of autocorrelation"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#ratio-effective-sample-size-total-sample-size",
    "href": "Presentations/Part2/Slides_Part2.html#ratio-effective-sample-size-total-sample-size",
    "title": "Applied Bayesian Analyses in R",
    "section": "Ratio effective sample size / total sample size",
    "text": "Ratio effective sample size / total sample size\n\nShould be higher than 0.1 (Gelman et al., 2013)\nVisualize making use of the mcmc_neff( ) function from bayesplot\n\n\nmcmc_neff(\n  neff_ratio(MarathonTimes_Mod2)\n  ) + \n  yaxis_text(hjust = 1)  # to print parameter names"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#plot-degree-of-autocorrelation",
    "href": "Presentations/Part2/Slides_Part2.html#plot-degree-of-autocorrelation",
    "title": "Applied Bayesian Analyses in R",
    "section": "Plot degree of autocorrelation",
    "text": "Plot degree of autocorrelation\n\nVisualize making use of the mcmc_acf( ) function\n\n\nmcmc_acf(\n  as.array(MarathonTimes_Mod2), \n  regex = \"b\") # to plot only the parameters starting with b (our beta's)"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#your-turn-4",
    "href": "Presentations/Part2/Slides_Part2.html#your-turn-4",
    "title": "Applied Bayesian Analyses in R",
    "section": " Your Turn",
    "text": "Your Turn\n\nYour data and model\nCheck the autocorrelation"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#rank-order-plots",
    "href": "Presentations/Part2/Slides_Part2.html#rank-order-plots",
    "title": "Applied Bayesian Analyses in R",
    "section": "Rank order plots",
    "text": "Rank order plots\n\nadditional way to assess the convergence of MCMC\nif the algorithm converged, plots of all chains look similar\n\n\nmcmc_rank_hist(\n  MarathonTimes_Mod2, \n  regex = \"b\" # only intercept and beta's\n  )"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#your-turn-5",
    "href": "Presentations/Part2/Slides_Part2.html#your-turn-5",
    "title": "Applied Bayesian Analyses in R",
    "section": " Your Turn",
    "text": "Your Turn\n\nYour data and model\nCheck the rank order plots"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#does-the-posterior-distribution-histogram-have-enough-information",
    "href": "Presentations/Part2/Slides_Part2.html#does-the-posterior-distribution-histogram-have-enough-information",
    "title": "Applied Bayesian Analyses in R",
    "section": "Does the posterior distribution histogram have enough information?",
    "text": "Does the posterior distribution histogram have enough information?\n\nHistogram of posterior for each parameter\nHave clear peak and sliding slopes"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#plotting-the-posterior-distribution-histogram",
    "href": "Presentations/Part2/Slides_Part2.html#plotting-the-posterior-distribution-histogram",
    "title": "Applied Bayesian Analyses in R",
    "section": "Plotting the posterior distribution histogram",
    "text": "Plotting the posterior distribution histogram\n\nStep 1: create a new object with ‘draws’ based on the final model\n\n\nposterior_PD &lt;- as_draws_df(MarathonTimes_Mod2)"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#plotting-the-posterior-distribution-histogram-1",
    "href": "Presentations/Part2/Slides_Part2.html#plotting-the-posterior-distribution-histogram-1",
    "title": "Applied Bayesian Analyses in R",
    "section": "Plotting the posterior distribution histogram",
    "text": "Plotting the posterior distribution histogram\n\nStep 2: create histogram making use of that object\n\n\npost_intercept &lt;- \n  posterior_PD %&gt;%\n  select(b_Intercept) %&gt;%\n  ggplot(aes(x = b_Intercept)) +\n  geom_histogram() +\n  ggtitle(\"Intercept\") \n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\npost_km4week &lt;- \n  posterior_PD %&gt;%\n  select(b_km4week) %&gt;%\n  ggplot(aes(x = b_km4week)) +\n  geom_histogram() +\n  ggtitle(\"Beta km4week\") \n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\npost_sp4week &lt;- \n  posterior_PD %&gt;%\n  select(b_sp4week) %&gt;%\n  ggplot(aes(x = b_sp4week)) +\n  geom_histogram() +\n  ggtitle(\"Beta sp4week\") \n\nWarning: Dropping 'draws_df' class as required metadata was removed."
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#plotting-the-posterior-distribution-histogram-2",
    "href": "Presentations/Part2/Slides_Part2.html#plotting-the-posterior-distribution-histogram-2",
    "title": "Applied Bayesian Analyses in R",
    "section": "Plotting the posterior distribution histogram",
    "text": "Plotting the posterior distribution histogram\n\nStep 3: print the plot making use of patchwork ’s workflow to combine plots \n\npost_intercept + post_km4week + post_sp4week +\n  plot_layout(ncol = 3)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#posterior-predictive-check-1",
    "href": "Presentations/Part2/Slides_Part2.html#posterior-predictive-check-1",
    "title": "Applied Bayesian Analyses in R",
    "section": "Posterior Predictive Check",
    "text": "Posterior Predictive Check\n\nGenerate data based on the posterior probability distribution\nCreate plot of distribution of y-values in these simulated datasets\nOverlay with distribution of observed data\n\nusing pp_check() again, now with our model\n\npp_check(MarathonTimes_Mod2, \n         ndraws = 100)"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#posterior-predictive-check-2",
    "href": "Presentations/Part2/Slides_Part2.html#posterior-predictive-check-2",
    "title": "Applied Bayesian Analyses in R",
    "section": "Posterior Predictive Check",
    "text": "Posterior Predictive Check\n\nWe can also focus on some summary statistics (like we did with prior predictive checks as well)\n\n\npp_check(MarathonTimes_Mod2, \n         ndraws = 300,\n         type = \"stat\",\n         stat = \"median\")"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#your-turn-6",
    "href": "Presentations/Part2/Slides_Part2.html#your-turn-6",
    "title": "Applied Bayesian Analyses in R",
    "section": " Your Turn",
    "text": "Your Turn\n\nYour data and model\nFocus on the posterior and do some checks!"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#why-prior-sensibility-analyses",
    "href": "Presentations/Part2/Slides_Part2.html#why-prior-sensibility-analyses",
    "title": "Applied Bayesian Analyses in R",
    "section": "Why prior sensibility analyses?",
    "text": "Why prior sensibility analyses?\n\nOften we rely on ‘arbitrary’ chosen (default) weakly informative priors\nWhat is the influence of the prior (and the likelihood) on our results?\nYou could ad hoc set new priors and re-run the analyses and compare (a lot of work, without strict sytematical guidelines)\nSemi-automated checks can be done with priorsense package"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#using-the-priorsense-package",
    "href": "Presentations/Part2/Slides_Part2.html#using-the-priorsense-package",
    "title": "Applied Bayesian Analyses in R",
    "section": "Using the priorsense package",
    "text": "Using the priorsense package\nRecently a package dedicated to prior sensibility analyses is launched\n\n# install.packages(\"remotes\")\nremotes::install_github(\"n-kall/priorsense\")\n\nKey-idea: power-scaling (both prior and likelihood)\nbackground reading:\n\nhttps://arxiv.org/pdf/2107.14054.pdf\n\nYouTube talk:\n\nhttps://www.youtube.com/watch?v=TBXD3HjcIps&t=920s"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#basic-table-with-indices",
    "href": "Presentations/Part2/Slides_Part2.html#basic-table-with-indices",
    "title": "Applied Bayesian Analyses in R",
    "section": "Basic table with indices",
    "text": "Basic table with indices\nFirst check is done by using the powerscale_sensitivity( ) function\n\ncolumn prior contains info on sensibility for prior (should be lower than 0.05)\ncolumn likelihood contains info on sensibility for likelihood (that we want to be high, ‘let our data speak’)\ncolumn diagnosis is a verbalization of potential problem (- if none)\n\n\npowerscale_sensitivity(MarathonTimes_Mod2)\n\nSensitivity based on cjs_dist:\n# A tibble: 4 × 4\n  variable       prior likelihood diagnosis\n  &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;    \n1 b_Intercept 0.000853     0.0851 -        \n2 b_km4week   0.000512     0.0802 -        \n3 b_sp4week   0.000370     0.0831 -        \n4 sigma       0.00571      0.151  -"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#visualization-of-prior-sensibility",
    "href": "Presentations/Part2/Slides_Part2.html#visualization-of-prior-sensibility",
    "title": "Applied Bayesian Analyses in R",
    "section": "Visualization of prior sensibility",
    "text": "Visualization of prior sensibility\n\npowerscale_plot_dens(\n  powerscale_sequence(\n    MarathonTimes_Mod2\n    ),\n  variable = c(\n      \"b_Intercept\",\n      \"b_km4week\",\n      \"b_sp4week\"\n    )\n  )"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#visualization-of-prior-sensibility-1",
    "href": "Presentations/Part2/Slides_Part2.html#visualization-of-prior-sensibility-1",
    "title": "Applied Bayesian Analyses in R",
    "section": "Visualization of prior sensibility",
    "text": "Visualization of prior sensibility\n\npowerscale_plot_quantities(\n  powerscale_sequence(\n    MarathonTimes_Mod2\n    ),\n  variable = c(\n      \"b_km4week\"\n      )\n  )"
  },
  {
    "objectID": "Presentations/Part2/Slides_Part2.html#your-turn-7",
    "href": "Presentations/Part2/Slides_Part2.html#your-turn-7",
    "title": "Applied Bayesian Analyses in R",
    "section": " Your Turn",
    "text": "Your Turn\n\nYour data and model\nCheck the prior sensibility of your results"
  },
  {
    "objectID": "Integrated_exercise/Exercise.html",
    "href": "Integrated_exercise/Exercise.html",
    "title": "Integrated excercise",
    "section": "",
    "text": "This dataset is a simulated dataset that is based on an existing study of Frumuselu et al. (2015). In this study, the key question was whether subtitles help in foreign language acquisition. Spanish students (n = 36) watched episodes of the popular tv-show “Friends” for half an hour each week, during 26 weeks. The students were assigned to 3 conditions:\n\nEnglish subtitled (condition “FL”)\nSpanish subtitled (condition “MT”)\nNo subtitles (condition “NoSub”)\n\nAt 3 occasions students got a Fluency test:\n\nBefore the 26 weeks started\nAfter 12 weeks\nAfter the experiment\n\nThe dependent variable is a measure based on the number of words used in a scripted spontaneous interview with a test taker. The data is structured as follows:\n\n\nCode\nload(file = \"Subtitles.RData\")\nhead(Subtitles, 9)\n\n\n  student occasion condition fluency\n1       1     Occ1        FL  101.25\n2       1     Occ2        FL  103.76\n3       1     Occ3        FL  117.39\n4       2     Occ1        MT   98.79\n5       2     Occ2        MT  106.75\n6       2     Occ3        MT  110.54\n7       3     Occ1     NoSub  104.83\n8       3     Occ2     NoSub  102.04\n9       3     Occ3     NoSub  100.63\n\n\nIf we visualize the dataset we get a first impression of the effect of the condition. In this exercise it is your task to do the proper Bayesian modelling and interpretation.\n\n\nCode\nlibrary(tidyverse)\n\ntheme_set(theme_linedraw() +\n            theme(text = element_text(family = \"Times\", size = 10),\n                  panel.grid = element_blank())\n)\n\nSubtitles %&gt;%\n  ggplot(\n    aes(\n      x = occasion,\n      y = fluency,\n      group = student\n      )\n  ) +\n  geom_path(\n    aes(\n      color = condition\n    )\n  )"
  },
  {
    "objectID": "Integrated_exercise/Exercise.html#subtask-2.1-what-about-the-priors",
    "href": "Integrated_exercise/Exercise.html#subtask-2.1-what-about-the-priors",
    "title": "Integrated excercise",
    "section": "Subtask 2.1: what about the priors?",
    "text": "Subtask 2.1: what about the priors?\nWhat are the default brms priors? Do they make sense? Do they generate impossible datasets? If necessary, specify your own (weakly informative) priors and approach them critically as well."
  },
  {
    "objectID": "Integrated_exercise/Exercise.html#subtask-2.2-did-the-model-converge-properly",
    "href": "Integrated_exercise/Exercise.html#subtask-2.2-did-the-model-converge-properly",
    "title": "Integrated excercise",
    "section": "Subtask 2.2: did the model converge properly?",
    "text": "Subtask 2.2: did the model converge properly?\nPerform different checks on the convergence of the model."
  },
  {
    "objectID": "Integrated_exercise/Exercise.html#subtask-2.3-does-the-posterior-distribution-histogram-have-enough-information",
    "href": "Integrated_exercise/Exercise.html#subtask-2.3-does-the-posterior-distribution-histogram-have-enough-information",
    "title": "Integrated excercise",
    "section": "Subtask 2.3: does the posterior distribution histogram have enough information?",
    "text": "Subtask 2.3: does the posterior distribution histogram have enough information?\nCheck if the posterior distribution histograms of the different parameters are informative enough to substantiate our inferences."
  },
  {
    "objectID": "Integrated_exercise/Exercise.html#subtask-2.4-how-well-does-the-model-predict-the-observed-data",
    "href": "Integrated_exercise/Exercise.html#subtask-2.4-how-well-does-the-model-predict-the-observed-data",
    "title": "Integrated excercise",
    "section": "Subtask 2.4: how well does the model predict the observed data?",
    "text": "Subtask 2.4: how well does the model predict the observed data?\nPerform posterior predictive checks based on the model."
  },
  {
    "objectID": "Integrated_exercise/Exercise.html#subtask-2.5-what-about-prior-sensitivity-of-the-results",
    "href": "Integrated_exercise/Exercise.html#subtask-2.5-what-about-prior-sensitivity-of-the-results",
    "title": "Integrated excercise",
    "section": "Subtask 2.5: what about prior sensitivity of the results?",
    "text": "Subtask 2.5: what about prior sensitivity of the results?\nFinally, we have to check if the results of our model are not too dependent on the priors we specified in the model."
  },
  {
    "objectID": "Part1.html",
    "href": "Part1.html",
    "title": "Part 1: the prior, the likelihood, and the posterior",
    "section": "",
    "text": "In the first part, we will introduce the basic rationale behind Bayesian statistics. We start by explaining the three key components of a Bayesian model: the prior, the likelihood, and the posterior.\nThen, we switch to the estimation of parameters by first introducing the basic idea of grid approximation and then outlining the basic idea of MCMC sampling.\nAt the end of the session we introduce brms and learn how to estimate a simple regression model with brms and just use the summary() and plot() functions to get insight in the model results.",
    "crumbs": [
      "Part1"
    ]
  },
  {
    "objectID": "Part1.html#outline",
    "href": "Part1.html#outline",
    "title": "Part 1: the prior, the likelihood, and the posterior",
    "section": "",
    "text": "In the first part, we will introduce the basic rationale behind Bayesian statistics. We start by explaining the three key components of a Bayesian model: the prior, the likelihood, and the posterior.\nThen, we switch to the estimation of parameters by first introducing the basic idea of grid approximation and then outlining the basic idea of MCMC sampling.\nAt the end of the session we introduce brms and learn how to estimate a simple regression model with brms and just use the summary() and plot() functions to get insight in the model results.",
    "crumbs": [
      "Part1"
    ]
  },
  {
    "objectID": "Part1.html#materials",
    "href": "Part1.html#materials",
    "title": "Part 1: the prior, the likelihood, and the posterior",
    "section": "Materials",
    "text": "Materials\n\nSlides\nThe htlm-version of the slides for this first part can be found here\n\n\nData\nFor this first part, we use a straightforward dataset on predicting racetimes for a marathon. The data can be downloaded  here  (right-click to save as).",
    "crumbs": [
      "Part1"
    ]
  },
  {
    "objectID": "Part1.html#references-and-resources",
    "href": "Part1.html#references-and-resources",
    "title": "Part 1: the prior, the likelihood, and the posterior",
    "section": "References and resources",
    "text": "References and resources\nData comes from Kaggle\nPaul Bürkner’s presentation available on YouTube: click here\nInteractive tool demonstrating MCMC sampling:  click here \nbrms homepage:  click here",
    "crumbs": [
      "Part1"
    ]
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#basic-output-brms-output",
    "href": "Presentations/Part1/Slides_Part1.html#basic-output-brms-output",
    "title": "Applied Bayesian Analyses in R",
    "section": "Basic output brms",
    "text": "Basic output brms\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: MT ~ 1 \n   Data: DataMT (Number of observations: 10) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   209.58     11.18   187.37   231.78 1.00     2386     1935\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    36.21      9.51    23.26    58.13 1.00     2487     2329\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#good-old-plot-function-output",
    "href": "Presentations/Part1/Slides_Part1.html#good-old-plot-function-output",
    "title": "Applied Bayesian Analyses in R",
    "section": "Good old plot() function",
    "text": "Good old plot() function"
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#posterior-predictive-check-output",
    "href": "Presentations/Part1/Slides_Part1.html#posterior-predictive-check-output",
    "title": "Applied Bayesian Analyses in R",
    "section": "Posterior-predictive check",
    "text": "Posterior-predictive check\n\nUsing 10 posterior draws for ppc type 'dens_overlay' by default."
  },
  {
    "objectID": "Presentations/Part1/Slides_Part1.html#model-comparison-with-loo-cross-validation-output",
    "href": "Presentations/Part1/Slides_Part1.html#model-comparison-with-loo-cross-validation-output",
    "title": "Applied Bayesian Analyses in R",
    "section": "Model comparison with loo cross-validation",
    "text": "Model comparison with loo cross-validation\n\nWarning: Found 1 observations with a pareto_k &gt; 0.7 in model\n'MarathonTimes_Mod2'. We recommend to set 'moment_match = TRUE' in order to\nperform moment matching for problematic observations.\n\nComparison&lt;- \n  loo_compare(\n    loo_Mod1, \n    loo_Mod2\n    )\n\nprint(Comparison, simplify = F)\n\n                   elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo\nMarathonTimes_Mod2    0.0       0.0  -356.3     12.0         7.2    4.4  \nMarathonTimes_Mod1  -39.9      12.6  -396.2      5.3         1.7    0.3  \n                   looic  se_looic\nMarathonTimes_Mod2  712.6   24.0  \nMarathonTimes_Mod1  792.3   10.6"
  }
]